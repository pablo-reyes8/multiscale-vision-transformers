MaxViT(
  (stem): MaxViTStem(
    (stem): ConvBNAct(
      (conv): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)
      (bn): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
      (act): SiLU(inplace=True)
    )
  )
  (stages): ModuleList(
    (0): MaxViTStage(
      (blocks): Sequential(
        (0): MaxViTBlock(
          (mbconv): MBConv(
            (expand): Sequential(
              (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): SiLU(inplace=True)
            )
            (depthwise): Sequential(
              (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)
              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): SiLU(inplace=True)
            )
            (se): SqueezeExcite(
              (pool): AdaptiveAvgPool2d(output_size=1)
              (fc1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
              (act): SiLU(inplace=True)
              (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
              (gate): Sigmoid()
            )
            (project): Sequential(
              (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (drop_path): Identity()
          )
          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (window_attn): LocalAttention2D(
            (mhsa): MultiHeadSelfAttention(
              (qkv): Linear(in_features=64, out_features=192, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=64, out_features=64, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
          )
          (drop_path1): Identity()
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (grid_attn): LocalAttention2D(
            (mhsa): MultiHeadSelfAttention(
              (qkv): Linear(in_features=64, out_features=192, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=64, out_features=64, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
          )
          (drop_path2): Identity()
          (norm3): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=64, out_features=256, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=256, out_features=64, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (drop_path3): Identity()
        )
        (1): MaxViTBlock(
          (mbconv): MBConv(
            (expand): Sequential(
              (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): SiLU(inplace=True)
            )
            (depthwise): Sequential(
              (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=256, bias=False)
              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): SiLU(inplace=True)
            )
            (se): SqueezeExcite(
              (pool): AdaptiveAvgPool2d(output_size=1)
              (fc1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1))
              (act): SiLU(inplace=True)
              (fc2): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1))
              (gate): Sigmoid()
            )
            (project): Sequential(
              (0): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (drop_path): Identity()
          )
          (norm1): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (window_attn): LocalAttention2D(
            (mhsa): MultiHeadSelfAttention(
              (qkv): Linear(in_features=64, out_features=192, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=64, out_features=64, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
          )
          (drop_path1): DropPath()
          (norm2): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (grid_attn): LocalAttention2D(
            (mhsa): MultiHeadSelfAttention(
              (qkv): Linear(in_features=64, out_features=192, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=64, out_features=64, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
          )
          (drop_path2): DropPath()
          (norm3): LayerNorm((64,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=64, out_features=256, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=256, out_features=64, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (drop_path3): DropPath()
        )
      )
    )
    (1): MaxViTStage(
      (blocks): Sequential(
        (0): MaxViTBlock(
          (mbconv): MBConv(
            (expand): Sequential(
              (0): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): SiLU(inplace=True)
            )
            (depthwise): Sequential(
              (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)
              (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): SiLU(inplace=True)
            )
            (se): SqueezeExcite(
              (pool): AdaptiveAvgPool2d(output_size=1)
              (fc1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
              (act): SiLU(inplace=True)
              (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
              (gate): Sigmoid()
            )
            (project): Sequential(
              (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (drop_path): Identity()
          )
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (window_attn): LocalAttention2D(
            (mhsa): MultiHeadSelfAttention(
              (qkv): Linear(in_features=128, out_features=384, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=128, out_features=128, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
          )
          (drop_path1): DropPath()
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (grid_attn): LocalAttention2D(
            (mhsa): MultiHeadSelfAttention(
              (qkv): Linear(in_features=128, out_features=384, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=128, out_features=128, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
          )
          (drop_path2): DropPath()
          (norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (drop_path3): DropPath()
        )
        (1): MaxViTBlock(
          (mbconv): MBConv(
            (expand): Sequential(
              (0): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): SiLU(inplace=True)
            )
            (depthwise): Sequential(
              (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=512, bias=False)
              (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): SiLU(inplace=True)
            )
            (se): SqueezeExcite(
              (pool): AdaptiveAvgPool2d(output_size=1)
              (fc1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1))
              (act): SiLU(inplace=True)
              (fc2): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1))
              (gate): Sigmoid()
            )
            (project): Sequential(
              (0): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (drop_path): Identity()
          )
          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (window_attn): LocalAttention2D(
            (mhsa): MultiHeadSelfAttention(
              (qkv): Linear(in_features=128, out_features=384, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=128, out_features=128, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
          )
          (drop_path1): DropPath()
          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (grid_attn): LocalAttention2D(
            (mhsa): MultiHeadSelfAttention(
              (qkv): Linear(in_features=128, out_features=384, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=128, out_features=128, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
          )
          (drop_path2): DropPath()
          (norm3): LayerNorm((128,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=128, out_features=512, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=512, out_features=128, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (drop_path3): DropPath()
        )
      )
    )
    (2): MaxViTStage(
      (blocks): Sequential(
        (0): MaxViTBlock(
          (mbconv): MBConv(
            (expand): Sequential(
              (0): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): SiLU(inplace=True)
            )
            (depthwise): Sequential(
              (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)
              (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): SiLU(inplace=True)
            )
            (se): SqueezeExcite(
              (pool): AdaptiveAvgPool2d(output_size=1)
              (fc1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
              (act): SiLU(inplace=True)
              (fc2): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
              (gate): Sigmoid()
            )
            (project): Sequential(
              (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (drop_path): Identity()
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (window_attn): LocalAttention2D(
            (mhsa): MultiHeadSelfAttention(
              (qkv): Linear(in_features=256, out_features=768, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
          )
          (drop_path1): DropPath()
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (grid_attn): LocalAttention2D(
            (mhsa): MultiHeadSelfAttention(
              (qkv): Linear(in_features=256, out_features=768, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
          )
          (drop_path2): DropPath()
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (drop_path3): DropPath()
        )
        (1): MaxViTBlock(
          (mbconv): MBConv(
            (expand): Sequential(
              (0): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): SiLU(inplace=True)
            )
            (depthwise): Sequential(
              (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)
              (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): SiLU(inplace=True)
            )
            (se): SqueezeExcite(
              (pool): AdaptiveAvgPool2d(output_size=1)
              (fc1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
              (act): SiLU(inplace=True)
              (fc2): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
              (gate): Sigmoid()
            )
            (project): Sequential(
              (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (drop_path): Identity()
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (window_attn): LocalAttention2D(
            (mhsa): MultiHeadSelfAttention(
              (qkv): Linear(in_features=256, out_features=768, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
          )
          (drop_path1): DropPath()
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (grid_attn): LocalAttention2D(
            (mhsa): MultiHeadSelfAttention(
              (qkv): Linear(in_features=256, out_features=768, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
          )
          (drop_path2): DropPath()
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (drop_path3): DropPath()
        )
        (2): MaxViTBlock(
          (mbconv): MBConv(
            (expand): Sequential(
              (0): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): SiLU(inplace=True)
            )
            (depthwise): Sequential(
              (0): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1024, bias=False)
              (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): SiLU(inplace=True)
            )
            (se): SqueezeExcite(
              (pool): AdaptiveAvgPool2d(output_size=1)
              (fc1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))
              (act): SiLU(inplace=True)
              (fc2): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1))
              (gate): Sigmoid()
            )
            (project): Sequential(
              (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (drop_path): Identity()
          )
          (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (window_attn): LocalAttention2D(
            (mhsa): MultiHeadSelfAttention(
              (qkv): Linear(in_features=256, out_features=768, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
          )
          (drop_path1): DropPath()
          (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (grid_attn): LocalAttention2D(
            (mhsa): MultiHeadSelfAttention(
              (qkv): Linear(in_features=256, out_features=768, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=256, out_features=256, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
          )
          (drop_path2): DropPath()
          (norm3): LayerNorm((256,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=256, out_features=1024, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=1024, out_features=256, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (drop_path3): DropPath()
        )
      )
    )
    (3): MaxViTStage(
      (blocks): Sequential(
        (0): MaxViTBlock(
          (mbconv): MBConv(
            (expand): Sequential(
              (0): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): SiLU(inplace=True)
            )
            (depthwise): Sequential(
              (0): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048, bias=False)
              (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): SiLU(inplace=True)
            )
            (se): SqueezeExcite(
              (pool): AdaptiveAvgPool2d(output_size=1)
              (fc1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))
              (act): SiLU(inplace=True)
              (fc2): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1))
              (gate): Sigmoid()
            )
            (project): Sequential(
              (0): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (drop_path): Identity()
          )
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (window_attn): LocalAttention2D(
            (mhsa): MultiHeadSelfAttention(
              (qkv): Linear(in_features=512, out_features=1536, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=512, out_features=512, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
          )
          (drop_path1): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (grid_attn): LocalAttention2D(
            (mhsa): MultiHeadSelfAttention(
              (qkv): Linear(in_features=512, out_features=1536, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=512, out_features=512, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
          )
          (drop_path2): DropPath()
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (drop_path3): DropPath()
        )
        (1): MaxViTBlock(
          (mbconv): MBConv(
            (expand): Sequential(
              (0): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): SiLU(inplace=True)
            )
            (depthwise): Sequential(
              (0): Conv2d(2048, 2048, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=2048, bias=False)
              (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
              (2): SiLU(inplace=True)
            )
            (se): SqueezeExcite(
              (pool): AdaptiveAvgPool2d(output_size=1)
              (fc1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1))
              (act): SiLU(inplace=True)
              (fc2): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1))
              (gate): Sigmoid()
            )
            (project): Sequential(
              (0): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)
              (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
            )
            (drop_path): Identity()
          )
          (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (window_attn): LocalAttention2D(
            (mhsa): MultiHeadSelfAttention(
              (qkv): Linear(in_features=512, out_features=1536, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=512, out_features=512, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
          )
          (drop_path1): DropPath()
          (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (grid_attn): LocalAttention2D(
            (mhsa): MultiHeadSelfAttention(
              (qkv): Linear(in_features=512, out_features=1536, bias=True)
              (attn_drop): Dropout(p=0.0, inplace=False)
              (proj): Linear(in_features=512, out_features=512, bias=True)
              (proj_drop): Dropout(p=0.0, inplace=False)
            )
          )
          (drop_path2): DropPath()
          (norm3): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
          (mlp): MLP(
            (fc1): Linear(in_features=512, out_features=2048, bias=True)
            (act): GELU(approximate='none')
            (drop1): Dropout(p=0.0, inplace=False)
            (fc2): Linear(in_features=2048, out_features=512, bias=True)
            (drop2): Dropout(p=0.0, inplace=False)
          )
          (drop_path3): DropPath()
        )
      )
    )
  )
  (downsamples): ModuleList(
    (0): Downsample(
      (op): Sequential(
        (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): SiLU(inplace=True)
      )
    )
    (1): Downsample(
      (op): Sequential(
        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): SiLU(inplace=True)
      )
    )
    (2): Downsample(
      (op): Sequential(
        (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)
        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)
        (2): SiLU(inplace=True)
      )
    )
  )
  (pool): AdaptiveAvgPool2d(output_size=1)
  (head): Linear(in_features=512, out_features=100, bias=True)
)