{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31240,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader, random_split\nfrom torchvision import datasets, transforms\nfrom torchvision.transforms import RandAugment\n\n\ndef get_cifar100_datasets(data_dir: str = \"./data\", val_split: float = 0.0):\n    \"\"\"\n    Descarga (si hace falta) y devuelve los datasets de CIFAR-100:\n    - train_dataset (o train_rest, val_dataset si val_split > 0)\n    - test_dataset\n\n    Args:\n        data_dir: carpeta donde se guardan/descargan los datos.\n        val_split: proporción del train que se reserva como validación (0.0 = sin validación).\n\n    Returns:\n        Si val_split == 0.0:\n            train_dataset, None, test_dataset\n        Si val_split > 0.0:\n            train_dataset, val_dataset, test_dataset\n    \"\"\"\n\n    # Stats típicas de CIFAR-100\n    cifar100_mean = (0.5071, 0.4867, 0.4408)\n    cifar100_std  = (0.2675, 0.2565, 0.2761)\n\n    train_transform = transforms.Compose([\n      transforms.RandomCrop(32, padding=4),\n      transforms.RandomHorizontalFlip(),\n      RandAugment(num_ops=2, magnitude=9),\n      transforms.ToTensor(),\n      transforms.Normalize(cifar100_mean, cifar100_std),])\n\n    # Transformaciones para test/val (solo resize + normalize)\n    test_transform = transforms.Compose([\n        transforms.ToTensor(),\n        transforms.Normalize(cifar100_mean, cifar100_std),])\n\n    # Dataset de training\n    full_train_dataset = datasets.CIFAR100(\n        root=data_dir,\n        train=True,\n        download=True,\n        transform=train_transform,)\n\n    # Dataset de test\n    test_dataset = datasets.CIFAR100(\n        root=data_dir,\n        train=False,\n        download=True,\n        transform=test_transform)\n\n    if val_split > 0.0:\n        n_total = len(full_train_dataset)\n        n_val = int(n_total * val_split)\n        n_train = n_total - n_val\n        train_dataset, val_dataset = random_split(\n            full_train_dataset,\n            [n_train, n_val],\n            generator=torch.Generator().manual_seed(7))\n\n    else:\n        train_dataset = full_train_dataset\n        val_dataset = None\n\n    return train_dataset, val_dataset, test_dataset\n\n\ndef get_cifar100_dataloaders(\n    batch_size: int = 128,\n    data_dir: str = \"./data\",\n    num_workers: int = 2,\n    val_split: float = 0.0,\n    pin_memory: bool = True):\n    \"\"\"\n    Devuelve DataLoaders para CIFAR-100.\n\n    Returns:\n        train_loader, val_loader, test_loader\n        (val_loader será None si val_split == 0.0)\n    \"\"\"\n    train_ds, val_ds, test_ds = get_cifar100_datasets(\n        data_dir=data_dir,\n        val_split=val_split)\n\n    train_loader = DataLoader(\n        train_ds,\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=num_workers,\n        pin_memory=pin_memory)\n\n\n    if val_ds is not None:\n        val_loader = DataLoader(\n            val_ds,\n            batch_size=batch_size,\n            shuffle=False,\n            num_workers=num_workers,\n            pin_memory=pin_memory,)\n\n    else:\n        val_loader = None\n\n    test_loader = DataLoader(\n        test_ds,\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=num_workers,\n        pin_memory=pin_memory)\n\n    return train_loader, val_loader, test_loader","metadata":{"id":"cQt7JNw2tal2","trusted":true,"execution":{"iopub.status.busy":"2025-12-18T05:29:22.947228Z","iopub.execute_input":"2025-12-18T05:29:22.947495Z","iopub.status.idle":"2025-12-18T05:29:29.577783Z","shell.execute_reply.started":"2025-12-18T05:29:22.947474Z","shell.execute_reply":"2025-12-18T05:29:29.577007Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"train_loader, val_loader, test_loader = get_cifar100_dataloaders(\n    batch_size=64,\n    data_dir=\"./data/cifar100\",\n    num_workers=2,\n    val_split=0.1)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N7tZS5vVt8Fj","outputId":"836461e0-6bdf-4b2d-cce2-2b920d6fb45b","trusted":true,"execution":{"iopub.status.busy":"2025-12-18T05:29:29.578985Z","iopub.execute_input":"2025-12-18T05:29:29.579549Z","iopub.status.idle":"2025-12-18T05:29:36.522102Z","shell.execute_reply.started":"2025-12-18T05:29:29.579521Z","shell.execute_reply":"2025-12-18T05:29:36.521476Z"}},"outputs":[{"name":"stderr","text":"100%|██████████| 169M/169M [00:03<00:00, 49.5MB/s] \n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nfrom torchvision.utils import make_grid\nfrom torchvision import datasets\n\nCIFAR100_MEAN = (0.5071, 0.4867, 0.4408)\nCIFAR100_STD  = (0.2675, 0.2565, 0.2761)\n\nimages, labels = next(iter(train_loader))\n\nprint(f\"Images shape: {images.shape}\")\nprint(f\"Labels shape: {labels.shape}\")\nprint(f\"Images dtype: {images.dtype}, Labels dtype: {labels.dtype}\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6BsWAfCcuBNR","outputId":"167262e6-f80e-4ed6-bd13-ac47ede7936b","trusted":true,"execution":{"iopub.status.busy":"2025-12-18T05:29:36.523266Z","iopub.execute_input":"2025-12-18T05:29:36.523665Z","iopub.status.idle":"2025-12-18T05:29:36.978431Z","shell.execute_reply.started":"2025-12-18T05:29:36.523638Z","shell.execute_reply":"2025-12-18T05:29:36.977536Z"}},"outputs":[{"name":"stdout","text":"Images shape: torch.Size([64, 3, 32, 32])\nLabels shape: torch.Size([64])\nImages dtype: torch.float32, Labels dtype: torch.int64\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"def unnormalize(images: torch.Tensor,\n                mean=CIFAR100_MEAN,\n                std=CIFAR100_STD):\n    \"\"\"\n    Des-normaliza un batch de imágenes.\n    images: tensor [B, C, H, W] normalizado.\n    \"\"\"\n    mean = torch.tensor(mean, device=images.device).view(1, -1, 1, 1)\n    std = torch.tensor(std, device=images.device).view(1, -1, 1, 1)\n    return images * std + mean\n\n\ndef show_batch(images: torch.Tensor,\n               labels: torch.Tensor,\n               class_names=None,\n               n: int = 8):\n    \"\"\"\n    Muestra las primeras n imágenes de un batch con sus labels.\n\n    Args:\n        images: tensor [B, C, H, W] (normalizado).\n        labels: tensor [B].\n        class_names: lista de nombres de clases (len = 100).\n        n: cuántas imágenes mostrar (en una fila).\n    \"\"\"\n    images = images[:n].cpu()\n    labels = labels[:n].cpu()\n    images_unnorm = unnormalize(images)\n\n    grid = make_grid(images_unnorm, nrow=n, padding=2)\n    npimg = grid.permute(1, 2, 0).numpy()\n\n    plt.figure(figsize=(2 * n, 2.5))\n    plt.imshow(npimg)\n    plt.axis(\"off\")\n\n    if class_names is not None:\n        title = \" | \".join(class_names[int(lbl)] for lbl in labels)\n        plt.title(title, fontsize=10)\n    plt.show()\n\ncifar100_train = datasets.CIFAR100(\n    root=\"./data/cifar100\",\n    train=True,\n    download=False)\nclass_names = cifar100_train.classes\n\nshow_batch(images, labels, class_names=class_names, n=8)","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":219},"id":"hVmpruwauC4O","outputId":"8392f3fe-dbfa-4c82-e801-46e4f3b8e1d7","trusted":true,"execution":{"iopub.status.busy":"2025-12-18T05:29:39.300460Z","iopub.execute_input":"2025-12-18T05:29:39.301245Z","iopub.status.idle":"2025-12-18T05:29:40.192506Z","shell.execute_reply.started":"2025-12-18T05:29:39.301210Z","shell.execute_reply":"2025-12-18T05:29:40.191594Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 1600x250 with 1 Axes>","image/png":"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\n"},"metadata":{}}],"execution_count":4},{"cell_type":"markdown","source":"---\n\n# Model","metadata":{"id":"jZeNU8T5wdRh"}},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass PatchEmbeddingConv(nn.Module):\n    \"\"\"\n    Patch embedding estilo Swin.\n\n    - Conv2d con kernel=stride=patch_size para convertir imagen -> grilla de patches.\n    - Devuelve el mapa 2D en formato canal-al-final: [B, Hp, Wp, D],\n      (más cómodo para window partition).\n    - Opcionalmente devuelve tokens [B, N, D].\n    - Opcional padding automático si H/W no son divisibles por patch_size.\n    \"\"\"\n\n    def __init__(\n        self,\n        patch_size: int | tuple[int, int] = 4,\n        in_chans: int = 3,\n        embed_dim: int = 192,\n        norm_layer: type[nn.Module] | None = nn.LayerNorm,\n        pad_if_needed: bool = True,\n        return_tokens: bool = True):\n\n        super().__init__()\n\n        if isinstance(patch_size, int):\n            patch_size = (patch_size, patch_size)\n\n        self.patch_size = patch_size  # (Ph, Pw)\n        self.in_chans = in_chans\n        self.embed_dim = embed_dim\n        self.pad_if_needed = pad_if_needed\n        self.return_tokens = return_tokens\n\n        # [B, C, H, W] -> [B, D, Hp, Wp]\n        self.proj = nn.Conv2d(\n            in_channels=in_chans,\n            out_channels=embed_dim,\n            kernel_size=patch_size,\n            stride=patch_size,\n            bias=True,)\n\n        # En Swin normalmente LayerNorm sobre la última dimensión\n        self.norm = norm_layer(embed_dim) if norm_layer is not None else None\n\n    def forward(self, x: torch.Tensor):\n        \"\"\"\n        Args:\n            x: [B, C, H, W]\n\n        Returns:\n            x_map:    [B, Hp, Wp, D]\n            (Hp, Wp): tamaño espacial en patches\n            x_tokens (opcional): [B, N, D]\n            pad_hw (opcional): (pad_h, pad_w) aplicados a la imagen\n        \"\"\"\n        B, C, H, W = x.shape\n        Ph, Pw = self.patch_size\n\n        pad_h = (Ph - (H % Ph)) % Ph\n        pad_w = (Pw - (W % Pw)) % Pw\n\n        if (pad_h != 0 or pad_w != 0):\n            if not self.pad_if_needed:\n                raise AssertionError(\n                    f\"Image size ({H}x{W}) no es divisible por patch_size {self.patch_size} \"\n                    f\"y pad_if_needed=False.\")\n\n            x = F.pad(x, (0, pad_w, 0, pad_h))\n\n        # [B, D, Hp, Wp]\n        x = self.proj(x)\n        Hp, Wp = x.shape[2], x.shape[3]\n\n        # canal al final -> [B, Hp, Wp, D]\n        x_map = x.permute(0, 2, 3, 1).contiguous()\n\n        if self.norm is not None:\n            x_map = self.norm(x_map)\n\n        if self.return_tokens:\n            x_tokens = x_map.view(B, Hp * Wp, self.embed_dim)\n            return x_map, (Hp, Wp), x_tokens, (pad_h, pad_w)\n\n        return x_map, (Hp, Wp), (pad_h, pad_w)","metadata":{"id":"vxPr-iDHuMiC","trusted":true,"execution":{"iopub.status.busy":"2025-12-18T05:29:45.906660Z","iopub.execute_input":"2025-12-18T05:29:45.907405Z","iopub.status.idle":"2025-12-18T05:29:45.915932Z","shell.execute_reply.started":"2025-12-18T05:29:45.907379Z","shell.execute_reply":"2025-12-18T05:29:45.915137Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"import torch\nimport torch.nn.functional as F\n\ndef pad_to_window_size(x: torch.Tensor, window_size: int, pad_value: float = 0.0):\n    \"\"\"\n    Asegura que H y W sean múltiplos de window_size mediante padding (bottom/right).\n\n    Args:\n        x: [B, H, W, C]\n        window_size: int\n        pad_value: valor de relleno\n\n    Returns:\n        x_pad: [B, Hp, Wp, C]\n        pad_hw: (pad_h, pad_w)\n        orig_hw: (H, W)\n    \"\"\"\n    assert x.dim() == 4, \"x debe ser [B, H, W, C]\"\n    B, H, W, C = x.shape\n\n    pad_h = (window_size - (H % window_size)) % window_size\n    pad_w = (window_size - (W % window_size)) % window_size\n\n    if pad_h == 0 and pad_w == 0:\n        return x, (0, 0), (H, W)\n\n    # F.pad para tensores 4D en este orden: (..., C) => (pad_C_left, pad_C_right, pad_W_left, pad_W_right, pad_H_left, pad_H_right)\n    x_pad = F.pad(x, (0, 0, 0, pad_w, 0, pad_h), value=pad_value)\n    return x_pad, (pad_h, pad_w), (H, W)\n\ndef unpad_from_window_size(x_pad: torch.Tensor, orig_hw: tuple[int, int]):\n    \"\"\"\n    Revierte el padding, recortando a (H, W) original.\n\n    Args:\n        x_pad: [B, Hp, Wp, C]\n        orig_hw: (H, W)\n\n    Returns:\n        x: [B, H, W, C]\n    \"\"\"\n    H, W = orig_hw\n    return x_pad[:, :H, :W, :]\n\ndef window_partition(x: torch.Tensor, window_size: int):\n    \"\"\"\n    Divide [B, H, W, C] en ventanas no solapadas.\n\n    Args:\n        x: [B, H, W, C] con H, W múltiplos de window_size\n        window_size: int\n\n    Returns:\n        windows: [num_windows * B, window_size, window_size, C]\n    \"\"\"\n    assert x.dim() == 4, \"x debe ser [B, H, W, C]\"\n\n    B, H, W, C = x.shape\n\n    assert H % window_size == 0 and W % window_size == 0, \\\n        f\"H,W deben ser múltiplos de window_size. Got {(H, W)} vs {window_size}\"\n\n    x = x.view(B,\n        H // window_size, window_size,\n        W // window_size, window_size, C)\n\n    # [B, nH, ws, nW, ws, C] -> [B, nH, nW, ws, ws, C] -> [B*nH*nW, ws, ws, C]\n    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)\n    return windows\n\n\n\ndef window_reverse(windows: torch.Tensor, window_size: int, H: int, W: int, B: int | None = None):\n    \"\"\"\n    Reconstruye [B, H, W, C] a partir de ventanas.\n\n    Args:\n        windows: [num_windows * B, window_size, window_size, C]\n        window_size: int\n        H, W: alto y ancho del mapa (padded) que queremos reconstruir\n        B: batch size (si None, se infiere)\n\n    Returns:\n        x: [B, H, W, C]\n    \"\"\"\n    assert windows.dim() == 4, \"windows debe ser [B*nW, ws, ws, C]\"\n    nBW, ws1, ws2, C = windows.shape\n    assert ws1 == window_size and ws2 == window_size\n\n    nH = H // window_size\n    nW = W // window_size\n\n    if B is None:\n        assert nBW % (nH * nW) == 0, \"No puedo inferir B: shapes incompatibles.\"\n        B = nBW // (nH * nW)\n\n    x = windows.view(B, nH, nW, window_size, window_size, C)\n    # [B, nH, nW, ws, ws, C] -> [B, nH, ws, nW, ws, C] -> [B, H, W, C]\n    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, H, W, C)\n    return x\n","metadata":{"id":"BqBtGxfUxFrY","trusted":true,"execution":{"iopub.status.busy":"2025-12-18T05:29:47.566298Z","iopub.execute_input":"2025-12-18T05:29:47.567036Z","iopub.status.idle":"2025-12-18T05:29:47.579466Z","shell.execute_reply.started":"2025-12-18T05:29:47.567007Z","shell.execute_reply":"2025-12-18T05:29:47.578862Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"def cyclic_shift(x: torch.Tensor, shift_size: int | tuple[int, int]):\n    \"\"\"\n    Swin usa un shift circular (torch.roll) en H y W.\n\n    Args:\n        x: [B, H, W, C]\n        shift_size: int o (shift_h, shift_w)\n            - shift positivo desplaza hacia abajo/derecha\n            - shift negativo hacia arriba/izquierda (lo típico en SW-MSA es negativo)\n\n    Returns:\n        x_shifted: [B, H, W, C]\n    \"\"\"\n    if isinstance(shift_size, int):\n        shift_h, shift_w = shift_size, shift_size\n    else:\n        shift_h, shift_w = shift_size\n    return torch.roll(x, shifts=(shift_h, shift_w), dims=(1, 2))\n\n\n@torch.no_grad()\ndef build_shifted_window_attention_mask(\n    H: int,\n    W: int,\n    window_size: int,\n    shift_size: int,\n    device: torch.device,\n    dtype: torch.dtype = torch.float32):\n    \"\"\"\n    Crea la máscara que evita que, tras el shift, una ventana atienda a tokens que\n    originalmente venían de regiones distintas (bordes \"cortados\" por el shift).\n\n    Convención:\n      - máscara devuelve shape [num_windows, ws*ws, ws*ws]\n      - valores: 0 para permitido, -inf (o gran negativo) para bloqueado\n\n    Nota: H y W deben ser múltiplos de window_size (usa pad_to_window_size antes).\n\n    Args:\n        H, W: dimensiones (padded)\n        window_size: ws\n        shift_size: ss (0 < ss < ws)\n        device: device\n        dtype: dtype (float)\n\n    Returns:\n        attn_mask: [num_windows, ws*ws, ws*ws]\n    \"\"\"\n    assert 0 < shift_size < window_size, \"shift_size debe estar en (0, window_size).\"\n\n    img_mask = torch.zeros((1, H, W, 1), device=device, dtype=torch.int64)\n\n    # Dividimos H y W en 3 bandas: [0:-ws], [-ws:-ss], [-ss:]\n    h_slices = (slice(0, -window_size),\n        slice(-window_size, -shift_size),\n        slice(-shift_size, None),)\n\n    w_slices = (slice(0, -window_size),\n        slice(-window_size, -shift_size),\n        slice(-shift_size, None),)\n\n    cnt = 0\n    for h in h_slices:\n        for w in w_slices:\n            img_mask[:, h, w, :] = cnt\n            cnt += 1\n\n    # Particionamos esa máscara en ventanas\n    mask_windows = window_partition(img_mask, window_size)\n    mask_windows = mask_windows.view(-1, window_size * window_size)\n\n    # Diferencias: si id distinto => bloquear atención\n    attn_mask = mask_windows.unsqueeze(1) - mask_windows.unsqueeze(2)\n    attn_mask = attn_mask.ne(0)  # bool\n\n    # Convertimos a float con -inf para posiciones bloqueadas\n    neg_inf = torch.finfo(dtype).min\n    attn_mask = attn_mask.to(dtype) * neg_inf\n    return attn_mask","metadata":{"id":"i9puKS55xn-j","trusted":true,"execution":{"iopub.status.busy":"2025-12-18T05:29:49.470990Z","iopub.execute_input":"2025-12-18T05:29:49.471703Z","iopub.status.idle":"2025-12-18T05:29:49.479430Z","shell.execute_reply.started":"2025-12-18T05:29:49.471679Z","shell.execute_reply":"2025-12-18T05:29:49.478737Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"def prepare_windows(\n    x: torch.Tensor,\n    window_size: int,\n    shift_size: int = 0,\n    pad_value: float = 0.0):\n    \"\"\"\n    Prepara todo lo necesario \"antes de la atención\":\n    - padding a múltiplos de window_size\n    - shift opcional (SW-MSA)\n    - window_partition\n    - attention mask (si shift_size > 0)\n\n    Args:\n        x: [B, H, W, C]\n        window_size: ws\n        shift_size: ss (0 para W-MSA, >0 para SW-MSA)\n        pad_value: padding\n\n    Returns:\n        windows_flat: [B*num_windows, ws*ws, C]  (listo para entrar a atención)\n        meta: dict con info para reconstruir (reverse)\n              incluye (Hp, Wp), (H, W), pad_hw, shift_size, attn_mask\n    \"\"\"\n    assert x.dim() == 4, \"x debe ser [B, H, W, C]\"\n    B, H, W, C = x.shape\n    device = x.device\n\n    x_pad, pad_hw, orig_hw = pad_to_window_size(x, window_size, pad_value=pad_value)\n    _, Hp, Wp, _ = x_pad.shape\n\n    attn_mask = None\n    if shift_size > 0:\n        x_pad = cyclic_shift(x_pad, (-shift_size, -shift_size))  # shift negativo (convención Swin)\n\n        attn_mask = build_shifted_window_attention_mask(\n            H=Hp, W=Wp,\n            window_size=window_size,\n            shift_size=shift_size,\n            device=device,\n            dtype=torch.float32, )\n\n    # [B*nW, ws, ws, C]\n    windows = window_partition(x_pad, window_size)\n    # [B*nW, ws*ws, C]  (forma típica para pasar a atención)\n    windows_flat = windows.view(-1, window_size * window_size, C)\n\n    meta = {\n        \"B\": B,\n        \"orig_hw\": orig_hw,   # (H, W) original\n        \"pad_hw\": pad_hw,     # (pad_h, pad_w)\n        \"HpWp\": (Hp, Wp),     # (Hp, Wp) padded\n        \"window_size\": window_size,\n        \"shift_size\": shift_size,\n        \"attn_mask\": attn_mask}\n\n    return windows_flat, meta\n\ndef restore_from_windows(\n    windows_flat: torch.Tensor,\n    meta: dict,\n    C: int):\n\n    \"\"\"\n    Revierte la preparación:\n    - windows_flat -> windows -> window_reverse -> unshift (si aplica) -> unpad\n\n    Args:\n        windows_flat: [B*nW, ws*ws, C]\n        meta: dict devuelto por prepare_windows\n        C: canales\n\n    Returns:\n        x: [B, H, W, C] (H,W original)\n    \"\"\"\n    B = meta[\"B\"]\n    (Hp, Wp) = meta[\"HpWp\"]\n    ws = meta[\"window_size\"]\n    ss = meta[\"shift_size\"]\n    orig_hw = meta[\"orig_hw\"]\n\n    windows = windows_flat.view(-1, ws, ws, C)\n    x_pad = window_reverse(windows, ws, Hp, Wp, B=B)\n\n    if ss > 0:\n        x_pad = cyclic_shift(x_pad, (ss, ss))  # deshacer shift\n\n    x = unpad_from_window_size(x_pad, orig_hw)\n    return x","metadata":{"id":"v8ttJuo6xyTQ","trusted":true,"execution":{"iopub.status.busy":"2025-12-18T05:29:51.539994Z","iopub.execute_input":"2025-12-18T05:29:51.540587Z","iopub.status.idle":"2025-12-18T05:29:51.548172Z","shell.execute_reply.started":"2025-12-18T05:29:51.540548Z","shell.execute_reply":"2025-12-18T05:29:51.547492Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"B, H, W, C = 2, 15, 17, 96\nws = 7\nss = ws // 2\n\nx = torch.randn(B, H, W, C)\n\n# SW-MSA prep\nwin, meta = prepare_windows(x, window_size=ws, shift_size=ss)\nmask = meta[\"attn_mask\"]\nprint(win.shape)      # [B*nW, ws*ws, C]\nprint(mask.shape)     # [nW, ws*ws, ws*ws]  (si ss>0)\n\n# \"Simulamos\" salida de atención: misma forma\ny = win.clone()\n\n# Reverse\nx_rec = restore_from_windows(y, meta, C=C)\nprint(x_rec.shape)    # [B, H, W, C]","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ue5xl6Hsx8sl","outputId":"ce3185e3-1a5a-430c-b2dc-eb275d44cb35","trusted":true,"execution":{"iopub.status.busy":"2025-12-18T05:29:53.599994Z","iopub.execute_input":"2025-12-18T05:29:53.600325Z","iopub.status.idle":"2025-12-18T05:29:53.623939Z","shell.execute_reply.started":"2025-12-18T05:29:53.600302Z","shell.execute_reply":"2025-12-18T05:29:53.623082Z"}},"outputs":[{"name":"stdout","text":"torch.Size([18, 49, 96])\ntorch.Size([9, 49, 49])\ntorch.Size([2, 15, 17, 96])\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass WindowAttention(nn.Module):\n    \"\"\"\n    Window Multi-Head Self-Attention (W-MSA / SW-MSA).\n\n    Entrada:\n      x: [B*nW, N, C]  donde N = ws*ws\n      mask (opcional): [nW, N, N] con 0 permitido y -inf bloqueado (tu máscara Swin)\n    \"\"\"\n\n    def __init__(\n        self,\n        dim: int,\n        window_size: int,\n        num_heads: int,\n        qkv_bias: bool = True,\n        attn_dropout: float = 0.0,\n        proj_dropout: float = 0.0,\n        use_rel_pos_bias: bool = True):\n\n        super().__init__()\n        assert dim % num_heads == 0, \"dim debe ser múltiplo de num_heads\"\n\n        self.dim = dim\n        self.window_size = window_size\n        self.num_heads = num_heads\n        self.head_dim = dim // num_heads\n        self.scale = self.head_dim ** -0.5\n        self.use_rel_pos_bias = use_rel_pos_bias\n\n        self.qkv = nn.Linear(dim, 3 * dim, bias=qkv_bias)\n        self.attn_drop = nn.Dropout(attn_dropout)\n        self.proj = nn.Linear(dim, dim)\n        self.proj_drop = nn.Dropout(proj_dropout)\n\n        if use_rel_pos_bias:\n            ws = window_size\n            num_rel = (2 * ws - 1) * (2 * ws - 1)\n            self.rel_pos_bias_table = nn.Parameter(torch.zeros(num_rel, num_heads))\n\n            # índice relativo [N, N] para mapear pares (i,j) -> entrada en la tabla\n            coords_h = torch.arange(ws)\n            coords_w = torch.arange(ws)\n            coords = torch.stack(torch.meshgrid(coords_h, coords_w, indexing=\"ij\"))\n            coords_flat = coords.flatten(1)\n            rel_coords = coords_flat[:, :, None] - coords_flat[:, None, :]\n            rel_coords = rel_coords.permute(1, 2, 0).contiguous()\n            rel_coords[:, :, 0] += ws - 1\n            rel_coords[:, :, 1] += ws - 1\n            rel_coords[:, :, 0] *= (2 * ws - 1)\n            rel_pos_index = rel_coords.sum(-1)  #\n            self.register_buffer(\"rel_pos_index\", rel_pos_index, persistent=False)\n\n            nn.init.trunc_normal_(self.rel_pos_bias_table, std=0.02)\n\n    def forward(self, x: torch.Tensor, mask: torch.Tensor | None = None):\n        \"\"\"\n        x: [BnW, N, C]\n        mask: [nW, N, N] float con 0 / -inf\n        \"\"\"\n        BnW, N, C = x.shape\n\n        qkv = self.qkv(x)\n        qkv = qkv.view(BnW, N, 3, self.num_heads, self.head_dim).permute(2, 0, 3, 1, 4)\n        q, k, v = qkv[0], qkv[1], qkv[2]\n\n        q = q * self.scale\n        attn = q @ k.transpose(-2, -1)\n\n        if self.use_rel_pos_bias:\n            rel_bias = self.rel_pos_bias_table[self.rel_pos_index.view(-1)]\n            rel_bias = rel_bias.view(N, N, self.num_heads).permute(2, 0, 1).contiguous()\n            attn = attn + rel_bias.unsqueeze(0)\n\n        if mask is not None:\n            # mask: [nW, N, N] -> se aplica por ventana compartir en batch\n            nW = mask.shape[0]\n            assert BnW % nW == 0, \"BnW debe ser múltiplo de nW para aplicar máscara Swin.\"\n\n            B = BnW // nW\n\n            attn = attn.view(B, nW, self.num_heads, N, N)\n            attn = attn + mask.unsqueeze(0).unsqueeze(2)\n            attn = attn.view(BnW, self.num_heads, N, N)\n\n        attn = F.softmax(attn, dim=-1)\n        attn = self.attn_drop(attn)\n\n        out = attn @ v\n        out = out.transpose(1, 2).contiguous().view(BnW, N, C)\n        out = self.proj(out)\n        out = self.proj_drop(out)\n        return out\n","metadata":{"id":"v128fFnnyqDR","trusted":true,"execution":{"iopub.status.busy":"2025-12-18T05:29:54.923254Z","iopub.execute_input":"2025-12-18T05:29:54.923757Z","iopub.status.idle":"2025-12-18T05:29:54.935419Z","shell.execute_reply.started":"2025-12-18T05:29:54.923733Z","shell.execute_reply":"2025-12-18T05:29:54.934714Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"torch.manual_seed(0)\n\nB, H, W, C = 2, 15, 17, 96\nws = 7\nss = ws // 2\nnum_heads = 3\n\nx = torch.randn(B, H, W, C)\n\nattn = WindowAttention(dim=C, window_size=ws, num_heads=num_heads,\n                       attn_dropout=0.0, proj_dropout=0.0, use_rel_pos_bias=True)\n\n#  W-MSA (sin shift, sin máscara)\nwin_w, meta_w = prepare_windows(x, window_size=ws, shift_size=0)\ny_w = attn(win_w, mask=meta_w[\"attn_mask\"])\n\nprint(\"W-MSA windows in :\", win_w.shape)     # [B*nW, ws*ws, C]\nprint(\"W-MSA windows out:\", y_w.shape)       # misma shape\nassert y_w.shape == win_w.shape\n\n#  SW-MSA (con shift + máscara)\nwin_sw, meta_sw = prepare_windows(x, window_size=ws, shift_size=ss)\nmask = meta_sw[\"attn_mask\"]\ny_sw = attn(win_sw, mask=mask)\n\nprint(\"SW-MSA windows in :\", win_sw.shape)\nprint(\"SW-MSA mask       :\", None if mask is None else mask.shape)  # [nW, N, N]\nprint(\"SW-MSA windows out:\", y_sw.shape)\nassert y_sw.shape == win_sw.shape\nassert mask is not None and mask.ndim == 3\n\n# mask sanity: debe tener 0 y valores muy negativos\nm_min = float(mask.min())\nm_max = float(mask.max())\nprint(\"Mask min/max:\", m_min, m_max)\nassert m_max == 0.0, \"La máscara Swin debe tener 0 en posiciones permitidas.\"\nassert m_min < -1e20, \"La máscara debería tener valores muy negativos (-inf aprox).\"\n\n#  grad sanity\nwin_sw.requires_grad_(True)\ny = attn(win_sw, mask=mask)\nloss = y.mean()\nloss.backward()\ng = win_sw.grad\nprint(\"Grad ok?  finite:\", torch.isfinite(g).all().item(), \" mean|g|:\", g.abs().mean().item())\nassert torch.isfinite(g).all(), \"Gradientes no finitos.\"\n\nprint(\"Sanity check WindowAttention OK\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CB2GhpsfznYH","outputId":"627b0751-937d-46f8-b348-fafbd8859cb2","trusted":true,"execution":{"iopub.status.busy":"2025-12-18T05:29:56.895402Z","iopub.execute_input":"2025-12-18T05:29:56.895706Z","iopub.status.idle":"2025-12-18T05:29:56.974024Z","shell.execute_reply.started":"2025-12-18T05:29:56.895683Z","shell.execute_reply":"2025-12-18T05:29:56.973078Z"}},"outputs":[{"name":"stdout","text":"W-MSA windows in : torch.Size([18, 49, 96])\nW-MSA windows out: torch.Size([18, 49, 96])\nSW-MSA windows in : torch.Size([18, 49, 96])\nSW-MSA mask       : torch.Size([9, 49, 49])\nSW-MSA windows out: torch.Size([18, 49, 96])\nMask min/max: -3.4028234663852886e+38 -0.0\nGrad ok?  finite: True  mean|g|: 3.2663756428519264e-06\nSanity check WindowAttention OK\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\n\n\nclass DropPath(nn.Module):\n    \"\"\"Stochastic Depth (por muestra).\"\"\"\n    def __init__(self, drop_prob: float = 0.0):\n        super().__init__()\n        self.drop_prob = float(drop_prob)\n\n    def forward(self, x: torch.Tensor):\n        if self.drop_prob == 0.0 or not self.training:\n            return x\n        keep_prob = 1.0 - self.drop_prob\n        # shape: [B, 1, 1, 1] para [B,H,W,C]\n        shape = (x.shape[0],) + (1,) * (x.dim() - 1)\n        rand = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n        mask = torch.floor(rand)\n        return x / keep_prob * mask\n\n\nclass MLP(nn.Module):\n    def __init__(self, dim: int, hidden_dim: int, dropout: float = 0.0):\n        super().__init__()\n        self.fc1 = nn.Linear(dim, hidden_dim)\n        self.act = nn.GELU()\n        self.fc2 = nn.Linear(hidden_dim, dim)\n        self.drop = nn.Dropout(dropout)\n\n    def forward(self, x: torch.Tensor):\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.drop(x)\n        x = self.fc2(x)\n        x = self.drop(x)\n        return x\n\n\nclass SwinTransformerBlock(nn.Module):\n    \"\"\"\n    Bloque Swin completo (pre-norm) en formato [B, H, W, C].\n\n    Usa:\n      - prepare_windows(x, window_size, shift_size) -> (windows_flat, meta)\n      - restore_from_windows(windows_flat, meta, C) -> x_rec\n      - WindowAttention(dim, window_size, num_heads)(windows_flat, mask)\n    \"\"\"\n\n    def __init__(\n        self,\n        dim: int,\n        num_heads: int,\n        window_size: int = 7,\n        shift_size: int = 0,\n        mlp_ratio: float = 4.0,\n        attn_dropout: float = 0.0,\n        proj_dropout: float = 0.0,\n        mlp_dropout: float = 0.0,\n        drop_path: float = 0.0,\n        use_rel_pos_bias: bool = True):\n\n        super().__init__()\n        assert 0 <= shift_size < window_size, \"shift_size debe estar en [0, window_size)\"\n        self.dim = dim\n        self.window_size = window_size\n        self.shift_size = shift_size\n\n        self.norm1 = nn.LayerNorm(dim)\n\n        self.attn = WindowAttention(\n            dim=dim,\n            window_size=window_size,\n            num_heads=num_heads,\n            qkv_bias=True,\n            attn_dropout=attn_dropout,\n            proj_dropout=proj_dropout,\n            use_rel_pos_bias=use_rel_pos_bias,)\n\n        self.drop_path1 = DropPath(drop_path)\n\n        self.norm2 = nn.LayerNorm(dim)\n        hidden_dim = int(dim * mlp_ratio)\n        self.mlp = MLP(dim=dim, hidden_dim=hidden_dim, dropout=mlp_dropout)\n        self.drop_path2 = DropPath(drop_path)\n\n    def forward(self, x: torch.Tensor):\n        \"\"\"\n        x: [B, H, W, C]\n        \"\"\"\n        B, H, W, C = x.shape\n        assert C == self.dim, f\"Canales C={C} != dim={self.dim}\"\n        assert H >= self.window_size and W >= self.window_size, \\\n            f\"H,W deben ser >= window_size ({self.window_size}). Got {(H, W)}.\"\n\n        #  Window Attention\n        shortcut = x\n        x_norm = self.norm1(x)\n\n        windows_flat, meta = prepare_windows(\n            x_norm,\n            window_size=self.window_size,\n            shift_size=self.shift_size,\n            pad_value=0.0)\n\n        attn_mask = meta[\"attn_mask\"]  # None o [nW, N, N] con 0/-inf\n\n        out_windows = self.attn(windows_flat, mask=attn_mask)  # [B*nW, N, C]\n        x_attn = restore_from_windows(out_windows, meta, C=C)  # [B, H, W, C]\n\n        x = shortcut + self.drop_path1(x_attn)\n\n        # MLP\n        x = x + self.drop_path2(self.mlp(self.norm2(x)))\n\n        return x\n","metadata":{"id":"SYSpdNpQ0RTs","trusted":true,"execution":{"iopub.status.busy":"2025-12-18T05:29:58.444627Z","iopub.execute_input":"2025-12-18T05:29:58.444956Z","iopub.status.idle":"2025-12-18T05:29:58.456454Z","shell.execute_reply.started":"2025-12-18T05:29:58.444936Z","shell.execute_reply":"2025-12-18T05:29:58.455630Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"torch.manual_seed(0)\nx = torch.randn(2, 15, 17, 96)\n\nblk = SwinTransformerBlock(dim=96, num_heads=3, window_size=7, shift_size=3,\n                           attn_dropout=0.0, proj_dropout=0.0, mlp_dropout=0.0, drop_path=0.0)\n\ny = blk(x)\nprint(x.shape, y.shape)\nassert y.shape == x.shape\nprint(\"SwinTransformerBlock OK\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6o5yL4Ez0lxc","outputId":"76a2709f-dee5-4427-8e84-dd01eab17dc5","trusted":true,"execution":{"iopub.status.busy":"2025-12-18T05:30:01.199706Z","iopub.execute_input":"2025-12-18T05:30:01.199972Z","iopub.status.idle":"2025-12-18T05:30:01.244121Z","shell.execute_reply.started":"2025-12-18T05:30:01.199953Z","shell.execute_reply":"2025-12-18T05:30:01.243203Z"}},"outputs":[{"name":"stdout","text":"torch.Size([2, 15, 17, 96]) torch.Size([2, 15, 17, 96])\nSwinTransformerBlock OK\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass PatchMerging(nn.Module):\n    \"\"\"\n    Patch Merging (Swin): [B, H, W, C] -> [B, H/2, W/2, 2C]\n\n    Pasos:\n      1) (opcional) pad si H o W es impar\n      2) agrupa 2x2 tokens y concatena canales -> 4C\n      3) LayerNorm(4C)\n      4) Linear(4C -> 2C)\n    \"\"\"\n\n    def __init__(self, dim: int, out_dim: int | None = None, norm_layer=nn.LayerNorm):\n        super().__init__()\n        self.dim = dim\n        self.out_dim = out_dim if out_dim is not None else 2 * dim\n\n        self.norm = norm_layer(4 * dim)\n        self.reduction = nn.Linear(4 * dim, self.out_dim, bias=False)\n\n    def forward(self, x: torch.Tensor):\n        \"\"\"\n        x: [B, H, W, C]\n        \"\"\"\n        assert x.dim() == 4, \"x debe ser [B, H, W, C]\"\n        B, H, W, C = x.shape\n        assert C == self.dim, f\"C={C} != dim={self.dim}\"\n\n        # Pad si H o W es impar (Swin lo hace para poder agrupar 2x2)\n        pad_h = H % 2\n        pad_w = W % 2\n        if pad_h != 0 or pad_w != 0:\n            x = F.pad(x, (0, 0, 0, pad_w, 0, pad_h))\n            H = H + pad_h\n            W = W + pad_w\n\n        # Tomar 2x2:\n        # x0 = (even rows, even cols), x1 = (odd rows, even cols), x2 = (even rows, odd cols), x3 = (odd rows, odd cols)\n        x0 = x[:, 0::2, 0::2, :]  # [B, H/2, W/2, C]\n        x1 = x[:, 1::2, 0::2, :]  # [B, H/2, W/2, C]\n        x2 = x[:, 0::2, 1::2, :]  # [B, H/2, W/2, C]\n        x3 = x[:, 1::2, 1::2, :]  # [B, H/2, W/2, C]\n\n        # Concatenar en canal: [B, H/2, W/2, 4C]\n        x = torch.cat([x0, x1, x2, x3], dim=-1)\n\n        # Norm + Linear: [B, H/2, W/2, 2C]\n        x = self.norm(x)\n        x = self.reduction(x)\n\n        return x","metadata":{"id":"-oA2jaX61HBu","trusted":true,"execution":{"iopub.status.busy":"2025-12-18T05:30:02.386160Z","iopub.execute_input":"2025-12-18T05:30:02.386426Z","iopub.status.idle":"2025-12-18T05:30:02.393882Z","shell.execute_reply.started":"2025-12-18T05:30:02.386407Z","shell.execute_reply":"2025-12-18T05:30:02.393085Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"torch.manual_seed(0)\nx = torch.randn(2, 15, 17, 96)\n\npm = PatchMerging(dim=96)\ny = pm(x)\n\nprint(\"in :\", x.shape)\nprint(\"out:\", y.shape)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vphxoA2w1UHG","outputId":"c161ca12-1aea-49ea-825d-bb449be74ca4","trusted":true,"execution":{"iopub.status.busy":"2025-12-18T05:30:04.066248Z","iopub.execute_input":"2025-12-18T05:30:04.066504Z","iopub.status.idle":"2025-12-18T05:30:04.077029Z","shell.execute_reply.started":"2025-12-18T05:30:04.066485Z","shell.execute_reply":"2025-12-18T05:30:04.076356Z"}},"outputs":[{"name":"stdout","text":"in : torch.Size([2, 15, 17, 96])\nout: torch.Size([2, 8, 9, 192])\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"\nclass BasicLayer(nn.Module):\n    \"\"\"\n    Un stage de Swin:\n      - depth bloques SwinTransformerBlock\n      - alterna shift: 0, ws//2, 0, ws//2, ...\n      - downsample opcional al final (PatchMerging) excepto en el último stage\n    \"\"\"\n    def __init__(\n        self,\n        dim: int,\n        depth: int,\n        num_heads: int,\n        window_size: int,\n        mlp_ratio: float = 4.0,\n        attn_dropout: float = 0.0,\n        proj_dropout: float = 0.0,\n        mlp_dropout: float = 0.0,\n        drop_path_rates: list[float] | None = None,\n        downsample: nn.Module | None = None,\n        use_rel_pos_bias: bool = True):\n\n        super().__init__()\n        self.dim = dim\n        self.depth = depth\n        self.window_size = window_size\n        self.downsample = downsample\n\n        if drop_path_rates is None:\n            drop_path_rates = [0.0] * depth\n        assert len(drop_path_rates) == depth, \"drop_path_rates debe tener longitud = depth\"\n\n        blocks = []\n        for i in range(depth):\n            shift_size = 0 if (i % 2 == 0) else (window_size // 2)\n            blocks.append(\n                SwinTransformerBlock(\n                    dim=dim,\n                    num_heads=num_heads,\n                    window_size=window_size,\n                    shift_size=shift_size,\n                    mlp_ratio=mlp_ratio,\n                    attn_dropout=attn_dropout,\n                    proj_dropout=proj_dropout,\n                    mlp_dropout=mlp_dropout,\n                    drop_path=drop_path_rates[i],\n                    use_rel_pos_bias=use_rel_pos_bias,\n                ))\n        self.blocks = nn.ModuleList(blocks)\n\n    def forward(self, x: torch.Tensor):\n        \"\"\"\n        x: [B, H, W, C]\n        \"\"\"\n        for blk in self.blocks:\n            x = blk(x)\n\n        x_down = None\n        if self.downsample is not None:\n            x_down = self.downsample(x)\n        return x, x_down","metadata":{"id":"ppGQjqlV1bwK","trusted":true,"execution":{"iopub.status.busy":"2025-12-18T05:30:05.309644Z","iopub.execute_input":"2025-12-18T05:30:05.309944Z","iopub.status.idle":"2025-12-18T05:30:05.317659Z","shell.execute_reply.started":"2025-12-18T05:30:05.309924Z","shell.execute_reply":"2025-12-18T05:30:05.316774Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"import math\n\nclass SwinTransformer(nn.Module):\n    def __init__(\n        self,\n        img_size: int | tuple[int, int] = 224,\n        patch_size: int = 4,\n        in_chans: int = 3,\n        num_classes: int = 100,\n        embed_dim: int = 96,\n        depths: tuple[int, int, int, int] = (2, 2, 6, 2),\n        num_heads: tuple[int, int, int, int] = (3, 6, 12, 24),\n        window_size: int = 7,\n        mlp_ratio: float = 4.0,\n        drop_rate: float = 0.0,\n        attn_dropout: float = 0.0,\n        proj_dropout: float = 0.0,\n        mlp_dropout: float = 0.0,\n        drop_path_rate: float = 0.1,\n        use_rel_pos_bias: bool = True,):\n        super().__init__()\n\n        if isinstance(img_size, int):\n            img_h, img_w = img_size, img_size\n        else:\n            img_h, img_w = img_size\n\n        self.patch_embed = PatchEmbeddingConv(\n            patch_size=patch_size,\n            in_chans=in_chans,\n            embed_dim=embed_dim,\n            norm_layer=nn.LayerNorm,\n            pad_if_needed=True,\n            return_tokens=False)\n\n        self.pos_drop = nn.Dropout(drop_rate)\n\n        # ---- window_size por stage (clip a la resolución del stage) ----\n        Hp = math.ceil(img_h / patch_size)\n        Wp = math.ceil(img_w / patch_size)\n        stage_res = [(math.ceil(Hp / (2**i)), math.ceil(Wp / (2**i))) for i in range(4)]\n        ws = [max(1, min(window_size, h, w)) for (h, w) in stage_res]\n        self.window_sizes = tuple(ws)  # para debug\n        # ---------------------------------------------------------------\n\n        total_blocks = sum(depths)\n        dpr = torch.linspace(0, drop_path_rate, total_blocks).tolist()\n\n        dims = [embed_dim, embed_dim * 2, embed_dim * 4, embed_dim * 8]\n\n        # Stage 1\n        self.layer1 = BasicLayer(\n            dim=dims[0], depth=depths[0], num_heads=num_heads[0],\n            window_size=ws[0], mlp_ratio=mlp_ratio,\n            attn_dropout=attn_dropout, proj_dropout=proj_dropout, mlp_dropout=mlp_dropout,\n            drop_path_rates=dpr[0:depths[0]],\n            downsample=PatchMerging(dim=dims[0], out_dim=dims[1]),\n            use_rel_pos_bias=use_rel_pos_bias,)\n\n        # Stage 2\n        idx = depths[0]\n        self.layer2 = BasicLayer(\n            dim=dims[1], depth=depths[1], num_heads=num_heads[1],\n            window_size=ws[1], mlp_ratio=mlp_ratio,\n            attn_dropout=attn_dropout, proj_dropout=proj_dropout, mlp_dropout=mlp_dropout,\n            drop_path_rates=dpr[idx:idx + depths[1]],\n            downsample=PatchMerging(dim=dims[1], out_dim=dims[2]),\n            use_rel_pos_bias=use_rel_pos_bias,)\n\n        # Stage 3\n        idx += depths[1]\n        self.layer3 = BasicLayer(\n            dim=dims[2], depth=depths[2], num_heads=num_heads[2],\n            window_size=ws[2], mlp_ratio=mlp_ratio,\n            attn_dropout=attn_dropout, proj_dropout=proj_dropout, mlp_dropout=mlp_dropout,\n            drop_path_rates=dpr[idx:idx + depths[2]],\n            downsample=PatchMerging(dim=dims[2], out_dim=dims[3]),\n            use_rel_pos_bias=use_rel_pos_bias,)\n\n        # Stage 4\n        idx += depths[2]\n        self.layer4 = BasicLayer(\n            dim=dims[3], depth=depths[3], num_heads=num_heads[3],\n            window_size=ws[3], mlp_ratio=mlp_ratio,\n            attn_dropout=attn_dropout, proj_dropout=proj_dropout, mlp_dropout=mlp_dropout,\n            drop_path_rates=dpr[idx:idx + depths[3]],\n            downsample=None,\n            use_rel_pos_bias=use_rel_pos_bias)\n\n        self.norm = nn.LayerNorm(dims[3])\n        self.head = nn.Linear(dims[3], num_classes)\n\n    def forward_features(self, x: torch.Tensor):\n        x_map, _, _ = self.patch_embed(x)\n        x_map = self.pos_drop(x_map)\n\n        _, x_map = self.layer1(x_map)\n        _, x_map = self.layer2(x_map)\n        _, x_map = self.layer3(x_map)\n        x4, _ = self.layer4(x_map)\n\n        x4 = self.norm(x4)\n        x4 = x4.mean(dim=(1, 2))\n        return x4\n\n    def forward(self, x: torch.Tensor):\n        return self.head(self.forward_features(x))","metadata":{"id":"xbFPqSRM1pBE","trusted":true,"execution":{"iopub.status.busy":"2025-12-18T05:30:07.043215Z","iopub.execute_input":"2025-12-18T05:30:07.043851Z","iopub.status.idle":"2025-12-18T05:30:07.058177Z","shell.execute_reply.started":"2025-12-18T05:30:07.043827Z","shell.execute_reply":"2025-12-18T05:30:07.057400Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"torch.manual_seed(0)\n\nmodel = SwinTransformer(\n    img_size=32,\n    patch_size=4,\n    in_chans=3,\n    num_classes=100,\n    embed_dim=96,\n    depths=(2, 2, 6, 2),\n    num_heads=(3, 6, 12, 24),\n    window_size=4,\n    drop_path_rate=0.1,)\n\nx = torch.randn(2, 3, 32, 32)\ny = model(x)\nprint(y.shape)  # [2, Clases]","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8rVWPbsu2FXG","outputId":"4df65c5a-9368-443f-f1ce-b802723c0373","trusted":true,"execution":{"iopub.status.busy":"2025-12-18T05:30:09.134699Z","iopub.execute_input":"2025-12-18T05:30:09.134963Z","iopub.status.idle":"2025-12-18T05:30:09.469775Z","shell.execute_reply.started":"2025-12-18T05:30:09.134942Z","shell.execute_reply":"2025-12-18T05:30:09.468903Z"}},"outputs":[{"name":"stdout","text":"torch.Size([2, 100])\n","output_type":"stream"}],"execution_count":19},{"cell_type":"markdown","source":"---","metadata":{"id":"GYKYLjKv22ee"}},{"cell_type":"code","source":"import math\nimport os\nimport time\nimport random\nimport inspect\nfrom contextlib import contextmanager, nullcontext\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\ndef seed_everything(seed: int = 0):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n\ndef accuracy_topk(logits: torch.Tensor, targets: torch.Tensor, ks=(1, 3, 5)):\n    with torch.no_grad():\n        max_k = max(ks)\n        B = targets.size(0)\n        _, pred = torch.topk(logits, k=max_k, dim=1)\n        correct = pred.eq(targets.view(-1, 1).expand_as(pred))\n        out = {}\n        for k in ks:\n            out[k] = 100.0 * correct[:, :k].any(dim=1).float().sum().item() / B\n        return out\n\n_DTYPE_MAP = {\n    \"bf16\": torch.bfloat16, \"bfloat16\": torch.bfloat16,\n    \"fp16\": torch.float16, \"float16\": torch.float16,\n    \"fp32\": torch.float32, \"float32\": torch.float32,}\n\n\ndef _cuda_dtype_supported(dtype: torch.dtype) -> bool:\n    if not torch.cuda.is_available():\n        return False\n    return dtype in (torch.bfloat16, torch.float16)\n\n\ndef make_grad_scaler(device: str = \"cuda\", enabled: bool = True):\n    if not enabled:\n        return None\n    if hasattr(torch, \"amp\") and hasattr(torch.amp, \"GradScaler\"):\n        try:\n            sig = inspect.signature(torch.amp.GradScaler)\n            if len(sig.parameters) >= 1:\n                return torch.amp.GradScaler(device if device in (\"cuda\", \"cpu\") else \"cuda\")\n            return torch.amp.GradScaler()\n        except Exception:\n            pass\n    if hasattr(torch.cuda, \"amp\") and hasattr(torch.cuda.amp, \"GradScaler\"):\n        return torch.cuda.amp.GradScaler()\n    return None\n\n\n@contextmanager\ndef autocast_ctx(device: str = \"cuda\", enabled: bool = True, dtype: str = \"bf16\", cache_enabled: bool = True):\n    if not enabled:\n        with nullcontext():\n            yield\n        return\n\n    if device == \"cuda\":\n        want = _DTYPE_MAP.get(dtype.lower(), torch.bfloat16)\n        use = want if _cuda_dtype_supported(want) else torch.float16\n        with torch.amp.autocast(device_type=\"cuda\", dtype=use, cache_enabled=cache_enabled):\n            yield\n        return\n\n    if device == \"cpu\":\n        try:\n            with torch.amp.autocast(device_type=\"cpu\", dtype=torch.bfloat16, cache_enabled=cache_enabled):\n                yield\n        except Exception:\n            with nullcontext():\n                yield\n        return\n\n    with nullcontext():\n        yield\n","metadata":{"id":"peU4INQx3Beg","trusted":true,"execution":{"iopub.status.busy":"2025-12-18T05:30:11.767565Z","iopub.execute_input":"2025-12-18T05:30:11.768265Z","iopub.status.idle":"2025-12-18T05:30:11.778686Z","shell.execute_reply.started":"2025-12-18T05:30:11.768242Z","shell.execute_reply":"2025-12-18T05:30:11.777835Z"}},"outputs":[],"execution_count":20},{"cell_type":"code","source":"def build_param_groups_no_wd(model: nn.Module, weight_decay: float):\n    decay, no_decay = [], []\n    for name, p in model.named_parameters():\n        if not p.requires_grad:\n            continue\n\n        # típicamente no decay en bias y LayerNorm/BatchNorm scale\n        if name.endswith(\".bias\") or (\"norm\" in name.lower()) or (\"bn\" in name.lower()) or (\"ln\" in name.lower()):\n            no_decay.append(p)\n        else:\n            decay.append(p)\n\n    return [\n        {\"params\": decay, \"weight_decay\": weight_decay},\n        {\"params\": no_decay, \"weight_decay\": 0.0}]\n\n\nclass WarmupCosineLR:\n    \"\"\"\n    Scheduler simple por STEP:\n      - warmup lineal por warmup_steps\n      - luego cosine hasta total_steps\n    \"\"\"\n    def __init__(self, optimizer, total_steps: int, warmup_steps: int, min_lr: float = 0.0):\n        self.optimizer = optimizer\n        self.total_steps = int(total_steps)\n        self.warmup_steps = int(warmup_steps)\n        self.min_lr = float(min_lr)\n        self.base_lrs = [g[\"lr\"] for g in optimizer.param_groups]\n        self.step_num = 0\n\n    def step(self):\n        self.step_num += 1\n        t = self.step_num\n\n        for i, group in enumerate(self.optimizer.param_groups):\n            base = self.base_lrs[i]\n            if t <= self.warmup_steps and self.warmup_steps > 0:\n                lr = base * (t / self.warmup_steps)\n            else:\n                tt = min(t, self.total_steps)\n                denom = max(1, self.total_steps - self.warmup_steps)\n                progress = (tt - self.warmup_steps) / denom\n                cosine = 0.5 * (1.0 + math.cos(math.pi * progress))\n                lr = self.min_lr + (base - self.min_lr) * cosine\n            group[\"lr\"] = lr\n\n    def state_dict(self):\n        return {\"step_num\": self.step_num}\n\n    def load_state_dict(self, d):\n        self.step_num = int(d.get(\"step_num\", 0))\n","metadata":{"id":"XlIONghJ4ME4","trusted":true,"execution":{"iopub.status.busy":"2025-12-18T05:30:13.031167Z","iopub.execute_input":"2025-12-18T05:30:13.031855Z","iopub.status.idle":"2025-12-18T05:30:13.039437Z","shell.execute_reply.started":"2025-12-18T05:30:13.031829Z","shell.execute_reply":"2025-12-18T05:30:13.038729Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"def save_checkpoint(path: str, model, optimizer, scheduler, scaler, epoch: int, best_top1: float, extra: dict | None = None):\n    ckpt = {\n        \"model\": model.state_dict(),\n        \"optimizer\": optimizer.state_dict(),\n        \"scheduler\": scheduler.state_dict() if scheduler is not None else None,\n        \"scaler\": scaler.state_dict() if scaler is not None else None,\n        \"epoch\": epoch,\n        \"best_top1\": best_top1,\n        \"extra\": extra or {},}\n    torch.save(ckpt, path)\n\n\ndef load_checkpoint(path: str, model, optimizer=None, scheduler=None, scaler=None, map_location=\"cpu\"):\n    ckpt = torch.load(path, map_location=map_location)\n    model.load_state_dict(ckpt[\"model\"], strict=True)\n    if optimizer is not None and ckpt.get(\"optimizer\") is not None:\n        optimizer.load_state_dict(ckpt[\"optimizer\"])\n    if scheduler is not None and ckpt.get(\"scheduler\") is not None:\n        scheduler.load_state_dict(ckpt[\"scheduler\"])\n    if scaler is not None and ckpt.get(\"scaler\") is not None:\n        scaler.load_state_dict(ckpt[\"scaler\"])\n    return ckpt","metadata":{"id":"BJg-O3qi4TDl","trusted":true,"execution":{"iopub.status.busy":"2025-12-18T05:30:14.696147Z","iopub.execute_input":"2025-12-18T05:30:14.696738Z","iopub.status.idle":"2025-12-18T05:30:14.702720Z","shell.execute_reply.started":"2025-12-18T05:30:14.696714Z","shell.execute_reply":"2025-12-18T05:30:14.701917Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"def train_one_epoch(\n    model: nn.Module,\n    dataloader,\n    optimizer: torch.optim.Optimizer,\n    scheduler,\n    device: str = \"cuda\",\n    scaler=None,                       # GradScaler (solo FP16)\n    autocast_dtype: str = \"fp16\",      # \"fp16\" o \"bf16\"\n    use_amp: bool = True,\n    grad_clip_norm: float | None = 1.0,\n    label_smoothing: float = 0.1,\n    print_every: int = 100,):\n\n    model.train().to(device)\n    criterion = nn.CrossEntropyLoss(label_smoothing=label_smoothing)\n\n    use_scaler = (scaler is not None) and use_amp and autocast_dtype.lower() in (\"fp16\", \"float16\")\n\n    running_loss = 0.0\n    total = 0\n    c1 = c3 = c5 = 0.0\n\n    t0 = time.time()\n    for step, (images, targets) in enumerate(dataloader, start=1):\n        images = images.to(device, non_blocking=True)\n        targets = targets.to(device, non_blocking=True)\n        B = targets.size(0)\n\n        optimizer.zero_grad(set_to_none=True)\n\n        with autocast_ctx(device=device, enabled=use_amp, dtype=autocast_dtype, cache_enabled=True):\n            logits = model(images)  # [B, num_classes]\n\n        # LOSS EN FP32 (clave para FP16 estable)\n        loss = criterion(logits.float(), targets)\n\n        if use_scaler:\n            scaler.scale(loss).backward()\n            # para clip hay que unscale primero\n            if grad_clip_norm is not None:\n                scaler.unscale_(optimizer)\n                torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip_norm)\n            scaler.step(optimizer)\n            scaler.update()\n        else:\n            loss.backward()\n            if grad_clip_norm is not None:\n                torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip_norm)\n            optimizer.step()\n\n        if scheduler is not None:\n            scheduler.step()\n\n        # metricas\n        running_loss += loss.item() * B\n        total += B\n        accs = accuracy_topk(logits.detach(), targets, ks=(1, 3, 5))\n        c1 += accs[1] * B / 100.0\n        c3 += accs[3] * B / 100.0\n        c5 += accs[5] * B / 100.0\n\n        if print_every and (step % print_every == 0):\n            dt = time.time() - t0\n            imgs_sec = total / max(dt, 1e-9)\n            print(\n                f\"[train step {step}/{len(dataloader)}] \"\n                f\"loss {running_loss/total:.4f} | \"\n                f\"top1 {100*c1/total:.2f}% | top3 {100*c3/total:.2f}% | top5 {100*c5/total:.2f}% | \"\n                f\"{imgs_sec:.1f} img/s\")\n\n    avg_loss = running_loss / total\n    metrics = {\"top1\": 100.0 * c1 / total, \"top3\": 100.0 * c3 / total, \"top5\": 100.0 * c5 / total}\n    return avg_loss, metrics\n\n\n@torch.no_grad()\ndef evaluate_one_epoch(\n    model: nn.Module,\n    dataloader,\n    device: str = \"cuda\",\n    autocast_dtype: str = \"fp16\",\n    use_amp: bool = True,\n    label_smoothing: float = 0.1):\n    model.eval().to(device)\n    criterion = nn.CrossEntropyLoss(label_smoothing=label_smoothing)\n\n    running_loss = 0.0\n    total = 0\n    c1 = c3 = c5 = 0.0\n\n    for images, targets in dataloader:\n        images = images.to(device, non_blocking=True)\n        targets = targets.to(device, non_blocking=True)\n        B = targets.size(0)\n\n        with autocast_ctx(device=device, enabled=use_amp, dtype=autocast_dtype, cache_enabled=True):\n            logits = model(images)\n\n        loss = criterion(logits.float(), targets)\n\n        running_loss += loss.item() * B\n        total += B\n\n        accs = accuracy_topk(logits, targets, ks=(1, 3, 5))\n        c1 += accs[1] * B / 100.0\n        c3 += accs[3] * B / 100.0\n        c5 += accs[5] * B / 100.0\n\n    avg_loss = running_loss / total\n    metrics = {\"top1\": 100.0 * c1 / total, \"top3\": 100.0 * c3 / total, \"top5\": 100.0 * c5 / total}\n    return avg_loss, metrics","metadata":{"id":"a_jbYUu24VJm","trusted":true,"execution":{"iopub.status.busy":"2025-12-18T05:30:16.254949Z","iopub.execute_input":"2025-12-18T05:30:16.255222Z","iopub.status.idle":"2025-12-18T05:30:16.267390Z","shell.execute_reply.started":"2025-12-18T05:30:16.255202Z","shell.execute_reply":"2025-12-18T05:30:16.266787Z"}},"outputs":[],"execution_count":23},{"cell_type":"code","source":"def train_swinvit(\n    model: nn.Module,\n    train_loader,\n    val_loader=None,\n    epochs: int = 50,\n    device: str = \"cuda\",\n    lr: float = 5e-4,\n    weight_decay: float = 0.05,\n    autocast_dtype: str = \"fp16\",   # para A100 usual: \"bf16\"\n    use_amp: bool = True,\n    grad_clip_norm: float | None = 1.0,\n    warmup_ratio: float = 0.05,     # 5% de steps\n    min_lr: float = 0.0,\n    label_smoothing: float = 0.1,\n    print_every: int = 100,\n    save_path: str = \"best_swinvit_cifar100.pt\",\n    resume_path: str | None = None,):\n\n    model.to(device)\n    # Optimizer (pro: param groups sin WD en norm/bias)\n    param_groups = build_param_groups_no_wd(model, weight_decay=weight_decay)\n    optimizer = torch.optim.AdamW(param_groups, lr=lr, betas=(0.9, 0.999), eps=1e-8)\n\n    # Scheduler warmup + cosine por step\n    total_steps = epochs * len(train_loader)\n    warmup_steps = int(total_steps * warmup_ratio)\n    scheduler = WarmupCosineLR(optimizer, total_steps=total_steps, warmup_steps=warmup_steps, min_lr=min_lr)\n\n    # AMP scaler: SOLO FP16\n    scaler = None\n    if use_amp and autocast_dtype.lower() in (\"fp16\", \"float16\"):\n        scaler = make_grad_scaler(device=device, enabled=True)\n\n    start_epoch = 0\n    best_val_top1 = -float(\"inf\")\n\n    if resume_path is not None:\n        ckpt = load_checkpoint(resume_path, model, optimizer=optimizer, scheduler=scheduler, scaler=scaler, map_location=device)\n        start_epoch = int(ckpt.get(\"epoch\", 0))\n        best_val_top1 = float(ckpt.get(\"best_top1\", best_val_top1))\n        print(f\"Resumed from {resume_path} at epoch {start_epoch} | best_top1 {best_val_top1:.2f}%\")\n\n    history = {\n        \"train_loss\": [], \"train_top1\": [], \"train_top3\": [], \"train_top5\": [],\n        \"val_loss\": [], \"val_top1\": [], \"val_top3\": [], \"val_top5\": [],}\n\n    for epoch in range(start_epoch + 1, epochs + 1):\n        print(f\"\\n=== Epoch {epoch}/{epochs} ===\")\n        t_epoch = time.time()\n\n        tr_loss, tr_m = train_one_epoch(\n            model=model,\n            dataloader=train_loader,\n            optimizer=optimizer,\n            scheduler=scheduler,\n            device=device,\n            scaler=scaler,\n            autocast_dtype=autocast_dtype,\n            use_amp=use_amp,\n            grad_clip_norm=grad_clip_norm,\n            label_smoothing=label_smoothing,\n            print_every=print_every,)\n\n        history[\"train_loss\"].append(tr_loss)\n        history[\"train_top1\"].append(tr_m[\"top1\"])\n        history[\"train_top3\"].append(tr_m[\"top3\"])\n        history[\"train_top5\"].append(tr_m[\"top5\"])\n\n        print(f\"[Train] loss {tr_loss:.4f} | top1 {tr_m['top1']:.2f}% | top3 {tr_m['top3']:.2f}% | top5 {tr_m['top5']:.2f}%\")\n\n        if val_loader is not None:\n            va_loss, va_m = evaluate_one_epoch(\n                model=model,\n                dataloader=val_loader,\n                device=device,\n                autocast_dtype=autocast_dtype,\n                use_amp=use_amp,\n                label_smoothing=label_smoothing,)\n\n            history[\"val_loss\"].append(va_loss)\n            history[\"val_top1\"].append(va_m[\"top1\"])\n            history[\"val_top3\"].append(va_m[\"top3\"])\n            history[\"val_top5\"].append(va_m[\"top5\"])\n\n            print(f\"[Val]   loss {va_loss:.4f} | top1 {va_m['top1']:.2f}% | top3 {va_m['top3']:.2f}% | top5 {va_m['top5']:.2f}%\")\n\n            if va_m[\"top1\"] > best_val_top1:\n                best_val_top1 = va_m[\"top1\"]\n                save_checkpoint(\n                    save_path, model, optimizer, scheduler, scaler,\n                    epoch=epoch, best_top1=best_val_top1,\n                    extra={\"autocast_dtype\": autocast_dtype, \"use_amp\": use_amp},)\n                print(f\" Best saved to {save_path} (val top1 {best_val_top1:.2f}%)\")\n\n        dt = time.time() - t_epoch\n        print(f\"Epoch time: {dt/60:.2f} min\")\n\n    return history","metadata":{"id":"JqbuTNf94fNs","trusted":true,"execution":{"iopub.status.busy":"2025-12-18T05:30:18.659444Z","iopub.execute_input":"2025-12-18T05:30:18.660178Z","iopub.status.idle":"2025-12-18T05:30:18.671370Z","shell.execute_reply.started":"2025-12-18T05:30:18.660152Z","shell.execute_reply":"2025-12-18T05:30:18.670627Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"model = SwinTransformer(\n    img_size=32,\n    patch_size=2,\n    in_chans=3,\n    num_classes=100,\n    embed_dim=96,\n    depths=(2, 2, 6, 2),\n    num_heads=(3, 6, 12, 24),\n    window_size=4,\n    drop_path_rate=0.2)\n\nif torch.cuda.device_count() >= 2:\n    print('Entrenando en',torch.cuda.device_count(), 'GPUs')\n    model = nn.DataParallel(model)   \nmodel = model.to(\"cuda\")\n\nhistory = train_swinvit(\n    model=model, train_loader=train_loader, val_loader=val_loader,\n    epochs=50, device=\"cuda\",\n    lr=5e-4, weight_decay=0.05,\n    autocast_dtype=\"bf16\", use_amp=True,\n    grad_clip_norm=1.0,\n    save_path=\"best_swin_cifar100.pt\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"cb5KF4H14m6x","outputId":"29cdfe11-844b-4b5e-98bc-c013b842fea2","trusted":true,"execution":{"iopub.status.busy":"2025-12-18T05:32:16.451169Z","iopub.execute_input":"2025-12-18T05:32:16.451872Z","iopub.status.idle":"2025-12-18T06:47:00.804603Z","shell.execute_reply.started":"2025-12-18T05:32:16.451846Z","shell.execute_reply":"2025-12-18T06:47:00.803650Z"}},"outputs":[{"name":"stdout","text":"Entrenando en 2 GPUs\n\n=== Epoch 1/50 ===\n[train step 100/704] loss 4.7562 | top1 1.56% | top3 4.94% | top5 7.67% | 515.4 img/s\n[train step 200/704] loss 4.6285 | top1 2.25% | top3 6.60% | top5 10.07% | 517.1 img/s\n[train step 300/704] loss 4.5704 | top1 2.77% | top3 7.52% | top5 11.53% | 521.2 img/s\n[train step 400/704] loss 4.5192 | top1 3.32% | top3 8.54% | top5 12.98% | 522.4 img/s\n[train step 500/704] loss 4.4740 | top1 3.78% | top3 9.73% | top5 14.53% | 524.9 img/s\n[train step 600/704] loss 4.4432 | top1 4.04% | top3 10.37% | top5 15.40% | 525.4 img/s\n[train step 700/704] loss 4.4105 | top1 4.43% | top3 11.21% | top5 16.59% | 526.9 img/s\n[Train] loss 4.4092 | top1 4.44% | top3 11.24% | top5 16.64%\n[Val]   loss 4.2126 | top1 5.80% | top3 15.36% | top5 22.26%\n Best saved to best_swin_cifar100.pt (val top1 5.80%)\nEpoch time: 1.51 min\n\n=== Epoch 2/50 ===\n[train step 100/704] loss 4.1963 | top1 6.94% | top3 17.03% | top5 24.22% | 518.2 img/s\n[train step 200/704] loss 4.1798 | top1 7.15% | top3 17.12% | top5 24.76% | 523.0 img/s\n[train step 300/704] loss 4.1642 | top1 7.40% | top3 17.69% | top5 25.45% | 524.1 img/s\n[train step 400/704] loss 4.1565 | top1 7.64% | top3 17.99% | top5 25.76% | 523.3 img/s\n[train step 500/704] loss 4.1453 | top1 7.80% | top3 18.28% | top5 26.09% | 524.5 img/s\n[train step 600/704] loss 4.1318 | top1 8.12% | top3 18.74% | top5 26.55% | 524.3 img/s\n[train step 700/704] loss 4.1178 | top1 8.36% | top3 19.25% | top5 27.10% | 525.2 img/s\n[Train] loss 4.1174 | top1 8.37% | top3 19.26% | top5 27.13%\n[Val]   loss 4.0173 | top1 10.54% | top3 22.68% | top5 30.96%\n Best saved to best_swin_cifar100.pt (val top1 10.54%)\nEpoch time: 1.52 min\n\n=== Epoch 3/50 ===\n[train step 100/704] loss 4.0039 | top1 9.56% | top3 22.88% | top5 31.89% | 528.6 img/s\n[train step 200/704] loss 3.9907 | top1 10.30% | top3 23.68% | top5 32.48% | 522.3 img/s\n[train step 300/704] loss 3.9840 | top1 10.47% | top3 23.66% | top5 32.34% | 526.2 img/s\n[train step 400/704] loss 3.9744 | top1 10.70% | top3 24.00% | top5 32.79% | 526.6 img/s\n[train step 500/704] loss 3.9573 | top1 11.11% | top3 24.62% | top5 33.54% | 527.6 img/s\n[train step 600/704] loss 3.9462 | top1 11.36% | top3 24.99% | top5 33.96% | 525.7 img/s\n[train step 700/704] loss 3.9315 | top1 11.62% | top3 25.46% | top5 34.55% | 524.7 img/s\n[Train] loss 3.9309 | top1 11.63% | top3 25.48% | top5 34.57%\n[Val]   loss 3.8309 | top1 13.20% | top3 28.54% | top5 38.00%\n Best saved to best_swin_cifar100.pt (val top1 13.20%)\nEpoch time: 1.52 min\n\n=== Epoch 4/50 ===\n[train step 100/704] loss 3.7899 | top1 15.12% | top3 30.55% | top5 40.47% | 525.5 img/s\n[train step 200/704] loss 3.8019 | top1 14.62% | top3 29.79% | top5 39.45% | 516.2 img/s\n[train step 300/704] loss 3.7991 | top1 14.56% | top3 29.82% | top5 39.52% | 520.4 img/s\n[train step 400/704] loss 3.7789 | top1 14.87% | top3 30.37% | top5 40.23% | 524.1 img/s\n[train step 500/704] loss 3.7667 | top1 15.12% | top3 30.72% | top5 40.53% | 525.1 img/s\n[train step 600/704] loss 3.7549 | top1 15.40% | top3 31.11% | top5 40.92% | 526.3 img/s\n[train step 700/704] loss 3.7478 | top1 15.52% | top3 31.33% | top5 41.19% | 526.3 img/s\n[Train] loss 3.7473 | top1 15.52% | top3 31.35% | top5 41.19%\n[Val]   loss 3.6426 | top1 17.28% | top3 33.56% | top5 44.36%\n Best saved to best_swin_cifar100.pt (val top1 17.28%)\nEpoch time: 1.52 min\n\n=== Epoch 5/50 ===\n[train step 100/704] loss 3.6542 | top1 16.91% | top3 34.31% | top5 45.06% | 522.6 img/s\n[train step 200/704] loss 3.6445 | top1 17.54% | top3 34.70% | top5 45.16% | 522.1 img/s\n[train step 300/704] loss 3.6353 | top1 17.56% | top3 35.01% | top5 45.34% | 524.2 img/s\n[train step 400/704] loss 3.6259 | top1 17.81% | top3 35.35% | top5 45.66% | 524.5 img/s\n[train step 500/704] loss 3.6143 | top1 18.09% | top3 35.67% | top5 46.07% | 525.4 img/s\n[train step 600/704] loss 3.6017 | top1 18.47% | top3 36.05% | top5 46.42% | 525.8 img/s\n[train step 700/704] loss 3.5963 | top1 18.62% | top3 36.21% | top5 46.53% | 526.2 img/s\n[Train] loss 3.5959 | top1 18.63% | top3 36.22% | top5 46.53%\n[Val]   loss 3.5043 | top1 20.46% | top3 38.72% | top5 49.16%\n Best saved to best_swin_cifar100.pt (val top1 20.46%)\nEpoch time: 1.52 min\n\n=== Epoch 6/50 ===\n[train step 100/704] loss 3.5072 | top1 21.22% | top3 39.50% | top5 49.89% | 520.1 img/s\n[train step 200/704] loss 3.5108 | top1 21.05% | top3 39.01% | top5 49.46% | 525.6 img/s\n[train step 300/704] loss 3.5063 | top1 21.05% | top3 39.02% | top5 49.60% | 531.1 img/s\n[train step 400/704] loss 3.4971 | top1 21.27% | top3 39.39% | top5 49.91% | 530.5 img/s\n[train step 500/704] loss 3.4937 | top1 21.21% | top3 39.47% | top5 49.99% | 530.7 img/s\n[train step 600/704] loss 3.4925 | top1 21.27% | top3 39.42% | top5 49.92% | 532.1 img/s\n[train step 700/704] loss 3.4868 | top1 21.36% | top3 39.57% | top5 50.14% | 532.0 img/s\n[Train] loss 3.4862 | top1 21.38% | top3 39.60% | top5 50.17%\n[Val]   loss 3.4156 | top1 22.62% | top3 41.50% | top5 52.58%\n Best saved to best_swin_cifar100.pt (val top1 22.62%)\nEpoch time: 1.50 min\n\n=== Epoch 7/50 ===\n[train step 100/704] loss 3.3929 | top1 23.77% | top3 42.47% | top5 52.91% | 516.1 img/s\n[train step 200/704] loss 3.4032 | top1 23.25% | top3 42.18% | top5 52.45% | 525.5 img/s\n[train step 300/704] loss 3.3979 | top1 23.42% | top3 42.27% | top5 52.62% | 529.1 img/s\n[train step 400/704] loss 3.3985 | top1 23.29% | top3 42.21% | top5 52.66% | 526.2 img/s\n[train step 500/704] loss 3.4059 | top1 23.03% | top3 42.14% | top5 52.65% | 525.8 img/s\n[train step 600/704] loss 3.4075 | top1 22.94% | top3 42.02% | top5 52.63% | 528.0 img/s\n[train step 700/704] loss 3.4034 | top1 23.00% | top3 42.14% | top5 52.76% | 527.3 img/s\n[Train] loss 3.4030 | top1 23.03% | top3 42.15% | top5 52.77%\n[Val]   loss 3.3463 | top1 24.88% | top3 43.16% | top5 54.76%\n Best saved to best_swin_cifar100.pt (val top1 24.88%)\nEpoch time: 1.51 min\n\n=== Epoch 8/50 ===\n[train step 100/704] loss 3.3349 | top1 23.89% | top3 43.88% | top5 54.80% | 517.4 img/s\n[train step 200/704] loss 3.3425 | top1 24.12% | top3 43.98% | top5 54.73% | 529.1 img/s\n[train step 300/704] loss 3.3425 | top1 24.31% | top3 44.19% | top5 55.10% | 530.8 img/s\n[train step 400/704] loss 3.3310 | top1 24.66% | top3 44.62% | top5 55.48% | 532.6 img/s\n[train step 500/704] loss 3.3275 | top1 24.75% | top3 44.54% | top5 55.41% | 532.2 img/s\n[train step 600/704] loss 3.3281 | top1 24.56% | top3 44.56% | top5 55.35% | 531.4 img/s\n[train step 700/704] loss 3.3239 | top1 24.74% | top3 44.68% | top5 55.42% | 532.2 img/s\n[Train] loss 3.3243 | top1 24.74% | top3 44.67% | top5 55.42%\n[Val]   loss 3.3545 | top1 24.08% | top3 43.76% | top5 54.14%\nEpoch time: 1.49 min\n\n=== Epoch 9/50 ===\n[train step 100/704] loss 3.2903 | top1 25.75% | top3 46.23% | top5 56.50% | 520.1 img/s\n[train step 200/704] loss 3.2767 | top1 26.08% | top3 46.68% | top5 57.05% | 519.4 img/s\n[train step 300/704] loss 3.2814 | top1 25.98% | top3 46.26% | top5 56.80% | 518.9 img/s\n[train step 400/704] loss 3.2811 | top1 26.02% | top3 46.24% | top5 56.81% | 521.4 img/s\n[train step 500/704] loss 3.2805 | top1 26.10% | top3 46.38% | top5 56.86% | 523.6 img/s\n[train step 600/704] loss 3.2784 | top1 26.02% | top3 46.55% | top5 57.03% | 524.2 img/s\n[train step 700/704] loss 3.2741 | top1 26.10% | top3 46.65% | top5 57.18% | 526.1 img/s\n[Train] loss 3.2739 | top1 26.11% | top3 46.66% | top5 57.18%\n[Val]   loss 3.2277 | top1 27.56% | top3 48.26% | top5 58.70%\n Best saved to best_swin_cifar100.pt (val top1 27.56%)\nEpoch time: 1.51 min\n\n=== Epoch 10/50 ===\n[train step 100/704] loss 3.2001 | top1 27.33% | top3 48.66% | top5 59.36% | 524.2 img/s\n[train step 200/704] loss 3.2194 | top1 27.07% | top3 48.20% | top5 58.62% | 524.6 img/s\n[train step 300/704] loss 3.2152 | top1 27.05% | top3 48.04% | top5 58.71% | 529.0 img/s\n[train step 400/704] loss 3.2151 | top1 27.08% | top3 48.02% | top5 58.75% | 528.6 img/s\n[train step 500/704] loss 3.2137 | top1 27.10% | top3 48.06% | top5 58.79% | 529.7 img/s\n[train step 600/704] loss 3.2079 | top1 27.39% | top3 48.28% | top5 59.01% | 532.2 img/s\n[train step 700/704] loss 3.2095 | top1 27.44% | top3 48.33% | top5 59.02% | 533.3 img/s\n[Train] loss 3.2098 | top1 27.46% | top3 48.32% | top5 59.02%\n[Val]   loss 3.1573 | top1 29.06% | top3 49.46% | top5 60.32%\n Best saved to best_swin_cifar100.pt (val top1 29.06%)\nEpoch time: 1.50 min\n\n=== Epoch 11/50 ===\n[train step 100/704] loss 3.1616 | top1 28.88% | top3 49.95% | top5 60.14% | 517.5 img/s\n[train step 200/704] loss 3.1567 | top1 28.59% | top3 50.23% | top5 60.58% | 521.3 img/s\n[train step 300/704] loss 3.1641 | top1 28.64% | top3 50.30% | top5 60.66% | 524.9 img/s\n[train step 400/704] loss 3.1580 | top1 28.78% | top3 50.18% | top5 60.64% | 526.9 img/s\n[train step 500/704] loss 3.1579 | top1 28.82% | top3 50.26% | top5 60.65% | 527.8 img/s\n[train step 600/704] loss 3.1579 | top1 28.76% | top3 50.18% | top5 60.64% | 528.9 img/s\n[train step 700/704] loss 3.1565 | top1 28.87% | top3 50.25% | top5 60.68% | 530.4 img/s\n[Train] loss 3.1562 | top1 28.88% | top3 50.28% | top5 60.69%\n[Val]   loss 3.1127 | top1 29.84% | top3 51.20% | top5 61.92%\n Best saved to best_swin_cifar100.pt (val top1 29.84%)\nEpoch time: 1.51 min\n\n=== Epoch 12/50 ===\n[train step 100/704] loss 3.1006 | top1 30.50% | top3 51.86% | top5 61.41% | 530.8 img/s\n[train step 200/704] loss 3.0896 | top1 30.81% | top3 51.88% | top5 61.79% | 536.8 img/s\n[train step 300/704] loss 3.0994 | top1 30.26% | top3 51.61% | top5 61.56% | 531.7 img/s\n[train step 400/704] loss 3.0995 | top1 30.35% | top3 51.68% | top5 61.71% | 531.2 img/s\n[train step 500/704] loss 3.1024 | top1 30.35% | top3 51.52% | top5 61.68% | 531.6 img/s\n[train step 600/704] loss 3.1034 | top1 30.38% | top3 51.54% | top5 61.78% | 531.8 img/s\n[train step 700/704] loss 3.1050 | top1 30.34% | top3 51.47% | top5 61.81% | 532.4 img/s\n[Train] loss 3.1049 | top1 30.36% | top3 51.48% | top5 61.83%\n[Val]   loss 3.1252 | top1 30.38% | top3 51.06% | top5 61.40%\n Best saved to best_swin_cifar100.pt (val top1 30.38%)\nEpoch time: 1.50 min\n\n=== Epoch 13/50 ===\n[train step 100/704] loss 3.0555 | top1 31.58% | top3 52.75% | top5 63.23% | 513.0 img/s\n[train step 200/704] loss 3.0650 | top1 31.75% | top3 52.40% | top5 62.90% | 522.6 img/s\n[train step 300/704] loss 3.0551 | top1 31.78% | top3 52.76% | top5 63.19% | 526.4 img/s\n[train step 400/704] loss 3.0631 | top1 31.39% | top3 52.50% | top5 62.98% | 527.4 img/s\n[train step 500/704] loss 3.0660 | top1 31.21% | top3 52.53% | top5 63.00% | 528.2 img/s\n[train step 600/704] loss 3.0641 | top1 31.37% | top3 52.68% | top5 63.19% | 530.8 img/s\n[train step 700/704] loss 3.0612 | top1 31.55% | top3 52.82% | top5 63.24% | 531.6 img/s\n[Train] loss 3.0616 | top1 31.53% | top3 52.80% | top5 63.24%\n[Val]   loss 3.0624 | top1 31.70% | top3 52.26% | top5 62.12%\n Best saved to best_swin_cifar100.pt (val top1 31.70%)\nEpoch time: 1.50 min\n\n=== Epoch 14/50 ===\n[train step 100/704] loss 3.0116 | top1 32.86% | top3 54.56% | top5 64.58% | 526.1 img/s\n[train step 200/704] loss 3.0205 | top1 32.43% | top3 54.18% | top5 64.38% | 530.1 img/s\n[train step 300/704] loss 3.0145 | top1 32.61% | top3 54.32% | top5 64.46% | 530.8 img/s\n[train step 400/704] loss 3.0164 | top1 32.81% | top3 54.38% | top5 64.37% | 532.0 img/s\n[train step 500/704] loss 3.0152 | top1 32.91% | top3 54.35% | top5 64.47% | 529.4 img/s\n[train step 600/704] loss 3.0135 | top1 33.01% | top3 54.42% | top5 64.48% | 529.8 img/s\n[train step 700/704] loss 3.0152 | top1 33.03% | top3 54.37% | top5 64.39% | 530.6 img/s\n[Train] loss 3.0157 | top1 33.03% | top3 54.36% | top5 64.38%\n[Val]   loss 3.0028 | top1 33.22% | top3 54.28% | top5 64.34%\n Best saved to best_swin_cifar100.pt (val top1 33.22%)\nEpoch time: 1.50 min\n\n=== Epoch 15/50 ===\n[train step 100/704] loss 2.9415 | top1 34.42% | top3 56.23% | top5 66.70% | 513.6 img/s\n[train step 200/704] loss 2.9517 | top1 34.07% | top3 55.87% | top5 66.20% | 517.9 img/s\n[train step 300/704] loss 2.9535 | top1 34.05% | top3 56.07% | top5 66.30% | 524.6 img/s\n[train step 400/704] loss 2.9555 | top1 34.14% | top3 55.92% | top5 66.12% | 528.6 img/s\n[train step 500/704] loss 2.9610 | top1 34.04% | top3 55.70% | top5 65.90% | 530.5 img/s\n[train step 600/704] loss 2.9627 | top1 34.07% | top3 55.66% | top5 65.82% | 529.6 img/s\n[train step 700/704] loss 2.9626 | top1 34.05% | top3 55.63% | top5 65.87% | 530.8 img/s\n[Train] loss 2.9623 | top1 34.04% | top3 55.66% | top5 65.89%\n[Val]   loss 2.9540 | top1 33.80% | top3 56.28% | top5 66.14%\n Best saved to best_swin_cifar100.pt (val top1 33.80%)\nEpoch time: 1.50 min\n\n=== Epoch 16/50 ===\n[train step 100/704] loss 2.8925 | top1 35.62% | top3 57.75% | top5 67.34% | 513.9 img/s\n[train step 200/704] loss 2.9110 | top1 35.20% | top3 57.52% | top5 67.17% | 529.4 img/s\n[train step 300/704] loss 2.9161 | top1 34.96% | top3 57.27% | top5 67.07% | 531.9 img/s\n[train step 400/704] loss 2.9155 | top1 35.05% | top3 57.38% | top5 67.09% | 534.2 img/s\n[train step 500/704] loss 2.9139 | top1 35.12% | top3 57.36% | top5 67.13% | 533.2 img/s\n[train step 600/704] loss 2.9151 | top1 35.15% | top3 57.27% | top5 67.16% | 531.5 img/s\n[train step 700/704] loss 2.9157 | top1 35.12% | top3 57.17% | top5 67.19% | 531.8 img/s\n[Train] loss 2.9160 | top1 35.10% | top3 57.15% | top5 67.17%\n[Val]   loss 2.9370 | top1 34.78% | top3 56.26% | top5 66.16%\n Best saved to best_swin_cifar100.pt (val top1 34.78%)\nEpoch time: 1.50 min\n\n=== Epoch 17/50 ===\n[train step 100/704] loss 2.8382 | top1 37.03% | top3 59.34% | top5 69.56% | 519.4 img/s\n[train step 200/704] loss 2.8619 | top1 36.81% | top3 58.70% | top5 68.63% | 526.0 img/s\n[train step 300/704] loss 2.8718 | top1 36.60% | top3 58.40% | top5 68.41% | 530.3 img/s\n[train step 400/704] loss 2.8754 | top1 36.39% | top3 58.05% | top5 68.20% | 529.2 img/s\n[train step 500/704] loss 2.8772 | top1 36.43% | top3 57.93% | top5 68.20% | 528.4 img/s\n[train step 600/704] loss 2.8804 | top1 36.33% | top3 57.89% | top5 68.11% | 527.9 img/s\n[train step 700/704] loss 2.8792 | top1 36.32% | top3 57.98% | top5 68.14% | 527.6 img/s\n[Train] loss 2.8793 | top1 36.32% | top3 57.98% | top5 68.12%\n[Val]   loss 2.8672 | top1 35.90% | top3 58.64% | top5 68.30%\n Best saved to best_swin_cifar100.pt (val top1 35.90%)\nEpoch time: 1.51 min\n\n=== Epoch 18/50 ===\n[train step 100/704] loss 2.8349 | top1 37.42% | top3 59.48% | top5 69.75% | 527.2 img/s\n[train step 200/704] loss 2.8294 | top1 37.50% | top3 59.60% | top5 69.61% | 531.7 img/s\n[train step 300/704] loss 2.8296 | top1 37.21% | top3 59.44% | top5 69.46% | 534.1 img/s\n[train step 400/704] loss 2.8249 | top1 37.39% | top3 59.69% | top5 69.62% | 530.1 img/s\n[train step 500/704] loss 2.8243 | top1 37.48% | top3 59.77% | top5 69.61% | 530.8 img/s\n[train step 600/704] loss 2.8333 | top1 37.40% | top3 59.46% | top5 69.28% | 531.3 img/s\n[train step 700/704] loss 2.8350 | top1 37.33% | top3 59.36% | top5 69.25% | 531.1 img/s\n[Train] loss 2.8351 | top1 37.34% | top3 59.36% | top5 69.23%\n[Val]   loss 2.8289 | top1 37.10% | top3 60.00% | top5 69.90%\n Best saved to best_swin_cifar100.pt (val top1 37.10%)\nEpoch time: 1.50 min\n\n=== Epoch 19/50 ===\n[train step 100/704] loss 2.7906 | top1 38.25% | top3 60.70% | top5 70.36% | 520.4 img/s\n[train step 200/704] loss 2.7946 | top1 38.07% | top3 60.09% | top5 70.13% | 525.5 img/s\n[train step 300/704] loss 2.7943 | top1 38.11% | top3 60.24% | top5 70.33% | 531.7 img/s\n[train step 400/704] loss 2.7842 | top1 38.46% | top3 60.70% | top5 70.56% | 531.7 img/s\n[train step 500/704] loss 2.7898 | top1 38.38% | top3 60.56% | top5 70.40% | 533.6 img/s\n[train step 600/704] loss 2.7933 | top1 38.30% | top3 60.55% | top5 70.40% | 534.0 img/s\n[train step 700/704] loss 2.7952 | top1 38.38% | top3 60.53% | top5 70.28% | 535.0 img/s\n[Train] loss 2.7945 | top1 38.40% | top3 60.56% | top5 70.29%\n[Val]   loss 2.8042 | top1 38.52% | top3 60.22% | top5 70.62%\n Best saved to best_swin_cifar100.pt (val top1 38.52%)\nEpoch time: 1.49 min\n\n=== Epoch 20/50 ===\n[train step 100/704] loss 2.7546 | top1 39.05% | top3 62.42% | top5 71.64% | 525.2 img/s\n[train step 200/704] loss 2.7559 | top1 39.12% | top3 62.09% | top5 71.61% | 530.2 img/s\n[train step 300/704] loss 2.7550 | top1 39.14% | top3 62.02% | top5 71.47% | 528.3 img/s\n[train step 400/704] loss 2.7521 | top1 39.35% | top3 62.00% | top5 71.64% | 529.3 img/s\n[train step 500/704] loss 2.7509 | top1 39.27% | top3 61.88% | top5 71.68% | 531.9 img/s\n[train step 600/704] loss 2.7547 | top1 39.27% | top3 61.67% | top5 71.52% | 532.6 img/s\n[train step 700/704] loss 2.7588 | top1 39.15% | top3 61.56% | top5 71.45% | 532.0 img/s\n[Train] loss 2.7590 | top1 39.16% | top3 61.55% | top5 71.46%\n[Val]   loss 2.8058 | top1 37.52% | top3 59.92% | top5 69.76%\nEpoch time: 1.49 min\n\n=== Epoch 21/50 ===\n[train step 100/704] loss 2.6995 | top1 41.22% | top3 63.66% | top5 72.95% | 531.2 img/s\n[train step 200/704] loss 2.7059 | top1 40.88% | top3 63.33% | top5 72.79% | 523.5 img/s\n[train step 300/704] loss 2.6984 | top1 41.06% | top3 63.48% | top5 72.82% | 527.2 img/s\n[train step 400/704] loss 2.6973 | top1 41.07% | top3 63.27% | top5 72.87% | 526.7 img/s\n[train step 500/704] loss 2.7077 | top1 40.73% | top3 63.00% | top5 72.65% | 526.5 img/s\n[train step 600/704] loss 2.7104 | top1 40.55% | top3 62.90% | top5 72.56% | 528.9 img/s\n[train step 700/704] loss 2.7133 | top1 40.50% | top3 62.77% | top5 72.38% | 529.5 img/s\n[Train] loss 2.7133 | top1 40.51% | top3 62.77% | top5 72.39%\n[Val]   loss 2.8143 | top1 38.04% | top3 59.94% | top5 69.66%\nEpoch time: 1.49 min\n\n=== Epoch 22/50 ===\n[train step 100/704] loss 2.6901 | top1 40.52% | top3 63.73% | top5 72.94% | 510.6 img/s\n[train step 200/704] loss 2.6775 | top1 41.15% | top3 63.90% | top5 73.29% | 523.4 img/s\n[train step 300/704] loss 2.6742 | top1 41.40% | top3 64.04% | top5 73.38% | 530.1 img/s\n[train step 400/704] loss 2.6705 | top1 41.64% | top3 64.00% | top5 73.47% | 531.7 img/s\n[train step 500/704] loss 2.6734 | top1 41.57% | top3 63.90% | top5 73.38% | 531.8 img/s\n[train step 600/704] loss 2.6679 | top1 41.77% | top3 64.04% | top5 73.47% | 531.7 img/s\n[train step 700/704] loss 2.6708 | top1 41.70% | top3 63.96% | top5 73.37% | 531.1 img/s\n[Train] loss 2.6710 | top1 41.68% | top3 63.95% | top5 73.37%\n[Val]   loss 2.7633 | top1 39.74% | top3 60.70% | top5 70.80%\n Best saved to best_swin_cifar100.pt (val top1 39.74%)\nEpoch time: 1.50 min\n\n=== Epoch 23/50 ===\n[train step 100/704] loss 2.6020 | top1 43.62% | top3 65.78% | top5 75.05% | 516.1 img/s\n[train step 200/704] loss 2.6156 | top1 43.24% | top3 65.61% | top5 74.91% | 531.4 img/s\n[train step 300/704] loss 2.6249 | top1 42.97% | top3 65.23% | top5 74.62% | 528.1 img/s\n[train step 400/704] loss 2.6354 | top1 42.65% | top3 64.94% | top5 74.34% | 529.4 img/s\n[train step 500/704] loss 2.6341 | top1 42.74% | top3 65.00% | top5 74.23% | 528.6 img/s\n[train step 600/704] loss 2.6353 | top1 42.71% | top3 64.93% | top5 74.31% | 529.5 img/s\n[train step 700/704] loss 2.6325 | top1 42.83% | top3 65.04% | top5 74.37% | 530.4 img/s\n[Train] loss 2.6315 | top1 42.87% | top3 65.07% | top5 74.39%\n[Val]   loss 2.7318 | top1 41.22% | top3 62.20% | top5 71.72%\n Best saved to best_swin_cifar100.pt (val top1 41.22%)\nEpoch time: 1.50 min\n\n=== Epoch 24/50 ===\n[train step 100/704] loss 2.5769 | top1 44.33% | top3 67.06% | top5 76.22% | 513.1 img/s\n[train step 200/704] loss 2.5844 | top1 43.84% | top3 66.66% | top5 75.86% | 524.8 img/s\n[train step 300/704] loss 2.5865 | top1 43.86% | top3 66.35% | top5 75.68% | 527.8 img/s\n[train step 400/704] loss 2.5892 | top1 43.78% | top3 66.18% | top5 75.47% | 529.2 img/s\n[train step 500/704] loss 2.5941 | top1 43.61% | top3 66.23% | top5 75.30% | 528.9 img/s\n[train step 600/704] loss 2.5932 | top1 43.64% | top3 66.22% | top5 75.36% | 528.7 img/s\n[train step 700/704] loss 2.5932 | top1 43.60% | top3 66.29% | top5 75.39% | 529.2 img/s\n[Train] loss 2.5935 | top1 43.58% | top3 66.27% | top5 75.38%\n[Val]   loss 2.6812 | top1 41.70% | top3 63.98% | top5 73.88%\n Best saved to best_swin_cifar100.pt (val top1 41.70%)\nEpoch time: 1.51 min\n\n=== Epoch 25/50 ===\n[train step 100/704] loss 2.5337 | top1 44.58% | top3 68.23% | top5 77.34% | 516.1 img/s\n[train step 200/704] loss 2.5484 | top1 44.62% | top3 67.53% | top5 76.79% | 523.5 img/s\n[train step 300/704] loss 2.5532 | top1 44.56% | top3 67.18% | top5 76.41% | 526.6 img/s\n[train step 400/704] loss 2.5592 | top1 44.49% | top3 67.12% | top5 76.33% | 524.2 img/s\n[train step 500/704] loss 2.5561 | top1 44.73% | top3 67.23% | top5 76.35% | 525.3 img/s\n[train step 600/704] loss 2.5563 | top1 44.71% | top3 67.27% | top5 76.40% | 523.9 img/s\n[train step 700/704] loss 2.5601 | top1 44.62% | top3 67.15% | top5 76.29% | 525.4 img/s\n[Train] loss 2.5606 | top1 44.60% | top3 67.15% | top5 76.28%\n[Val]   loss 2.6506 | top1 43.04% | top3 64.32% | top5 74.32%\n Best saved to best_swin_cifar100.pt (val top1 43.04%)\nEpoch time: 1.52 min\n\n=== Epoch 26/50 ===\n[train step 100/704] loss 2.5076 | top1 45.75% | top3 68.91% | top5 77.98% | 516.9 img/s\n[train step 200/704] loss 2.5039 | top1 46.13% | top3 68.89% | top5 77.64% | 527.0 img/s\n[train step 300/704] loss 2.5110 | top1 45.78% | top3 68.54% | top5 77.52% | 528.0 img/s\n[train step 400/704] loss 2.5077 | top1 45.98% | top3 68.71% | top5 77.54% | 531.1 img/s\n[train step 500/704] loss 2.5134 | top1 45.89% | top3 68.59% | top5 77.45% | 533.2 img/s\n[train step 600/704] loss 2.5144 | top1 45.81% | top3 68.55% | top5 77.45% | 532.5 img/s\n[train step 700/704] loss 2.5155 | top1 45.95% | top3 68.46% | top5 77.32% | 531.3 img/s\n[Train] loss 2.5156 | top1 45.95% | top3 68.46% | top5 77.34%\n[Val]   loss 2.6219 | top1 43.80% | top3 65.92% | top5 75.32%\n Best saved to best_swin_cifar100.pt (val top1 43.80%)\nEpoch time: 1.50 min\n\n=== Epoch 27/50 ===\n[train step 100/704] loss 2.4609 | top1 47.22% | top3 69.89% | top5 78.78% | 520.5 img/s\n[train step 200/704] loss 2.4596 | top1 47.62% | top3 70.09% | top5 78.78% | 525.2 img/s\n[train step 300/704] loss 2.4599 | top1 47.67% | top3 69.95% | top5 78.78% | 528.7 img/s\n[train step 400/704] loss 2.4685 | top1 47.39% | top3 69.80% | top5 78.59% | 529.2 img/s\n[train step 500/704] loss 2.4685 | top1 47.41% | top3 69.87% | top5 78.61% | 528.4 img/s\n[train step 600/704] loss 2.4722 | top1 47.36% | top3 69.75% | top5 78.47% | 529.9 img/s\n[train step 700/704] loss 2.4735 | top1 47.35% | top3 69.75% | top5 78.41% | 530.0 img/s\n[Train] loss 2.4735 | top1 47.37% | top3 69.76% | top5 78.40%\n[Val]   loss 2.6361 | top1 43.58% | top3 64.46% | top5 73.90%\nEpoch time: 1.49 min\n\n=== Epoch 28/50 ===\n[train step 100/704] loss 2.4375 | top1 48.66% | top3 70.22% | top5 78.80% | 526.8 img/s\n[train step 200/704] loss 2.4315 | top1 48.79% | top3 70.85% | top5 79.02% | 526.9 img/s\n[train step 300/704] loss 2.4295 | top1 48.80% | top3 70.97% | top5 79.24% | 524.4 img/s\n[train step 400/704] loss 2.4313 | top1 48.68% | top3 70.71% | top5 79.06% | 528.1 img/s\n[train step 500/704] loss 2.4327 | top1 48.49% | top3 70.71% | top5 79.07% | 528.8 img/s\n[train step 600/704] loss 2.4383 | top1 48.31% | top3 70.48% | top5 78.93% | 529.1 img/s\n[train step 700/704] loss 2.4397 | top1 48.37% | top3 70.46% | top5 78.91% | 528.3 img/s\n[Train] loss 2.4394 | top1 48.36% | top3 70.47% | top5 78.93%\n[Val]   loss 2.6125 | top1 43.44% | top3 65.86% | top5 74.58%\nEpoch time: 1.50 min\n\n=== Epoch 29/50 ===\n[train step 100/704] loss 2.3468 | top1 51.52% | top3 72.83% | top5 80.89% | 521.1 img/s\n[train step 200/704] loss 2.3731 | top1 50.65% | top3 72.45% | top5 80.55% | 518.8 img/s\n[train step 300/704] loss 2.3866 | top1 50.23% | top3 72.16% | top5 80.23% | 523.8 img/s\n[train step 400/704] loss 2.3933 | top1 49.80% | top3 71.98% | top5 80.18% | 528.7 img/s\n[train step 500/704] loss 2.3927 | top1 49.76% | top3 71.97% | top5 80.17% | 528.7 img/s\n[train step 600/704] loss 2.3905 | top1 49.81% | top3 71.94% | top5 80.21% | 528.5 img/s\n[train step 700/704] loss 2.3903 | top1 49.83% | top3 71.84% | top5 80.16% | 528.7 img/s\n[Train] loss 2.3899 | top1 49.84% | top3 71.85% | top5 80.17%\n[Val]   loss 2.5628 | top1 44.78% | top3 67.46% | top5 76.44%\n Best saved to best_swin_cifar100.pt (val top1 44.78%)\nEpoch time: 1.51 min\n\n=== Epoch 30/50 ===\n[train step 100/704] loss 2.3535 | top1 50.59% | top3 73.19% | top5 81.70% | 518.1 img/s\n[train step 200/704] loss 2.3495 | top1 50.64% | top3 73.15% | top5 81.72% | 524.3 img/s\n[train step 300/704] loss 2.3568 | top1 50.41% | top3 72.80% | top5 81.12% | 529.5 img/s\n[train step 400/704] loss 2.3575 | top1 50.45% | top3 72.73% | top5 81.15% | 531.7 img/s\n[train step 500/704] loss 2.3568 | top1 50.59% | top3 72.75% | top5 81.16% | 532.5 img/s\n[train step 600/704] loss 2.3579 | top1 50.58% | top3 72.70% | top5 81.12% | 533.7 img/s\n[train step 700/704] loss 2.3582 | top1 50.56% | top3 72.68% | top5 81.18% | 534.6 img/s\n[Train] loss 2.3579 | top1 50.55% | top3 72.70% | top5 81.18%\n[Val]   loss 2.5821 | top1 45.30% | top3 66.64% | top5 75.82%\n Best saved to best_swin_cifar100.pt (val top1 45.30%)\nEpoch time: 1.49 min\n\n=== Epoch 31/50 ===\n[train step 100/704] loss 2.2971 | top1 52.39% | top3 74.62% | top5 82.56% | 533.6 img/s\n[train step 200/704] loss 2.3064 | top1 52.21% | top3 74.23% | top5 82.23% | 531.2 img/s\n[train step 300/704] loss 2.3074 | top1 52.36% | top3 74.12% | top5 82.21% | 532.7 img/s\n[train step 400/704] loss 2.3141 | top1 52.14% | top3 73.94% | top5 82.06% | 534.0 img/s\n[train step 500/704] loss 2.3214 | top1 51.89% | top3 73.68% | top5 81.85% | 535.6 img/s\n[train step 600/704] loss 2.3230 | top1 51.80% | top3 73.70% | top5 81.85% | 536.7 img/s\n[train step 700/704] loss 2.3257 | top1 51.61% | top3 73.71% | top5 81.85% | 537.9 img/s\n[Train] loss 2.3259 | top1 51.60% | top3 73.70% | top5 81.84%\n[Val]   loss 2.5471 | top1 46.06% | top3 67.36% | top5 76.32%\n Best saved to best_swin_cifar100.pt (val top1 46.06%)\nEpoch time: 1.49 min\n\n=== Epoch 32/50 ===\n[train step 100/704] loss 2.2598 | top1 53.55% | top3 75.41% | top5 83.42% | 527.0 img/s\n[train step 200/704] loss 2.2774 | top1 53.22% | top3 75.33% | top5 83.05% | 531.1 img/s\n[train step 300/704] loss 2.2836 | top1 52.91% | top3 74.84% | top5 82.79% | 533.9 img/s\n[train step 400/704] loss 2.2857 | top1 52.76% | top3 74.55% | top5 82.62% | 535.2 img/s\n[train step 500/704] loss 2.2865 | top1 52.84% | top3 74.58% | top5 82.68% | 537.9 img/s\n[train step 600/704] loss 2.2857 | top1 52.85% | top3 74.57% | top5 82.61% | 538.1 img/s\n[train step 700/704] loss 2.2912 | top1 52.58% | top3 74.45% | top5 82.55% | 537.0 img/s\n[Train] loss 2.2914 | top1 52.58% | top3 74.44% | top5 82.56%\n[Val]   loss 2.5226 | top1 47.16% | top3 67.34% | top5 76.32%\n Best saved to best_swin_cifar100.pt (val top1 47.16%)\nEpoch time: 1.48 min\n\n=== Epoch 33/50 ===\n[train step 100/704] loss 2.2529 | top1 53.42% | top3 75.78% | top5 83.47% | 529.2 img/s\n[train step 200/704] loss 2.2374 | top1 54.10% | top3 76.09% | top5 83.84% | 527.6 img/s\n[train step 300/704] loss 2.2410 | top1 53.99% | top3 75.84% | top5 83.72% | 531.2 img/s\n[train step 400/704] loss 2.2435 | top1 54.01% | top3 75.78% | top5 83.65% | 533.0 img/s\n[train step 500/704] loss 2.2453 | top1 53.90% | top3 75.80% | top5 83.62% | 534.0 img/s\n[train step 600/704] loss 2.2492 | top1 53.75% | top3 75.75% | top5 83.60% | 533.8 img/s\n[train step 700/704] loss 2.2552 | top1 53.62% | top3 75.56% | top5 83.40% | 533.8 img/s\n[Train] loss 2.2557 | top1 53.61% | top3 75.55% | top5 83.40%\n[Val]   loss 2.5082 | top1 47.50% | top3 68.98% | top5 77.44%\n Best saved to best_swin_cifar100.pt (val top1 47.50%)\nEpoch time: 1.50 min\n\n=== Epoch 34/50 ===\n[train step 100/704] loss 2.2238 | top1 54.70% | top3 76.22% | top5 84.00% | 530.3 img/s\n[train step 200/704] loss 2.2125 | top1 54.93% | top3 76.39% | top5 84.13% | 533.9 img/s\n[train step 300/704] loss 2.2120 | top1 55.02% | top3 76.51% | top5 84.19% | 534.6 img/s\n[train step 400/704] loss 2.2155 | top1 55.09% | top3 76.47% | top5 84.07% | 535.7 img/s\n[train step 500/704] loss 2.2163 | top1 55.02% | top3 76.47% | top5 84.06% | 536.4 img/s\n[train step 600/704] loss 2.2176 | top1 55.01% | top3 76.48% | top5 84.11% | 536.4 img/s\n[train step 700/704] loss 2.2194 | top1 54.96% | top3 76.37% | top5 84.02% | 537.6 img/s\n[Train] loss 2.2198 | top1 54.93% | top3 76.36% | top5 84.00%\n[Val]   loss 2.4702 | top1 48.44% | top3 70.16% | top5 78.64%\n Best saved to best_swin_cifar100.pt (val top1 48.44%)\nEpoch time: 1.48 min\n\n=== Epoch 35/50 ===\n[train step 100/704] loss 2.1742 | top1 56.23% | top3 77.89% | top5 85.12% | 527.4 img/s\n[train step 200/704] loss 2.1697 | top1 56.49% | top3 77.76% | top5 85.10% | 528.0 img/s\n[train step 300/704] loss 2.1784 | top1 56.19% | top3 77.59% | top5 84.97% | 525.7 img/s\n[train step 400/704] loss 2.1804 | top1 55.99% | top3 77.30% | top5 84.90% | 527.2 img/s\n[train step 500/704] loss 2.1843 | top1 55.86% | top3 77.15% | top5 84.79% | 529.7 img/s\n[train step 600/704] loss 2.1870 | top1 55.80% | top3 77.15% | top5 84.75% | 531.0 img/s\n[train step 700/704] loss 2.1905 | top1 55.76% | top3 77.08% | top5 84.62% | 530.6 img/s\n[Train] loss 2.1906 | top1 55.76% | top3 77.08% | top5 84.63%\n[Val]   loss 2.4740 | top1 48.70% | top3 69.74% | top5 78.02%\n Best saved to best_swin_cifar100.pt (val top1 48.70%)\nEpoch time: 1.50 min\n\n=== Epoch 36/50 ===\n[train step 100/704] loss 2.1326 | top1 57.19% | top3 78.94% | top5 86.11% | 515.6 img/s\n[train step 200/704] loss 2.1357 | top1 57.27% | top3 78.61% | top5 85.95% | 520.9 img/s\n[train step 300/704] loss 2.1458 | top1 57.03% | top3 78.39% | top5 85.64% | 526.4 img/s\n[train step 400/704] loss 2.1532 | top1 56.93% | top3 78.21% | top5 85.44% | 527.9 img/s\n[train step 500/704] loss 2.1483 | top1 57.06% | top3 78.27% | top5 85.50% | 529.1 img/s\n[train step 600/704] loss 2.1483 | top1 57.14% | top3 78.23% | top5 85.51% | 529.4 img/s\n[train step 700/704] loss 2.1526 | top1 56.96% | top3 78.12% | top5 85.44% | 530.5 img/s\n[Train] loss 2.1528 | top1 56.95% | top3 78.11% | top5 85.44%\n[Val]   loss 2.4879 | top1 48.38% | top3 69.36% | top5 78.58%\nEpoch time: 1.49 min\n\n=== Epoch 37/50 ===\n[train step 100/704] loss 2.1212 | top1 58.14% | top3 78.91% | top5 85.70% | 523.1 img/s\n[train step 200/704] loss 2.1226 | top1 57.84% | top3 78.90% | top5 86.02% | 531.8 img/s\n[train step 300/704] loss 2.1226 | top1 57.85% | top3 78.96% | top5 86.05% | 534.3 img/s\n[train step 400/704] loss 2.1220 | top1 57.86% | top3 79.05% | top5 86.08% | 535.0 img/s\n[train step 500/704] loss 2.1182 | top1 58.04% | top3 79.06% | top5 86.18% | 537.0 img/s\n[train step 600/704] loss 2.1203 | top1 58.08% | top3 79.05% | top5 86.06% | 536.7 img/s\n[train step 700/704] loss 2.1214 | top1 57.88% | top3 78.97% | top5 86.05% | 536.6 img/s\n[Train] loss 2.1210 | top1 57.88% | top3 78.98% | top5 86.06%\n[Val]   loss 2.4545 | top1 50.22% | top3 70.52% | top5 78.32%\n Best saved to best_swin_cifar100.pt (val top1 50.22%)\nEpoch time: 1.49 min\n\n=== Epoch 38/50 ===\n[train step 100/704] loss 2.0592 | top1 59.25% | top3 80.64% | top5 87.17% | 538.1 img/s\n[train step 200/704] loss 2.0847 | top1 58.63% | top3 79.95% | top5 86.72% | 541.4 img/s\n[train step 300/704] loss 2.0807 | top1 59.27% | top3 79.96% | top5 86.68% | 538.3 img/s\n[train step 400/704] loss 2.0838 | top1 59.20% | top3 79.91% | top5 86.75% | 537.8 img/s\n[train step 500/704] loss 2.0889 | top1 58.99% | top3 79.70% | top5 86.66% | 536.7 img/s\n[train step 600/704] loss 2.0950 | top1 58.75% | top3 79.51% | top5 86.55% | 537.4 img/s\n[train step 700/704] loss 2.0962 | top1 58.70% | top3 79.46% | top5 86.54% | 537.5 img/s\n[Train] loss 2.0961 | top1 58.71% | top3 79.46% | top5 86.54%\n[Val]   loss 2.4325 | top1 48.72% | top3 70.46% | top5 78.98%\nEpoch time: 1.47 min\n\n=== Epoch 39/50 ===\n[train step 100/704] loss 2.0697 | top1 59.69% | top3 79.81% | top5 86.78% | 517.7 img/s\n[train step 200/704] loss 2.0698 | top1 59.42% | top3 80.40% | top5 87.13% | 530.2 img/s\n[train step 300/704] loss 2.0689 | top1 59.71% | top3 80.28% | top5 87.02% | 528.5 img/s\n[train step 400/704] loss 2.0703 | top1 59.66% | top3 80.35% | top5 87.03% | 529.6 img/s\n[train step 500/704] loss 2.0677 | top1 59.71% | top3 80.41% | top5 87.12% | 529.4 img/s\n[train step 600/704] loss 2.0660 | top1 59.65% | top3 80.41% | top5 87.15% | 530.9 img/s\n[train step 700/704] loss 2.0680 | top1 59.62% | top3 80.33% | top5 87.09% | 530.9 img/s\n[Train] loss 2.0679 | top1 59.63% | top3 80.34% | top5 87.09%\n[Val]   loss 2.4282 | top1 49.96% | top3 71.02% | top5 79.48%\nEpoch time: 1.49 min\n\n=== Epoch 40/50 ===\n[train step 100/704] loss 2.0378 | top1 60.77% | top3 81.03% | top5 87.67% | 524.6 img/s\n[train step 200/704] loss 2.0355 | top1 60.61% | top3 80.88% | top5 87.55% | 528.9 img/s\n[train step 300/704] loss 2.0454 | top1 60.15% | top3 80.58% | top5 87.39% | 530.0 img/s\n[train step 400/704] loss 2.0471 | top1 60.09% | top3 80.72% | top5 87.39% | 532.6 img/s\n[train step 500/704] loss 2.0420 | top1 60.15% | top3 80.90% | top5 87.62% | 533.0 img/s\n[train step 600/704] loss 2.0421 | top1 60.21% | top3 80.85% | top5 87.61% | 533.8 img/s\n[train step 700/704] loss 2.0444 | top1 60.10% | top3 80.86% | top5 87.57% | 532.5 img/s\n[Train] loss 2.0442 | top1 60.11% | top3 80.88% | top5 87.58%\n[Val]   loss 2.4417 | top1 49.66% | top3 70.62% | top5 78.68%\nEpoch time: 1.49 min\n\n=== Epoch 41/50 ===\n[train step 100/704] loss 2.0143 | top1 61.47% | top3 81.45% | top5 88.25% | 515.7 img/s\n[train step 200/704] loss 2.0177 | top1 61.42% | top3 81.64% | top5 88.24% | 533.1 img/s\n[train step 300/704] loss 2.0178 | top1 61.27% | top3 81.52% | top5 88.07% | 535.5 img/s\n[train step 400/704] loss 2.0137 | top1 61.48% | top3 81.61% | top5 88.06% | 537.0 img/s\n[train step 500/704] loss 2.0151 | top1 61.51% | top3 81.49% | top5 88.00% | 538.5 img/s\n[train step 600/704] loss 2.0170 | top1 61.51% | top3 81.55% | top5 88.00% | 539.7 img/s\n[train step 700/704] loss 2.0208 | top1 61.42% | top3 81.44% | top5 87.93% | 540.7 img/s\n[Train] loss 2.0209 | top1 61.42% | top3 81.44% | top5 87.92%\n[Val]   loss 2.4312 | top1 49.98% | top3 71.20% | top5 79.58%\nEpoch time: 1.47 min\n\n=== Epoch 42/50 ===\n[train step 100/704] loss 2.0022 | top1 61.19% | top3 81.44% | top5 88.16% | 529.3 img/s\n[train step 200/704] loss 1.9978 | top1 61.35% | top3 81.79% | top5 88.14% | 529.5 img/s\n[train step 300/704] loss 1.9931 | top1 62.07% | top3 81.99% | top5 88.33% | 531.7 img/s\n[train step 400/704] loss 1.9922 | top1 62.21% | top3 82.07% | top5 88.27% | 534.7 img/s\n[train step 500/704] loss 1.9982 | top1 62.12% | top3 81.90% | top5 88.19% | 537.7 img/s\n[train step 600/704] loss 1.9988 | top1 62.07% | top3 81.90% | top5 88.24% | 538.3 img/s\n[train step 700/704] loss 2.0021 | top1 61.90% | top3 81.81% | top5 88.23% | 537.8 img/s\n[Train] loss 2.0017 | top1 61.92% | top3 81.83% | top5 88.24%\n[Val]   loss 2.4213 | top1 49.40% | top3 71.36% | top5 79.64%\nEpoch time: 1.47 min\n\n=== Epoch 43/50 ===\n[train step 100/704] loss 1.9822 | top1 62.83% | top3 82.02% | top5 88.25% | 521.6 img/s\n[train step 200/704] loss 1.9832 | top1 62.67% | top3 82.34% | top5 88.59% | 530.2 img/s\n[train step 300/704] loss 1.9826 | top1 62.50% | top3 82.33% | top5 88.73% | 531.4 img/s\n[train step 400/704] loss 1.9858 | top1 62.35% | top3 82.13% | top5 88.64% | 532.8 img/s\n[train step 500/704] loss 1.9815 | top1 62.42% | top3 82.38% | top5 88.82% | 534.2 img/s\n[train step 600/704] loss 1.9819 | top1 62.52% | top3 82.36% | top5 88.76% | 535.4 img/s\n[train step 700/704] loss 1.9830 | top1 62.43% | top3 82.42% | top5 88.76% | 536.0 img/s\n[Train] loss 1.9830 | top1 62.42% | top3 82.42% | top5 88.75%\n[Val]   loss 2.4145 | top1 51.04% | top3 71.30% | top5 79.88%\n Best saved to best_swin_cifar100.pt (val top1 51.04%)\nEpoch time: 1.49 min\n\n=== Epoch 44/50 ===\n[train step 100/704] loss 1.9691 | top1 62.41% | top3 82.42% | top5 89.44% | 525.8 img/s\n[train step 200/704] loss 1.9613 | top1 63.08% | top3 82.70% | top5 89.17% | 531.3 img/s\n[train step 300/704] loss 1.9637 | top1 63.07% | top3 82.79% | top5 89.08% | 534.1 img/s\n[train step 400/704] loss 1.9713 | top1 62.79% | top3 82.51% | top5 88.87% | 531.8 img/s\n[train step 500/704] loss 1.9714 | top1 62.85% | top3 82.67% | top5 88.96% | 532.3 img/s\n[train step 600/704] loss 1.9754 | top1 62.76% | top3 82.62% | top5 88.89% | 533.6 img/s\n[train step 700/704] loss 1.9762 | top1 62.67% | top3 82.59% | top5 88.87% | 534.6 img/s\n[Train] loss 1.9760 | top1 62.68% | top3 82.58% | top5 88.87%\n[Val]   loss 2.4096 | top1 50.46% | top3 72.00% | top5 79.52%\nEpoch time: 1.48 min\n\n=== Epoch 45/50 ===\n[train step 100/704] loss 1.9566 | top1 64.16% | top3 83.28% | top5 88.88% | 539.2 img/s\n[train step 200/704] loss 1.9448 | top1 64.14% | top3 83.54% | top5 89.34% | 542.3 img/s\n[train step 300/704] loss 1.9457 | top1 63.93% | top3 83.51% | top5 89.26% | 538.7 img/s\n[train step 400/704] loss 1.9571 | top1 63.80% | top3 83.19% | top5 89.12% | 539.8 img/s\n[train step 500/704] loss 1.9597 | top1 63.66% | top3 83.01% | top5 89.04% | 537.7 img/s\n[train step 600/704] loss 1.9608 | top1 63.59% | top3 82.87% | top5 89.02% | 537.2 img/s\n[train step 700/704] loss 1.9607 | top1 63.59% | top3 82.88% | top5 89.00% | 538.0 img/s\n[Train] loss 1.9611 | top1 63.56% | top3 82.88% | top5 89.00%\n[Val]   loss 2.4162 | top1 50.02% | top3 71.52% | top5 79.84%\nEpoch time: 1.47 min\n\n=== Epoch 46/50 ===\n[train step 100/704] loss 1.9342 | top1 63.92% | top3 83.66% | top5 89.83% | 527.9 img/s\n[train step 200/704] loss 1.9412 | top1 63.91% | top3 83.29% | top5 89.52% | 535.1 img/s\n[train step 300/704] loss 1.9493 | top1 63.64% | top3 83.22% | top5 89.42% | 533.7 img/s\n[train step 400/704] loss 1.9472 | top1 63.64% | top3 83.30% | top5 89.43% | 534.9 img/s\n[train step 500/704] loss 1.9463 | top1 63.65% | top3 83.31% | top5 89.47% | 534.0 img/s\n[train step 600/704] loss 1.9465 | top1 63.63% | top3 83.34% | top5 89.40% | 534.7 img/s\n[train step 700/704] loss 1.9472 | top1 63.67% | top3 83.34% | top5 89.32% | 537.0 img/s\n[Train] loss 1.9474 | top1 63.66% | top3 83.33% | top5 89.32%\n[Val]   loss 2.3989 | top1 50.34% | top3 72.12% | top5 80.18%\nEpoch time: 1.47 min\n\n=== Epoch 47/50 ===\n[train step 100/704] loss 1.9172 | top1 64.00% | top3 84.36% | top5 90.14% | 516.6 img/s\n[train step 200/704] loss 1.9306 | top1 64.02% | top3 83.66% | top5 89.61% | 527.5 img/s\n[train step 300/704] loss 1.9375 | top1 63.74% | top3 83.48% | top5 89.51% | 528.6 img/s\n[train step 400/704] loss 1.9379 | top1 63.85% | top3 83.51% | top5 89.46% | 531.8 img/s\n[train step 500/704] loss 1.9394 | top1 63.86% | top3 83.54% | top5 89.52% | 534.2 img/s\n[train step 600/704] loss 1.9346 | top1 63.98% | top3 83.68% | top5 89.57% | 535.4 img/s\n[train step 700/704] loss 1.9373 | top1 63.95% | top3 83.62% | top5 89.52% | 536.3 img/s\n[Train] loss 1.9372 | top1 63.95% | top3 83.64% | top5 89.53%\n[Val]   loss 2.4365 | top1 50.24% | top3 71.16% | top5 79.28%\nEpoch time: 1.47 min\n\n=== Epoch 48/50 ===\n[train step 100/704] loss 1.9235 | top1 64.41% | top3 83.75% | top5 89.69% | 527.8 img/s\n[train step 200/704] loss 1.9325 | top1 63.97% | top3 83.48% | top5 89.42% | 525.8 img/s\n[train step 300/704] loss 1.9412 | top1 63.68% | top3 83.30% | top5 89.33% | 530.0 img/s\n[train step 400/704] loss 1.9403 | top1 63.84% | top3 83.23% | top5 89.30% | 531.0 img/s\n[train step 500/704] loss 1.9396 | top1 63.96% | top3 83.36% | top5 89.34% | 531.3 img/s\n[train step 600/704] loss 1.9415 | top1 63.95% | top3 83.29% | top5 89.30% | 532.0 img/s\n[train step 700/704] loss 1.9426 | top1 63.89% | top3 83.26% | top5 89.29% | 532.3 img/s\n[Train] loss 1.9426 | top1 63.89% | top3 83.27% | top5 89.29%\n[Val]   loss 2.4050 | top1 50.84% | top3 71.42% | top5 79.80%\nEpoch time: 1.49 min\n\n=== Epoch 49/50 ===\n[train step 100/704] loss 1.9290 | top1 64.20% | top3 83.53% | top5 89.67% | 519.9 img/s\n[train step 200/704] loss 1.9194 | top1 64.53% | top3 84.01% | top5 89.95% | 525.4 img/s\n[train step 300/704] loss 1.9248 | top1 64.36% | top3 84.06% | top5 89.82% | 530.4 img/s\n[train step 400/704] loss 1.9223 | top1 64.51% | top3 83.97% | top5 89.77% | 532.8 img/s\n[train step 500/704] loss 1.9264 | top1 64.48% | top3 83.83% | top5 89.68% | 534.5 img/s\n[train step 600/704] loss 1.9279 | top1 64.39% | top3 83.75% | top5 89.65% | 535.6 img/s\n[train step 700/704] loss 1.9278 | top1 64.46% | top3 83.75% | top5 89.64% | 537.1 img/s\n[Train] loss 1.9275 | top1 64.46% | top3 83.77% | top5 89.66%\n[Val]   loss 2.4229 | top1 50.78% | top3 71.64% | top5 79.90%\nEpoch time: 1.47 min\n\n=== Epoch 50/50 ===\n[train step 100/704] loss 1.9501 | top1 63.73% | top3 83.59% | top5 89.53% | 528.4 img/s\n[train step 200/704] loss 1.9499 | top1 63.72% | top3 83.58% | top5 89.24% | 529.1 img/s\n[train step 300/704] loss 1.9512 | top1 63.57% | top3 83.37% | top5 89.22% | 531.1 img/s\n[train step 400/704] loss 1.9484 | top1 63.77% | top3 83.31% | top5 89.19% | 532.2 img/s\n[train step 500/704] loss 1.9452 | top1 63.81% | top3 83.34% | top5 89.30% | 533.8 img/s\n[train step 600/704] loss 1.9433 | top1 63.89% | top3 83.35% | top5 89.35% | 535.5 img/s\n[train step 700/704] loss 1.9394 | top1 64.05% | top3 83.39% | top5 89.37% | 537.8 img/s\n[Train] loss 1.9392 | top1 64.07% | top3 83.40% | top5 89.37%\n[Val]   loss 2.3925 | top1 52.34% | top3 73.02% | top5 80.42%\n Best saved to best_swin_cifar100.pt (val top1 52.34%)\nEpoch time: 1.48 min\n","output_type":"stream"}],"execution_count":27}]}