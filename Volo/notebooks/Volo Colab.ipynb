{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "JBBQVf2br3RQ"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.transforms import RandAugment\n",
        "\n",
        "def get_cifar100_datasets(\n",
        "    data_dir: str = \"./data\",\n",
        "    val_split: float = 0.0,\n",
        "    ra_num_ops: int = 2,\n",
        "    ra_magnitude: int = 7,\n",
        "    random_erasing_p: float = 0.25,\n",
        "    erasing_scale=(0.02, 0.20),\n",
        "    erasing_ratio=(0.3, 3.3),\n",
        "    img_size: int = 32,):\n",
        "\n",
        "    \"\"\"\n",
        "    CIFAR-100 datasets con augmentations \"mix-friendly\":\n",
        "    diseñadas para complementar Mixup/CutMix (en el loop) sin pasarse.\n",
        "\n",
        "    img_size:\n",
        "      - 32 (default): CIFAR nativo.\n",
        "      - >32: upsample (p.ej. 64) para experimentos (más tokens/compute).\n",
        "    \"\"\"\n",
        "    if img_size < 32:\n",
        "        raise ValueError(f\"img_size must be >= 32 for CIFAR-100. Got {img_size}.\")\n",
        "\n",
        "    cifar100_mean = (0.5071, 0.4867, 0.4408)\n",
        "    cifar100_std  = (0.2675, 0.2565, 0.2761)\n",
        "\n",
        "    # Si subimos resolución, primero hacemos resize y adaptamos crop/padding.\n",
        "    # Padding recomendado proporcional: 32->4, 64->8, etc.\n",
        "\n",
        "    crop_padding = max(4, img_size // 8)\n",
        "\n",
        "    train_ops = []\n",
        "    if img_size != 32:\n",
        "        train_ops.append(transforms.Resize(img_size, interpolation=transforms.InterpolationMode.BICUBIC))\n",
        "\n",
        "    train_ops += [\n",
        "        transforms.RandomCrop(img_size, padding=crop_padding),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        RandAugment(num_ops=ra_num_ops, magnitude=ra_magnitude),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(cifar100_mean, cifar100_std),\n",
        "        transforms.RandomErasing(\n",
        "            p=random_erasing_p,\n",
        "            scale=erasing_scale,\n",
        "            ratio=erasing_ratio,\n",
        "            value=\"random\",),]\n",
        "\n",
        "    train_transform = transforms.Compose(train_ops)\n",
        "\n",
        "    test_ops = []\n",
        "    if img_size != 32:\n",
        "        test_ops.append(transforms.Resize(img_size, interpolation=transforms.InterpolationMode.BICUBIC))\n",
        "\n",
        "    test_ops += [\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(cifar100_mean, cifar100_std),]\n",
        "\n",
        "    test_transform = transforms.Compose(test_ops)\n",
        "\n",
        "    full_train_dataset = datasets.CIFAR100(\n",
        "        root=data_dir, train=True, download=True, transform=train_transform)\n",
        "\n",
        "    test_dataset = datasets.CIFAR100(\n",
        "        root=data_dir, train=False, download=True, transform=test_transform)\n",
        "\n",
        "    if val_split > 0.0:\n",
        "        n_total = len(full_train_dataset)\n",
        "        n_val = int(n_total * val_split)\n",
        "        n_train = n_total - n_val\n",
        "        train_dataset, val_dataset = random_split(\n",
        "            full_train_dataset,\n",
        "            [n_train, n_val],\n",
        "            generator=torch.Generator().manual_seed(7),)\n",
        "\n",
        "    else:\n",
        "        train_dataset = full_train_dataset\n",
        "        val_dataset = None\n",
        "\n",
        "    return train_dataset, val_dataset, test_dataset\n",
        "\n",
        "\n",
        "def get_cifar100_dataloaders(\n",
        "    batch_size: int = 128,\n",
        "    data_dir: str = \"./data\",\n",
        "    num_workers: int = 2,\n",
        "    val_split: float = 0.0,\n",
        "    pin_memory: bool = True,\n",
        "    ra_num_ops: int = 2,\n",
        "    ra_magnitude: int = 7,\n",
        "    random_erasing_p: float = 0.25,\n",
        "    img_size: int = 32,):\n",
        "    \"\"\"\n",
        "    Dataloaders CIFAR-100 listos para entrenar con Mixup/CutMix en el loop.\n",
        "    Augmentations no tan agresivas.\n",
        "\n",
        "    img_size:\n",
        "      - 32 (default): CIFAR nativo.\n",
        "      - 64: experimento de upsample (ojo: más compute).\n",
        "    \"\"\"\n",
        "    train_ds, val_ds, test_ds = get_cifar100_datasets(\n",
        "        data_dir=data_dir,\n",
        "        val_split=val_split,\n",
        "        ra_num_ops=ra_num_ops,\n",
        "        ra_magnitude=ra_magnitude,\n",
        "        random_erasing_p=random_erasing_p,\n",
        "        img_size=img_size,)\n",
        "\n",
        "    train_loader = DataLoader(\n",
        "        train_ds,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=True,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=pin_memory,\n",
        "        persistent_workers=(num_workers > 0),)\n",
        "\n",
        "    val_loader = None\n",
        "    if val_ds is not None:\n",
        "        val_loader = DataLoader(\n",
        "            val_ds,\n",
        "            batch_size=batch_size,\n",
        "            shuffle=False,\n",
        "            num_workers=num_workers,\n",
        "            pin_memory=pin_memory,\n",
        "            persistent_workers=(num_workers > 0),)\n",
        "\n",
        "    test_loader = DataLoader(\n",
        "        test_ds,\n",
        "        batch_size=batch_size,\n",
        "        shuffle=False,\n",
        "        num_workers=num_workers,\n",
        "        pin_memory=pin_memory,\n",
        "        persistent_workers=(num_workers > 0),)\n",
        "\n",
        "    return train_loader, val_loader, test_loader"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_loader, val_loader, test_loader = get_cifar100_dataloaders(\n",
        "    batch_size=128,\n",
        "    data_dir=\"./data/cifar100\",\n",
        "    num_workers=2,\n",
        "    val_split=0.1,\n",
        "    img_size=32)"
      ],
      "metadata": {
        "id": "nq5hFdSnsarL"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class PatchEmbeddingConv(nn.Module):\n",
        "    \"\"\"\n",
        "    Patch embedding estilo Swin.\n",
        "\n",
        "    - Conv2d con kernel=stride=patch_size para convertir imagen -> grilla de patches.\n",
        "    - Devuelve el mapa 2D en formato canal-al-final: [B, Hp, Wp, D],\n",
        "      (más cómodo para window partition).\n",
        "    - Opcionalmente devuelve tokens [B, N, D].\n",
        "    - Opcional padding automático si H/W no son divisibles por patch_size.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        patch_size: int | tuple[int, int] = 4,\n",
        "        in_chans: int = 3,\n",
        "        embed_dim: int = 192,\n",
        "        norm_layer: type[nn.Module] | None = nn.LayerNorm,\n",
        "        pad_if_needed: bool = True,\n",
        "        return_tokens: bool = True):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        if isinstance(patch_size, int):\n",
        "            patch_size = (patch_size, patch_size)\n",
        "\n",
        "        self.patch_size = patch_size  # (Ph, Pw)\n",
        "        self.in_chans = in_chans\n",
        "        self.embed_dim = embed_dim\n",
        "        self.pad_if_needed = pad_if_needed\n",
        "        self.return_tokens = return_tokens\n",
        "\n",
        "        # [B, C, H, W] -> [B, D, Hp, Wp]\n",
        "        self.proj = nn.Conv2d(\n",
        "            in_channels=in_chans,\n",
        "            out_channels=embed_dim,\n",
        "            kernel_size=patch_size,\n",
        "            stride=patch_size,\n",
        "            bias=True,)\n",
        "\n",
        "        # En Swin normalmente LayerNorm sobre la última dimensión\n",
        "        self.norm = norm_layer(embed_dim) if norm_layer is not None else None\n",
        "\n",
        "    def forward(self, x: torch.Tensor):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: [B, C, H, W]\n",
        "\n",
        "        Returns:\n",
        "            x_map:    [B, Hp, Wp, D]\n",
        "            (Hp, Wp): tamaño espacial en patches\n",
        "            x_tokens (opcional): [B, N, D]\n",
        "            pad_hw (opcional): (pad_h, pad_w) aplicados a la imagen\n",
        "        \"\"\"\n",
        "        B, C, H, W = x.shape\n",
        "        Ph, Pw = self.patch_size\n",
        "\n",
        "        pad_h = (Ph - (H % Ph)) % Ph\n",
        "        pad_w = (Pw - (W % Pw)) % Pw\n",
        "\n",
        "        if (pad_h != 0 or pad_w != 0):\n",
        "            if not self.pad_if_needed:\n",
        "                raise AssertionError(\n",
        "                    f\"Image size ({H}x{W}) no es divisible por patch_size {self.patch_size} \"\n",
        "                    f\"y pad_if_needed=False.\")\n",
        "\n",
        "            x = F.pad(x, (0, pad_w, 0, pad_h))\n",
        "\n",
        "        # [B, D, Hp, Wp]\n",
        "        x = self.proj(x)\n",
        "        Hp, Wp = x.shape[2], x.shape[3]\n",
        "\n",
        "        # canal al final -> [B, Hp, Wp, D]\n",
        "        x_map = x.permute(0, 2, 3, 1).contiguous()\n",
        "\n",
        "        if self.norm is not None:\n",
        "            x_map = self.norm(x_map)\n",
        "\n",
        "        if self.return_tokens:\n",
        "            x_tokens = x_map.view(B, Hp * Wp, self.embed_dim)\n",
        "            return x_map, (Hp, Wp), x_tokens, (pad_h, pad_w)\n",
        "\n",
        "        return x_map, (Hp, Wp), (pad_h, pad_w)\n",
        "\n"
      ],
      "metadata": {
        "id": "bsqDnkuOs2kr"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_patch_embedding_conv():\n",
        "    torch.manual_seed(0)\n",
        "\n",
        "    #  tamaño divisible (64 con patch=4)\n",
        "    B, C, H, W = 2, 3, 64, 64\n",
        "    x = torch.randn(B, C, H, W)\n",
        "\n",
        "    pe = PatchEmbeddingConv(\n",
        "        patch_size=4,\n",
        "        in_chans=3,\n",
        "        embed_dim=192,\n",
        "        norm_layer=torch.nn.LayerNorm,\n",
        "        pad_if_needed=True,\n",
        "        return_tokens=True,)\n",
        "\n",
        "    x_map, (Hp, Wp), x_tok, (pad_h, pad_w) = pe(x)\n",
        "\n",
        "    assert x_map.shape == (B, Hp, Wp, 192)\n",
        "    assert x_tok.shape == (B, Hp * Wp, 192)\n",
        "    assert (pad_h, pad_w) == (0, 0)\n",
        "    assert (Hp, Wp) == (H // 4, W // 4)\n",
        "\n",
        "    print(\"[OK] PatchEmbeddingConv divisible:\",\n",
        "          \"x_map\", tuple(x_map.shape),\n",
        "          \"| x_tok\", tuple(x_tok.shape),\n",
        "          \"| pad\", (pad_h, pad_w))\n",
        "\n",
        "    # tamaño NO divisible (65x63 con patch=4) -> debería paddear\n",
        "    H2, W2 = 65, 63\n",
        "    x2 = torch.randn(B, C, H2, W2)\n",
        "\n",
        "    x_map2, (Hp2, Wp2), x_tok2, (pad_h2, pad_w2) = pe(x2)\n",
        "\n",
        "    assert (H2 + pad_h2) % 4 == 0\n",
        "    assert (W2 + pad_w2) % 4 == 0\n",
        "    assert x_map2.shape == (B, Hp2, Wp2, 192)\n",
        "    assert x_tok2.shape == (B, Hp2 * Wp2, 192)\n",
        "\n",
        "    print(\"[OK] PatchEmbeddingConv non-divisible:\",\n",
        "          \"input\", (H2, W2),\n",
        "          \"| padded by\", (pad_h2, pad_w2),\n",
        "          \"| patches\", (Hp2, Wp2),\n",
        "          \"| x_map\", tuple(x_map2.shape))\n",
        "\n",
        "test_patch_embedding_conv()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WwDktYWGxF5U",
        "outputId": "fdfe21dd-4b2e-4d04-d1eb-ee5fe0581be5"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OK] PatchEmbeddingConv divisible: x_map (2, 16, 16, 192) | x_tok (2, 256, 192) | pad (0, 0)\n",
            "[OK] PatchEmbeddingConv non-divisible: input (65, 63) | padded by (3, 1) | patches (17, 16) | x_map (2, 17, 16, 192)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class OutlookAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Outlook Attention (VOLO): agregación local dinámica sobre ventanas.\n",
        "\n",
        "    Entrada:  x_map [B, H, W, C]  (channel-last)\n",
        "    Salida:   y_map [B, H, W, C]\n",
        "\n",
        "    Parámetros:\n",
        "      - dim: canales C\n",
        "      - kernel_size: k (vecindario k×k)\n",
        "      - stride: s (si s>1 hace downsample tipo \"outlook pooling\"; para CIFAR típicamente s=1)\n",
        "      - num_heads: h (partimos canales en cabezas, como MHSA)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim: int,\n",
        "        num_heads: int = 6,\n",
        "        kernel_size: int = 3,\n",
        "        stride: int = 1,\n",
        "        attn_drop: float = 0.0,\n",
        "        proj_drop: float = 0.0,):\n",
        "\n",
        "        super().__init__()\n",
        "        assert dim % num_heads == 0, \"dim must be divisible by num_heads\"\n",
        "\n",
        "        self.dim = dim\n",
        "        self.num_heads = num_heads\n",
        "        self.head_dim = dim // num_heads\n",
        "\n",
        "        self.kernel_size = kernel_size\n",
        "        self.stride = stride\n",
        "\n",
        "        # Genera pesos de atención por posición: [B, H, W, heads * k*k]\n",
        "        self.attn = nn.Linear(dim, num_heads * kernel_size * kernel_size, bias=True)\n",
        "\n",
        "        # Proyección para values (antes de unfold)\n",
        "        self.v = nn.Linear(dim, dim, bias=True)\n",
        "\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Linear(dim, dim, bias=True)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "\n",
        "    def forward(self, x_map: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        x_map: [B, H, W, C]\n",
        "        \"\"\"\n",
        "        B, H, W, C = x_map.shape\n",
        "        k = self.kernel_size\n",
        "        s = self.stride\n",
        "        heads = self.num_heads\n",
        "        hd = self.head_dim\n",
        "\n",
        "        # attention weights\n",
        "        a = self.attn(x_map)\n",
        "        # si stride>1, la atención se evalúa en posiciones downsampled\n",
        "        if s > 1:\n",
        "            # downsample espacialmente (simple avg pool sobre channel-last)\n",
        "            a = a.permute(0, 3, 1, 2)                       # [B, heads*k*k, H, W]\n",
        "            a = F.avg_pool2d(a, kernel_size=s, stride=s)    # [B, heads*k*k, Hs, Ws]\n",
        "            a = a.permute(0, 2, 3, 1).contiguous()          # [B, Hs, Ws, heads*k*k]\n",
        "\n",
        "        Hs, Ws = a.shape[1], a.shape[2]\n",
        "        a = a.view(B, Hs * Ws, heads, k * k)\n",
        "        a = F.softmax(a, dim=-1)\n",
        "        a = self.attn_drop(a)\n",
        "\n",
        "        # values map\n",
        "        v = self.v(x_map)\n",
        "        v = v.permute(0, 3, 1, 2).contiguous()\n",
        "\n",
        "        # unfold extrae vecindarios k×k para cada posición (con padding para \"same\")\n",
        "        pad = k // 2\n",
        "        v_unf = F.unfold(v, kernel_size=k, padding=pad, stride=s)\n",
        "        v_unf = v_unf.view(B, C, k * k, Hs * Ws).permute(0, 3, 1, 2).contiguous()\n",
        "        v_unf = v_unf.view(B, Hs * Ws, heads, hd, k * k)\n",
        "\n",
        "        # apply attention: weighted sum over neighborhood\n",
        "        # a:     [B, Hs*Ws, heads, k*k]\n",
        "        # v_unf: [B, Hs*Ws, heads, hd, k*k]\n",
        "        y = (v_unf * a.unsqueeze(3)).sum(dim=-1)\n",
        "        y = y.reshape(B, Hs * Ws, C)              # concat heads\n",
        "\n",
        "        # fold back to spatial map\n",
        "        y_map = y.view(B, Hs, Ws, C)\n",
        "\n",
        "        y_map = self.proj(y_map)\n",
        "        y_map = self.proj_drop(y_map)\n",
        "        return y_map"
      ],
      "metadata": {
        "id": "HOk76_dwwpZq"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_outlook_attention_stride1():\n",
        "    torch.manual_seed(0)\n",
        "\n",
        "    B, H, W, C = 2, 16, 16, 192\n",
        "    x_map = torch.randn(B, H, W, C, requires_grad=True)\n",
        "\n",
        "    oa = OutlookAttention(\n",
        "        dim=C,\n",
        "        num_heads=6,\n",
        "        kernel_size=3,\n",
        "        stride=1,\n",
        "        attn_drop=0.0,\n",
        "        proj_drop=0.0)\n",
        "\n",
        "    y = oa(x_map)\n",
        "    assert y.shape == x_map.shape, f\"Expected {x_map.shape}, got {y.shape}\"\n",
        "\n",
        "    loss = y.mean()\n",
        "    loss.backward()\n",
        "\n",
        "    assert x_map.grad is not None, \"No gradient flowed to input!\"\n",
        "    assert torch.isfinite(x_map.grad).all(), \"Non-finite grads!\"\n",
        "\n",
        "    print(\"[OK] OutlookAttention stride=1:\",\n",
        "          \"in\", tuple(x_map.shape),\n",
        "          \"| out\", tuple(y.shape),\n",
        "          \"| grad mean\", float(x_map.grad.abs().mean()))\n",
        "\n",
        "test_outlook_attention_stride1()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pPAUVyTQxM5d",
        "outputId": "40dc70c1-7cfe-46ae-bd5a-906ded533d72"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OK] OutlookAttention stride=1: in (2, 16, 16, 192) | out (2, 16, 16, 192) | grad mean 2.7283142571832286e-06\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_outlook_attention_stride2():\n",
        "    torch.manual_seed(0)\n",
        "\n",
        "    B, H, W, C = 2, 16, 16, 192\n",
        "    x_map = torch.randn(B, H, W, C, requires_grad=True)\n",
        "\n",
        "    oa = OutlookAttention(\n",
        "        dim=C,\n",
        "        num_heads=6,\n",
        "        kernel_size=3,\n",
        "        stride=2,\n",
        "        attn_drop=0.0,\n",
        "        proj_drop=0.0)\n",
        "\n",
        "    y = oa(x_map)\n",
        "\n",
        "    assert y.shape[0] == B and y.shape[-1] == C\n",
        "    assert y.shape[1] == H // 2 and y.shape[2] == W // 2, f\"Got {y.shape[1:3]}\"\n",
        "\n",
        "    loss = y.mean()\n",
        "    loss.backward()\n",
        "    assert x_map.grad is not None\n",
        "    assert torch.isfinite(x_map.grad).all()\n",
        "\n",
        "    print(\"[OK] OutlookAttention stride=2:\",\n",
        "          \"in\", (B, H, W, C),\n",
        "          \"| out\", tuple(y.shape))\n",
        "\n",
        "test_outlook_attention_stride2()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tGRO09XqxQMJ",
        "outputId": "dd5b47dd-f474-4f03-bda5-411b8318baf7"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OK] OutlookAttention stride=2: in (2, 16, 16, 192) | out (2, 8, 8, 192)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DropPath(nn.Module):\n",
        "    def __init__(self, drop_prob: float = 0.0):\n",
        "        super().__init__()\n",
        "        self.drop_prob = float(drop_prob)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        if self.drop_prob == 0.0 or not self.training:\n",
        "            return x\n",
        "\n",
        "        keep_prob = 1.0 - self.drop_prob\n",
        "        shape = (x.shape[0],) + (1,) * (x.ndim - 1)\n",
        "        random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
        "        random_tensor.floor_()\n",
        "        return x.div(keep_prob) * random_tensor\n",
        "\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, dim: int, hidden_dim: int, drop: float = 0.0):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(dim, hidden_dim)\n",
        "        self.act = nn.GELU()\n",
        "        self.fc2 = nn.Linear(hidden_dim, dim)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop(x)\n",
        "        return x\n",
        "\n",
        "class OutlookerBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Bloque VOLO Outlooker:\n",
        "      x -> LN -> OutlookAttention -> DropPath + residual\n",
        "        -> LN -> MLP -> DropPath + residual\n",
        "\n",
        "    Input/Output: [B, H, W, C]\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim: int,\n",
        "        num_heads: int,\n",
        "        kernel_size: int = 3,\n",
        "        stride: int = 1,\n",
        "        mlp_ratio: float = 4.0,\n",
        "        attn_drop: float = 0.0,\n",
        "        proj_drop: float = 0.0,\n",
        "        drop_path: float = 0.0,\n",
        "        mlp_drop: float = 0.0):\n",
        "\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(dim)\n",
        "\n",
        "        self.attn = OutlookAttention(\n",
        "            dim=dim,\n",
        "            num_heads=num_heads,\n",
        "            kernel_size=kernel_size,\n",
        "            stride=stride,\n",
        "            attn_drop=attn_drop,\n",
        "            proj_drop=proj_drop,)\n",
        "\n",
        "        self.drop_path1 = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n",
        "\n",
        "        self.norm2 = nn.LayerNorm(dim)\n",
        "        hidden_dim = int(dim * mlp_ratio)\n",
        "\n",
        "        self.mlp = MLP(dim=dim, hidden_dim=hidden_dim, drop=mlp_drop)\n",
        "        self.drop_path2 = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n",
        "\n",
        "    def forward(self, x_map: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        x_map: tensor de forma (B, C, H, W) o (B, N, C), según el bloque.\n",
        "        \"\"\"\n",
        "\n",
        "        # Primer sub-bloque: Norm -> Atención -> DropPath -> Residual\n",
        "\n",
        "        # Normalización del input\n",
        "        x_norm_1 = self.norm1(x_map)\n",
        "\n",
        "        # Atención\n",
        "        attn_out = self.attn(x_norm_1)\n",
        "        attn_out = self.drop_path1(attn_out)\n",
        "\n",
        "        # Suma residual\n",
        "        x_map = x_map + attn_out\n",
        "\n",
        "        # Segundo sub-bloque: Norm -> MLP -> DropPath -> Residual ---\n",
        "\n",
        "        x_norm_2 = self.norm2(x_map)\n",
        "\n",
        "        # MLP\n",
        "        mlp_out = self.mlp(x_norm_2)\n",
        "        mlp_out = self.drop_path2(mlp_out)\n",
        "\n",
        "        # Segunda suma residual\n",
        "        x_out = x_map + mlp_out\n",
        "\n",
        "        return x_out"
      ],
      "metadata": {
        "id": "eZ5e4lAMxT00"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_outlooker_block():\n",
        "    torch.manual_seed(0)\n",
        "\n",
        "    B, H, W, C = 2, 16, 16, 192\n",
        "    x_map = torch.randn(B, H, W, C, requires_grad=True)\n",
        "\n",
        "    blk = OutlookerBlock(\n",
        "        dim=C,\n",
        "        num_heads=6,\n",
        "        kernel_size=3,\n",
        "        stride=1,\n",
        "        mlp_ratio=4.0,\n",
        "        attn_drop=0.0,\n",
        "        proj_drop=0.0,\n",
        "        drop_path=0.0,\n",
        "        mlp_drop=0.0,)\n",
        "\n",
        "    y = blk(x_map)\n",
        "    assert y.shape == x_map.shape\n",
        "\n",
        "    y.mean().backward()\n",
        "    assert x_map.grad is not None\n",
        "    assert torch.isfinite(x_map.grad).all()\n",
        "\n",
        "    print(\"[OK] OutlookerBlock:\",\n",
        "          \"in/out\", tuple(y.shape),\n",
        "          \"| grad mean\", float(x_map.grad.abs().mean()))\n",
        "\n",
        "test_outlooker_block()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ncbCj-egxW5v",
        "outputId": "f9922edc-927c-40f7-86d6-bc8cf0dd8ec8"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OK] OutlookerBlock: in/out (2, 16, 16, 192) | grad mean 1.0187763109570369e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_embed_then_outlook(img_size=64, patch_size=4, dim=192, heads=6):\n",
        "    torch.manual_seed(0)\n",
        "\n",
        "    B = 2\n",
        "    x = torch.randn(B, 3, img_size, img_size, requires_grad=True)\n",
        "\n",
        "    pe = PatchEmbeddingConv(\n",
        "        patch_size=patch_size,\n",
        "        in_chans=3,\n",
        "        embed_dim=dim,\n",
        "        norm_layer=torch.nn.LayerNorm,\n",
        "        pad_if_needed=True,\n",
        "        return_tokens=True,)\n",
        "\n",
        "    blk = OutlookerBlock(\n",
        "        dim=dim,\n",
        "        num_heads=heads,\n",
        "        kernel_size=3,\n",
        "        stride=1,\n",
        "        mlp_ratio=4.0,\n",
        "        drop_path=0.0,)\n",
        "\n",
        "    x_map, (Hp, Wp), x_tok, pad_hw = pe(x)\n",
        "    y_map = blk(x_map)\n",
        "\n",
        "    assert y_map.shape == x_map.shape == (B, Hp, Wp, dim)\n",
        "\n",
        "    # grad\n",
        "    y_map.mean().backward()\n",
        "    assert x.grad is not None and torch.isfinite(x.grad).all()\n",
        "\n",
        "    print(\"[OK] Embed->Outlook:\",\n",
        "          \"img\", (img_size, img_size),\n",
        "          \"| patches\", (Hp, Wp),\n",
        "          \"| map\", tuple(y_map.shape),\n",
        "          \"| pad\", pad_hw)\n",
        "\n",
        "test_embed_then_outlook(img_size=32)\n",
        "test_embed_then_outlook(img_size=64)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IcrvTt48x2gj",
        "outputId": "12405db1-f6e4-4409-83d7-c9365bf00583"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OK] Embed->Outlook: img (32, 32) | patches (8, 8) | map (2, 8, 8, 192) | pad (0, 0)\n",
            "[OK] Embed->Outlook: img (64, 64) | patches (16, 16) | map (2, 16, 16, 192) | pad (0, 0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class VOLOStage(nn.Module):\n",
        "    \"\"\"\n",
        "    Un stage VOLO basado en OutlookerBlocks.\n",
        "\n",
        "    Mantiene el formato channel-last:\n",
        "      Input:  [B, H, W, C]\n",
        "      Output: [B, H, W, C]  (si stride=1)\n",
        "    Si quisieras un stage que haga downsample, usa stride>1 en los bloques\n",
        "    (pero en CIFAR te recomiendo stride=1 en el stage inicial).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim: int,\n",
        "        depth: int,\n",
        "        num_heads: int,\n",
        "        kernel_size: int = 3,\n",
        "        stride: int = 1,\n",
        "        mlp_ratio: float = 4.0,\n",
        "        attn_drop: float = 0.0,\n",
        "        proj_drop: float = 0.0,\n",
        "        drop_path: float | list[float] = 0.0,\n",
        "        mlp_drop: float = 0.0,):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        if isinstance(drop_path, float):\n",
        "            dpr = [drop_path] * depth\n",
        "        else:\n",
        "            assert len(drop_path) == depth, \"drop_path list must have length=depth\"\n",
        "            dpr = drop_path\n",
        "\n",
        "        self.blocks = nn.ModuleList([\n",
        "            OutlookerBlock(\n",
        "                dim=dim,\n",
        "                num_heads=num_heads,\n",
        "                kernel_size=kernel_size,\n",
        "                stride=stride,\n",
        "                mlp_ratio=mlp_ratio,\n",
        "                attn_drop=attn_drop,\n",
        "                proj_drop=proj_drop,\n",
        "                drop_path=dpr[i],\n",
        "                mlp_drop=mlp_drop,) for i in range(depth)])\n",
        "\n",
        "    def forward(self, x_map: torch.Tensor) -> torch.Tensor:\n",
        "        for blk in self.blocks:\n",
        "            x_map = blk(x_map)\n",
        "        return x_map"
      ],
      "metadata": {
        "id": "whzUQ5eEx_Mo"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_volo_stage():\n",
        "    torch.manual_seed(0)\n",
        "\n",
        "    B, H, W, C = 2, 16, 16, 192\n",
        "    x = torch.randn(B, H, W, C, requires_grad=True)\n",
        "\n",
        "    stage = VOLOStage(\n",
        "        dim=C,\n",
        "        depth=3,\n",
        "        num_heads=6,\n",
        "        kernel_size=3,\n",
        "        stride=1,\n",
        "        drop_path=[0.0, 0.05, 0.1])\n",
        "\n",
        "    y = stage(x)\n",
        "    assert y.shape == x.shape\n",
        "    y.mean().backward()\n",
        "    assert x.grad is not None and torch.isfinite(x.grad).all()\n",
        "\n",
        "    print(\"[OK] VOLOStage:\", tuple(y.shape), \"| grad mean\", float(x.grad.abs().mean()))\n",
        "\n",
        "test_volo_stage()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7iSxXS3XyIwb",
        "outputId": "61d8fd2c-3449-4a46-e77f-1f480b4d9f8c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OK] VOLOStage: (2, 16, 16, 192) | grad mean 1.0873188330151606e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Attention"
      ],
      "metadata": {
        "id": "r7w_cBO4yzYY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def scaled_dot_product_attention(q, k, v, mask=None, attn_dropout_p: float = 0.0, training: bool = True):\n",
        "    \"\"\"\n",
        "    q: (B, H, Lq, d)\n",
        "    k: (B, H, Lk, d)\n",
        "    v: (B, H, Lk, d)\n",
        "    mask: broadcastable a (B, H, Lq, Lk)\n",
        "          - bool: True = BLOQUEAR (poner -inf)\n",
        "          - float: 1.0 = permitir, 0.0 = bloquear\n",
        "    \"\"\"\n",
        "    scores = torch.matmul(q, k.transpose(-2, -1))\n",
        "    dk = q.size(-1)\n",
        "    scores = scores / (dk ** 0.5)\n",
        "\n",
        "    if mask is not None:\n",
        "        if mask.dtype == torch.bool:\n",
        "            scores = scores.masked_fill(mask, float(\"-inf\"))\n",
        "        else:\n",
        "            scores = scores.masked_fill(mask <= 0, float(\"-inf\"))\n",
        "\n",
        "    attn = F.softmax(scores, dim=-1)\n",
        "    if attn_dropout_p > 0.0:\n",
        "        attn = F.dropout(attn, p=attn_dropout_p, training=training)\n",
        "\n",
        "    output = torch.matmul(attn, v)\n",
        "    return output, attn"
      ],
      "metadata": {
        "id": "LY-tnHuuyvbW"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiHeadAttention(nn.Module):\n",
        "    def __init__(self, d_model: int, num_heads: int, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        assert d_model % num_heads == 0, \"d_model debe ser múltiplo de num_heads\"\n",
        "\n",
        "        self.num_heads = num_heads\n",
        "        self.d_head = d_model // num_heads\n",
        "        self.d_model = d_model\n",
        "\n",
        "        self.w_q = nn.Linear(d_model, d_model)\n",
        "        self.w_k = nn.Linear(d_model, d_model)\n",
        "        self.w_v = nn.Linear(d_model, d_model)\n",
        "        self.w_o = nn.Linear(d_model, d_model)\n",
        "\n",
        "        # \"dropout\" lo usaremos como dropout de atención (sobre attn)\n",
        "        self.attn_dropout_p = dropout\n",
        "        # y también dejamos dropout de salida si quieres (común en ViT)\n",
        "        self.out_dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def _split_heads(self, x):\n",
        "        B, L, _ = x.shape\n",
        "        return x.view(B, L, self.num_heads, self.d_head).transpose(1, 2)\n",
        "\n",
        "    def _combine_heads(self, x):\n",
        "        B, H, L, D = x.shape\n",
        "        return x.transpose(1, 2).contiguous().view(B, L, H * D)\n",
        "\n",
        "    def forward(self, x_q, x_kv, mask=None):\n",
        "        q = self._split_heads(self.w_q(x_q))\n",
        "        k = self._split_heads(self.w_k(x_kv))\n",
        "        v = self._split_heads(self.w_v(x_kv))\n",
        "\n",
        "        if mask is not None:\n",
        "            if mask.dim() == 2:\n",
        "                mask = mask[:, None, None, :]\n",
        "            elif mask.dim() == 3:\n",
        "                mask = mask[:, None, :, :]\n",
        "            elif mask.dim() == 4:\n",
        "                pass\n",
        "            else:\n",
        "                raise ValueError(f\"Máscara con dims no soportadas: {mask.shape}\")\n",
        "\n",
        "            if mask.dtype != torch.bool:\n",
        "                mask = (mask <= 0)\n",
        "\n",
        "        attn_out, _ = scaled_dot_product_attention(\n",
        "            q, k, v,\n",
        "            mask=mask,\n",
        "            attn_dropout_p=self.attn_dropout_p,\n",
        "            training=self.training)\n",
        "\n",
        "        attn_out = self._combine_heads(attn_out)\n",
        "\n",
        "        attn_out = self.w_o(attn_out)\n",
        "        attn_out = self.out_dropout(attn_out)\n",
        "        return attn_out"
      ],
      "metadata": {
        "id": "Qip2P-Jby1h7"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, dim: int, hidden_dim: int, dropout: float = 0.1):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(dim, hidden_dim)\n",
        "        self.act = nn.GELU()\n",
        "        self.fc2 = nn.Linear(hidden_dim, dim)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.dropout(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class TransformerBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Bloque encoder para ViT (pre-norm):\n",
        "    x -> LN -> MHA -> DropPath -> +residual\n",
        "       -> LN -> MLP -> DropPath -> +residual\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim: int,\n",
        "        num_heads: int,\n",
        "        mlp_ratio: float = 4.0,\n",
        "        attn_dropout: float = 0.0,\n",
        "        dropout: float = 0.1,\n",
        "        drop_path: float = 0.0):\n",
        "\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(dim)\n",
        "        self.attn = MultiHeadAttention(d_model=dim, num_heads=num_heads, dropout=attn_dropout)\n",
        "        self.drop_path1 = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n",
        "\n",
        "        self.norm2 = nn.LayerNorm(dim)\n",
        "        hidden_dim = int(dim * mlp_ratio)\n",
        "        self.mlp = FeedForward(dim, hidden_dim, dropout=dropout)\n",
        "        self.drop_path2 = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.drop_path1(self.attn(self.norm1(x), self.norm1(x), mask=None))\n",
        "        x = x + self.drop_path2(self.mlp(self.norm2(x)))\n",
        "        return x\n"
      ],
      "metadata": {
        "id": "EQqNseVTy7Lp"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TransformerStack(nn.Module):\n",
        "    \"\"\"Stack simple de TransformerBlock sobre tokens [B, N, C].\"\"\"\n",
        "    def __init__(self, dim: int, depth: int, num_heads: int, mlp_ratio=4.0,\n",
        "                 attn_dropout=0.0, dropout=0.1, drop_path: float | list[float] = 0.0):\n",
        "        super().__init__()\n",
        "        if isinstance(drop_path, float):\n",
        "            dpr = [drop_path] * depth\n",
        "        else:\n",
        "            assert len(drop_path) == depth\n",
        "            dpr = drop_path\n",
        "\n",
        "        self.blocks = nn.ModuleList([\n",
        "            TransformerBlock(\n",
        "                dim=dim,\n",
        "                num_heads=num_heads,\n",
        "                mlp_ratio=mlp_ratio,\n",
        "                attn_dropout=attn_dropout,\n",
        "                dropout=dropout,\n",
        "                drop_path=dpr[i] if \"drop_path\" in TransformerBlock.__init__.__code__.co_varnames else 0.0) for i in range(depth)])\n",
        "\n",
        "    def forward(self, x_tok: torch.Tensor) -> torch.Tensor:\n",
        "        for blk in self.blocks:\n",
        "            x_tok = blk(x_tok)\n",
        "        return x_tok"
      ],
      "metadata": {
        "id": "qDcU6lW4zkjU"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_transformer_block():\n",
        "    torch.manual_seed(0)\n",
        "    B, N, C = 2, 256, 192\n",
        "    x = torch.randn(B, N, C, requires_grad=True)\n",
        "\n",
        "    blk = TransformerBlock(dim=C, num_heads=6, mlp_ratio=4.0, attn_dropout=0.0, dropout=0.1, drop_path=0.0)\n",
        "    y = blk(x)\n",
        "    assert y.shape == x.shape\n",
        "    y.mean().backward()\n",
        "    assert x.grad is not None and torch.isfinite(x.grad).all()\n",
        "    print(\"[OK] TransformerBlock:\", tuple(y.shape), \"grad\", float(x.grad.abs().mean()))\n",
        "\n",
        "test_transformer_block()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IODXdVf0y81n",
        "outputId": "19eeb4eb-042f-4d95-e796-52220afa2d51"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OK] TransformerBlock: (2, 256, 192) grad 1.0180706340179313e-05\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hiratical"
      ],
      "metadata": {
        "id": "gmtkfE3VzVmw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class MapDownsample(nn.Module):\n",
        "    \"\"\"\n",
        "    Downsample para mapas channel-last: [B, H, W, C_in] -> [B, H/2, W/2, C_out]\n",
        "    usando conv2d stride=2 en formato channel-first internamente.\n",
        "    \"\"\"\n",
        "    def __init__(self, dim_in: int, dim_out: int, kernel_size: int = 3, norm_layer=nn.LayerNorm):\n",
        "        super().__init__()\n",
        "        pad = kernel_size // 2\n",
        "        self.conv = nn.Conv2d(dim_in, dim_out, kernel_size=kernel_size, stride=2, padding=pad, bias=True)\n",
        "        self.norm = norm_layer(dim_out) if norm_layer is not None else None\n",
        "\n",
        "    def forward(self, x_map: torch.Tensor):\n",
        "        # x_map: [B, H, W, C_in]\n",
        "        B, H, W, C = x_map.shape\n",
        "        x = x_map.permute(0, 3, 1, 2).contiguous()     # [B, C, H, W]\n",
        "        x = self.conv(x)                               # [B, C_out, H2, W2]\n",
        "        x_map = x.permute(0, 2, 3, 1).contiguous()     # [B, H2, W2, C_out]\n",
        "        if self.norm is not None:\n",
        "            x_map = self.norm(x_map)\n",
        "        return x_map"
      ],
      "metadata": {
        "id": "LIn3ZrvLzXgV"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PoolingLayer(nn.Module):\n",
        "    \"\"\"\n",
        "    Pooling jerárquico para ViT:\n",
        "\n",
        "    - Toma tokens [B, N, D_in] + grid_size (H, W)\n",
        "    - Los reinterpreta como feature map [B, D_in, H, W]\n",
        "    - Aplica:\n",
        "        depthwise conv (3x3, stride=2, padding=1)\n",
        "        pointwise conv (1x1) para cambiar D_in -> D_out\n",
        "    - Devuelve:\n",
        "        tokens [B, N_out, D_out] y nuevo grid_size (H_out, W_out)\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self,\n",
        "        dim_in: int,\n",
        "        dim_out: int,\n",
        "        kernel_size: int = 3,\n",
        "        stride: int = 2,\n",
        "        norm_layer: type[nn.Module] | None = nn.LayerNorm):\n",
        "\n",
        "        super().__init__()\n",
        "        padding = kernel_size // 2\n",
        "\n",
        "        # Depthwise conv: cada canal se filtra por separado\n",
        "        self.depthwise_conv = nn.Conv2d(\n",
        "            in_channels=dim_in,\n",
        "            out_channels=dim_in,\n",
        "            kernel_size=kernel_size,\n",
        "            stride=stride,\n",
        "            padding=padding,\n",
        "            groups=dim_in)\n",
        "\n",
        "        # Pointwise conv: mezcla canales y cambia dim\n",
        "        self.pointwise_conv = nn.Conv2d(\n",
        "            in_channels=dim_in,\n",
        "            out_channels=dim_out,\n",
        "            kernel_size=1,\n",
        "            stride=1,\n",
        "            padding=0)\n",
        "\n",
        "        self.norm = norm_layer(dim_out) if norm_layer is not None else None\n",
        "\n",
        "        self.dim_in = dim_in\n",
        "        self.dim_out = dim_out\n",
        "        self.stride = stride\n",
        "\n",
        "    def forward(self, x: torch.Tensor, grid_size: tuple[int, int]):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            x: tokens [B, N, D_in]\n",
        "            grid_size: (H, W) tal que H*W = N\n",
        "\n",
        "        Returns:\n",
        "            x_out: tokens [B, N_out, D_out]\n",
        "            new_grid: (H_out, W_out)\n",
        "        \"\"\"\n",
        "        B, N, D_in = x.shape\n",
        "        H, W = grid_size\n",
        "\n",
        "        assert D_in == self.dim_in, f\"dim_in {D_in} != {self.dim_in}\"\n",
        "        assert H * W == N, f\"H*W={H*W} no coincide con N={N}\"\n",
        "\n",
        "        # [B, N, D_in] -> [B, D_in, H, W]\n",
        "        x = x.view(B, H, W, D_in).permute(0, 3, 1, 2)\n",
        "\n",
        "        # Depthwise + pointwise\n",
        "        x = self.depthwise_conv(x)\n",
        "        x = self.pointwise_conv(x)\n",
        "\n",
        "        B, D_out, H_out, W_out = x.shape\n",
        "        N_out = H_out * W_out\n",
        "\n",
        "        # Volver a tokens: [B, D_out, H_out, W_out] -> [B, N_out, D_out]\n",
        "        x = x.flatten(2).transpose(1, 2)\n",
        "\n",
        "        if self.norm is not None:\n",
        "            x = self.norm(x)\n",
        "\n",
        "        new_grid = (H_out, W_out)\n",
        "        return x, new_grid"
      ],
      "metadata": {
        "id": "-m016eqBzwAL"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VOLO BackBone"
      ],
      "metadata": {
        "id": "qqlP5kIAzm8a"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def map_to_tokens(x_map: torch.Tensor) -> torch.Tensor:\n",
        "    B, H, W, C = x_map.shape\n",
        "    return x_map.view(B, H * W, C)\n",
        "\n",
        "def tokens_to_map(x_tok: torch.Tensor, H: int, W: int) -> torch.Tensor:\n",
        "    B, N, C = x_tok.shape\n",
        "    assert N == H * W\n",
        "    return x_tok.view(B, H, W, C)\n",
        "\n",
        "class VOLOPyramid(nn.Module):\n",
        "    \"\"\"\n",
        "    Backbone jerárquico para VOLO (sin classifier head aún).\n",
        "    - Local: VOLOStage (Outlooker)\n",
        "    - Global: TransformerStack (opcional)\n",
        "    - Downsample: map-space (recomendado) o token-space (PoolingLayer tuyo)\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        dims: tuple[int, ...],                 # ej (192, 256, 384)\n",
        "        outlooker_depths: tuple[int, ...],     # ej (4, 2, 0)  (0 si no hay outlooker en ese nivel)\n",
        "        outlooker_heads: tuple[int, ...],      # ej (6, 8, 12)\n",
        "        transformer_depths: tuple[int, ...],   # ej (0, 4, 6)\n",
        "        transformer_heads: tuple[int, ...],    # ej (6, 8, 12)\n",
        "        kernel_size: int = 3,\n",
        "        mlp_ratio: float = 4.0,\n",
        "        downsample_kind: str = \"map\",          # \"map\" o \"token\"\n",
        "        drop_path_rate: float = 0.0):\n",
        "\n",
        "        super().__init__()\n",
        "        L = len(dims)\n",
        "\n",
        "        assert len(outlooker_depths) == L\n",
        "        assert len(outlooker_heads) == L\n",
        "        assert len(transformer_depths) == L\n",
        "        assert len(transformer_heads) == L\n",
        "\n",
        "        # schedule lineal de droppath a través de todos los bloques (local+global)\n",
        "        total_blocks = sum(outlooker_depths) + sum(transformer_depths)\n",
        "        dpr = torch.linspace(0, drop_path_rate, total_blocks).tolist() if total_blocks > 0 else []\n",
        "        dp_i = 0\n",
        "\n",
        "        self.levels = nn.ModuleList()\n",
        "        self.downsamples = nn.ModuleList()\n",
        "        self.downsample_kind = downsample_kind\n",
        "\n",
        "        for i in range(L):\n",
        "            dim = dims[i]\n",
        "\n",
        "            # Local stage (Outlooker)\n",
        "            local = None\n",
        "            if outlooker_depths[i] > 0:\n",
        "                local_dpr = dpr[dp_i: dp_i + outlooker_depths[i]]\n",
        "                dp_i += outlooker_depths[i]\n",
        "                local = VOLOStage(\n",
        "                    dim=dim,\n",
        "                    depth=outlooker_depths[i],\n",
        "                    num_heads=outlooker_heads[i],\n",
        "                    kernel_size=kernel_size,\n",
        "                    stride=1,\n",
        "                    mlp_ratio=mlp_ratio,\n",
        "                    drop_path=local_dpr)\n",
        "\n",
        "            # Global stage (Transformer)\n",
        "            global_ = None\n",
        "            if transformer_depths[i] > 0:\n",
        "                glob_dpr = dpr[dp_i: dp_i + transformer_depths[i]]\n",
        "                dp_i += transformer_depths[i]\n",
        "\n",
        "                global_ = TransformerStack(\n",
        "                    dim=dim,\n",
        "                    depth=transformer_depths[i],\n",
        "                    num_heads=transformer_heads[i],\n",
        "                    mlp_ratio=mlp_ratio,\n",
        "                    attn_dropout=0.0,\n",
        "                    dropout=0.1,\n",
        "                    drop_path=glob_dpr,)\n",
        "\n",
        "            self.levels.append(nn.ModuleDict({\"local\": local, \"global\": global_}))\n",
        "\n",
        "            # Downsample para pasar dim_i -> dim_{i+1} (si no es el último nivel)\n",
        "            if i < L - 1:\n",
        "                if downsample_kind == \"map\":\n",
        "                    self.downsamples.append(MapDownsample(dim_in=dim, dim_out=dims[i + 1], kernel_size=3))\n",
        "                elif downsample_kind == \"token\":\n",
        "                    # reusar PoolingLayer\n",
        "                    self.downsamples.append(PoolingLayer(dim_in=dim, dim_out=dims[i + 1], kernel_size=3, stride=2))\n",
        "                else:\n",
        "                    raise ValueError(f\"downsample_kind must be 'map' or 'token'. Got {downsample_kind}\")\n",
        "\n",
        "        assert dp_i == total_blocks\n",
        "\n",
        "    def forward(self, x_map: torch.Tensor):\n",
        "        \"\"\"\n",
        "        x_map: [B, H, W, C0]\n",
        "        returns:\n",
        "          x_final_tokens: [B, N_last, C_last]\n",
        "          last_grid: (H_last, W_last)\n",
        "        \"\"\"\n",
        "        B, H, W, C = x_map.shape\n",
        "\n",
        "        for i, lvl in enumerate(self.levels):\n",
        "            # local stage en map\n",
        "            if lvl[\"local\"] is not None:\n",
        "                x_map = lvl[\"local\"](x_map)\n",
        "\n",
        "            # global stage en tokens (si existe)\n",
        "            if lvl[\"global\"] is not None:\n",
        "                x_tok = map_to_tokens(x_map)\n",
        "                x_tok = lvl[\"global\"](x_tok)\n",
        "                x_map = tokens_to_map(x_tok, H, W)\n",
        "\n",
        "            # downsample (si aplica)\n",
        "            if i < len(self.downsamples):\n",
        "                ds = self.downsamples[i]\n",
        "                if self.downsample_kind == \"map\":\n",
        "                    x_map = ds(x_map)\n",
        "                    H, W = x_map.shape[1], x_map.shape[2]\n",
        "                else:\n",
        "                    # token downsample: necesita grid\n",
        "                    x_tok = map_to_tokens(x_map)\n",
        "                    x_tok, (H, W) = ds(x_tok, (H, W))\n",
        "                    x_map = tokens_to_map(x_tok, H, W)\n",
        "\n",
        "        x_final = map_to_tokens(x_map)\n",
        "        return x_final, (H, W)"
      ],
      "metadata": {
        "id": "t_EUv_hqzfvv"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_volo_pyramid_map():\n",
        "    torch.manual_seed(0)\n",
        "    B = 2\n",
        "    H = W = 16\n",
        "    x_map = torch.randn(B, H, W, 192)\n",
        "\n",
        "    pyr = VOLOPyramid(\n",
        "        dims=(192, 256, 384),\n",
        "        outlooker_depths=(2, 2, 0),\n",
        "        outlooker_heads=(6, 8, 12),\n",
        "        transformer_depths=(0, 2, 2),\n",
        "        transformer_heads=(6, 8, 12),\n",
        "        downsample_kind=\"map\",\n",
        "        drop_path_rate=0.1,)\n",
        "\n",
        "    x_tok, (Hf, Wf) = pyr(x_map)\n",
        "    print(\"[OK] Pyramid-map:\", x_tok.shape, \"grid\", (Hf, Wf))\n",
        "    assert x_tok.shape[0] == B\n",
        "    assert x_tok.shape[2] == 384\n",
        "    assert Hf * Wf == x_tok.shape[1]\n",
        "\n",
        "test_volo_pyramid_map()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fO9L_Fabz4SY",
        "outputId": "6870ea2e-35e6-480f-c7b0-fc404da1363d"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OK] Pyramid-map: torch.Size([2, 16, 384]) grid (4, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def test_volo_pyramid_token():\n",
        "    torch.manual_seed(0)\n",
        "    B = 2\n",
        "    H = W = 16\n",
        "    x_map = torch.randn(B, H, W, 192)\n",
        "\n",
        "    pyr = VOLOPyramid(\n",
        "        dims=(192, 256, 384),\n",
        "        outlooker_depths=(2, 2, 0),\n",
        "        outlooker_heads=(6, 8, 12),\n",
        "        transformer_depths=(0, 2, 2),\n",
        "        transformer_heads=(6, 8, 12),\n",
        "        downsample_kind=\"token\",\n",
        "        drop_path_rate=0.1)\n",
        "\n",
        "    x_tok, (Hf, Wf) = pyr(x_map)\n",
        "    print(\"[OK] Pyramid-token:\", x_tok.shape, \"grid\", (Hf, Wf))\n",
        "    assert x_tok.shape[2] == 384\n",
        "    assert Hf * Wf == x_tok.shape[1]\n",
        "\n",
        "test_volo_pyramid_token()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nr-zTpnMz73Y",
        "outputId": "3a419435-ee86-468d-c336-06e3a80ca0bf"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OK] Pyramid-token: torch.Size([2, 16, 384]) grid (4, 4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ClassAttention(nn.Module):\n",
        "    \"\"\"\n",
        "    Class Attention: sólo el CLS atiende al conjunto [CLS | tokens].\n",
        "    Inputs:\n",
        "      cls:    [B, 1, C]\n",
        "      tokens: [B, N, C]\n",
        "    Output:\n",
        "      cls_out: [B, 1, C]\n",
        "    \"\"\"\n",
        "    def __init__(self, dim: int, num_heads: int, attn_dropout: float = 0.0, proj_dropout: float = 0.0):\n",
        "        super().__init__()\n",
        "        self.attn = MultiHeadAttention(d_model=dim, num_heads=num_heads, dropout=attn_dropout)\n",
        "        self.proj_drop = nn.Dropout(proj_dropout)\n",
        "\n",
        "    def forward(self, cls: torch.Tensor, tokens: torch.Tensor) -> torch.Tensor:\n",
        "        kv = torch.cat([cls, tokens], dim=1)        # [B, 1+N, C]\n",
        "        cls_out = self.attn(q=cls, kv=kv, mask=None) # [B, 1, C] (solo CLS sale actualizado)\n",
        "        return self.proj_drop(cls_out)\n",
        "\n",
        "\n",
        "class ClassAttentionBlock(nn.Module):\n",
        "    \"\"\"\n",
        "    Pre-norm (CaiT-style):\n",
        "      cls -> LN -> ClassAttn(cls, [cls|tokens]) -> +res\n",
        "          -> LN -> MLP -> +res\n",
        "    Nota: tokens NO se actualizan.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        dim: int,\n",
        "        num_heads: int,\n",
        "        mlp_ratio: float = 4.0,\n",
        "        attn_dropout: float = 0.0,\n",
        "        dropout: float = 0.0):\n",
        "\n",
        "        super().__init__()\n",
        "        self.norm_cls = nn.LayerNorm(dim)\n",
        "        self.norm_tok = nn.LayerNorm(dim)\n",
        "        self.ca = ClassAttention(dim, num_heads, attn_dropout=attn_dropout, proj_dropout=dropout)\n",
        "\n",
        "        self.norm2 = nn.LayerNorm(dim)\n",
        "        self.mlp = FeedForward(dim, int(dim * mlp_ratio), dropout=dropout)\n",
        "\n",
        "    def forward(self, cls: torch.Tensor, tokens: torch.Tensor) -> torch.Tensor:\n",
        "        # Class attention update (solo CLS)\n",
        "        cls_norm = self.norm_cls(cls)\n",
        "        tok_norm = self.norm_tok(tokens)\n",
        "        cls = cls + self.ca(cls_norm, tok_norm)\n",
        "\n",
        "        # MLP update (solo CLS)\n",
        "        cls = cls + self.mlp(self.norm2(cls))\n",
        "        return cls"
      ],
      "metadata": {
        "id": "Nzbf97Ut9lSQ"
      },
      "execution_count": 58,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CLIPool(nn.Module):\n",
        "    \"\"\"\n",
        "    \"CLI\" style pooling: mezcla aprendible entre CLS y mean(tokens).\n",
        "      z = alpha * cls + (1-alpha) * mean\n",
        "    \"\"\"\n",
        "    def __init__(self, init_alpha: float = 0.5):\n",
        "        super().__init__()\n",
        "        # parametriza alpha en logits para mantenerlo en (0,1)\n",
        "        init_alpha = float(init_alpha)\n",
        "        init_alpha = min(max(init_alpha, 1e-4), 1 - 1e-4)\n",
        "        logit = math.log(init_alpha / (1 - init_alpha))\n",
        "        self.alpha_logit = nn.Parameter(torch.tensor([logit], dtype=torch.float32))\n",
        "\n",
        "    def forward(self, cls_vec: torch.Tensor, tok_mean: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        cls_vec:  [B, C]\n",
        "        tok_mean: [B, C]\n",
        "        \"\"\"\n",
        "        alpha = torch.sigmoid(self.alpha_logit)  # scalar in (0,1)\n",
        "        return alpha * cls_vec + (1.0 - alpha) * tok_mean"
      ],
      "metadata": {
        "id": "w-uPdjcF9n5L"
      },
      "execution_count": 66,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# VOLO"
      ],
      "metadata": {
        "id": "jg1a84bu0E2D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import math\n",
        "\n",
        "def trunc_normal_(tensor, mean=0., std=1.):\n",
        "    with torch.no_grad():\n",
        "        return tensor.normal_(mean=mean, std=std)\n",
        "\n",
        "class PosEmbed2D(nn.Module):\n",
        "    \"\"\"\n",
        "    Positional embedding aprendible para grilla (H, W) en tokens.\n",
        "\n",
        "    Guarda [1, H*W, C]. Si en forward llega otro (H,W), interpola.\n",
        "    \"\"\"\n",
        "    def __init__(self, H: int, W: int, dim: int):\n",
        "        super().__init__()\n",
        "        self.H0 = H\n",
        "        self.W0 = W\n",
        "        self.dim = dim\n",
        "        self.pos = nn.Parameter(torch.zeros(1, H * W, dim))\n",
        "        trunc_normal_(self.pos, std=0.02)\n",
        "\n",
        "    def forward(self, x_tok: torch.Tensor, grid: tuple[int, int]):\n",
        "        \"\"\"\n",
        "        x_tok: [B, N, C]\n",
        "        grid: (H, W)\n",
        "        \"\"\"\n",
        "        B, N, C = x_tok.shape\n",
        "        H, W = grid\n",
        "        if (H == self.H0) and (W == self.W0):\n",
        "            return x_tok + self.pos\n",
        "\n",
        "        # Interpola pos emb como mapa [1, C, H, W] -> nuevo tamaño\n",
        "        pos = self.pos.reshape(1, self.H0, self.W0, self.dim).permute(0, 3, 1, 2)  # [1,C,H0,W0]\n",
        "        pos = nn.functional.interpolate(pos, size=(H, W), mode=\"bicubic\", align_corners=False)\n",
        "        pos = pos.permute(0, 2, 3, 1).reshape(1, H * W, self.dim)\n",
        "        return x_tok + pos\n",
        "\n",
        "class VOLOClassifier(nn.Module):\n",
        "    \"\"\"\n",
        "    VOLO para CIFAR-100 (y similares), con dos modos:\n",
        "      - flat: OutlookerStage -> TransformerStack (sin downsample)\n",
        "              pooling: mean | cls | cli (cls via class-attn final)\n",
        "      - hierarchical: pirámide con downsample (map o token)\n",
        "              pooling: SOLO mean (por ahora)\n",
        "\n",
        "    Flujo base:\n",
        "      x [B,3,H,W]\n",
        "        -> PatchEmbeddingConv -> x_tok [B, N, C0]\n",
        "        -> pos emb (opcional)\n",
        "        -> backbone (flat o pyramid)\n",
        "        -> pooling\n",
        "        -> head\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(\n",
        "        self,\n",
        "        num_classes: int = 100,\n",
        "        img_size: int = 32,\n",
        "        in_chans: int = 3,\n",
        "        patch_size: int = 4,\n",
        "\n",
        "        # mode\n",
        "        hierarchical: bool = False,\n",
        "        downsample_kind: str = \"map\",   # si hierarchical=True: \"map\" o \"token\"\n",
        "\n",
        "        # dims / depths (flat)\n",
        "        embed_dim: int = 192,\n",
        "        outlooker_depth: int = 4,\n",
        "        outlooker_heads: int = 6,\n",
        "        transformer_depth: int = 6,\n",
        "        transformer_heads: int = 6,\n",
        "\n",
        "        # hierarchical configs (si hierarchical=True)\n",
        "        dims: tuple[int, ...] = (192, 256, 384),\n",
        "        outlooker_depths: tuple[int, ...] = (2, 2, 0),\n",
        "        outlooker_heads_list: tuple[int, ...] = (6, 8, 12),\n",
        "        transformer_depths: tuple[int, ...] = (0, 2, 2),\n",
        "        transformer_heads_list: tuple[int, ...] = (6, 8, 12),\n",
        "\n",
        "        # block hyperparams\n",
        "        kernel_size: int = 3,\n",
        "        mlp_ratio: float = 4.0,\n",
        "        dropout: float = 0.1,\n",
        "        attn_dropout: float = 0.0,\n",
        "        drop_path_rate: float = 0.0,\n",
        "\n",
        "        # head / pooling\n",
        "        pooling: str = \"mean\",          # flat: \"mean\"|\"cls\"|\"cli\" ; hierarchical: \"mean\"\n",
        "        use_pos_embed: bool = True,\n",
        "\n",
        "        # cls refinamiento (flat)\n",
        "        cls_attn_depth: int = 2,        # # capas ClassAttentionBlock\n",
        "        cli_init_alpha: float = 0.5,    # init alpha para pooling=\"cli\"\n",
        "        use_cls_pos: bool = True):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.hierarchical = hierarchical\n",
        "        self.use_pos_embed = use_pos_embed\n",
        "\n",
        "        if self.hierarchical:\n",
        "            assert pooling == \"mean\", \"Por ahora hierarchical solo soporta pooling='mean'.\"\n",
        "        else:\n",
        "            assert pooling in [\"mean\", \"cls\", \"cli\"], \"pooling en flat debe ser 'mean', 'cls' o 'cli'.\"\n",
        "        self.pooling = pooling\n",
        "\n",
        "        # ---- Patch Embedding ----\n",
        "        C0 = (dims[0] if hierarchical else embed_dim)\n",
        "\n",
        "        self.patch_embed = PatchEmbeddingConv(\n",
        "            patch_size=patch_size,\n",
        "            in_chans=in_chans,\n",
        "            embed_dim=C0,\n",
        "            norm_layer=nn.LayerNorm,\n",
        "            pad_if_needed=True,\n",
        "            return_tokens=True,)\n",
        "\n",
        "        Hp0 = math.ceil(img_size / patch_size)\n",
        "        Wp0 = math.ceil(img_size / patch_size)\n",
        "\n",
        "        self.pos_embed = PosEmbed2D(Hp0, Wp0, C0) if use_pos_embed else None\n",
        "        self.pos_drop = nn.Dropout(dropout)\n",
        "\n",
        "        # ---- Backbone ----\n",
        "        if not hierarchical:\n",
        "            total = outlooker_depth + transformer_depth\n",
        "            dpr = torch.linspace(0, drop_path_rate, total).tolist() if total > 0 else []\n",
        "            dpr_local = dpr[:outlooker_depth]\n",
        "            dpr_glob = dpr[outlooker_depth:]\n",
        "\n",
        "            self.local_stage = VOLOStage(\n",
        "                dim=embed_dim,\n",
        "                depth=outlooker_depth,\n",
        "                num_heads=outlooker_heads,\n",
        "                kernel_size=kernel_size,\n",
        "                stride=1,\n",
        "                mlp_ratio=mlp_ratio,\n",
        "                attn_drop=attn_dropout,\n",
        "                proj_drop=dropout,\n",
        "                drop_path=dpr_local if len(dpr_local) else 0.0,\n",
        "                mlp_drop=dropout)\n",
        "\n",
        "            self.global_blocks = nn.ModuleList([\n",
        "                TransformerBlock(\n",
        "                    dim=embed_dim,\n",
        "                    num_heads=transformer_heads,\n",
        "                    mlp_ratio=mlp_ratio,\n",
        "                    attn_dropout=attn_dropout,\n",
        "                    dropout=dropout,\n",
        "                    drop_path=(dpr_glob[i] if len(dpr_glob) else 0.0),\n",
        "                ) for i in range(transformer_depth)])\n",
        "\n",
        "            # --- CLS  (solo si pooling usa cls/cli) ---\n",
        "            self.use_cls = (pooling in [\"cls\", \"cli\"])\n",
        "            if self.use_cls:\n",
        "                self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "                trunc_normal_(self.cls_token, std=0.02)\n",
        "\n",
        "                self.cls_pos = None\n",
        "                if use_cls_pos:\n",
        "                    self.cls_pos = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "                    trunc_normal_(self.cls_pos, std=0.02)\n",
        "\n",
        "                self.cls_attn_blocks = nn.ModuleList([\n",
        "                    ClassAttentionBlock(\n",
        "                        dim=embed_dim,\n",
        "                        num_heads=transformer_heads,\n",
        "                        mlp_ratio=mlp_ratio,\n",
        "                        attn_dropout=attn_dropout,\n",
        "                        dropout=dropout,) for _ in range(int(cls_attn_depth))])\n",
        "\n",
        "                self.cli_pool = CLIPool(init_alpha=cli_init_alpha) if pooling == \"cli\" else None\n",
        "            else:\n",
        "                self.cls_token = None\n",
        "                self.cls_pos = None\n",
        "                self.cls_attn_blocks = None\n",
        "                self.cli_pool = None\n",
        "\n",
        "\n",
        "            self.norm = nn.LayerNorm(embed_dim)\n",
        "            self.norm_feat = nn.LayerNorm(embed_dim)\n",
        "\n",
        "            self.head = nn.Linear(embed_dim, num_classes)\n",
        "\n",
        "        else:\n",
        "            self.pyramid = VOLOPyramid(\n",
        "                dims=dims,\n",
        "                outlooker_depths=outlooker_depths,\n",
        "                outlooker_heads=outlooker_heads_list,\n",
        "                transformer_depths=transformer_depths,\n",
        "                transformer_heads=transformer_heads_list,\n",
        "                kernel_size=kernel_size,\n",
        "                mlp_ratio=mlp_ratio,\n",
        "                downsample_kind=downsample_kind,\n",
        "                drop_path_rate=drop_path_rate,)\n",
        "\n",
        "\n",
        "            self.norm = nn.LayerNorm(dims[-1])\n",
        "            self.norm_feat = nn.LayerNorm(dims[-1])\n",
        "            self.head = nn.Linear(dims[-1], num_classes)\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        # Patch embedding\n",
        "        x_map, (Hp, Wp), x_tok, _pad = self.patch_embed(x)   # x_tok [B,N,C0]\n",
        "        B, N, C0 = x_tok.shape\n",
        "\n",
        "        # Pos emb sobre tokens del grid\n",
        "        if self.use_pos_embed and (self.pos_embed is not None):\n",
        "            x_tok = self.pos_embed(x_tok, (Hp, Wp))\n",
        "        x_tok = self.pos_drop(x_tok)\n",
        "\n",
        "        if not self.hierarchical:\n",
        "            # ---- Flat backbone ----\n",
        "            # Outlooker trabaja en map (sin CLS)\n",
        "            x_map = x_tok.view(B, Hp, Wp, C0)\n",
        "            x_map = self.local_stage(x_map)\n",
        "            x_tok = x_map.view(B, Hp * Wp, C0)  # [B,N,C]\n",
        "\n",
        "            # Transformer global (tokens sin CLS)\n",
        "            for blk in self.global_blocks:\n",
        "                x_tok = blk(x_tok)\n",
        "\n",
        "            #  Pooling\n",
        "            if self.pooling == \"mean\":\n",
        "                # Normaliza tokens y promedia\n",
        "                x_tok_n = self.norm(x_tok)           # [B,N,C]\n",
        "                feat = x_tok_n.mean(dim=1)           # [B,C]\n",
        "                feat = self.norm_feat(feat)          # [B,C]\n",
        "                return self.head(feat)\n",
        "\n",
        "            # CLS refinado con class-attn final (CaiT-style)\n",
        "            cls = self.cls_token.expand(B, -1, -1)   # [B,1,C]\n",
        "            if self.cls_pos is not None:\n",
        "                cls = cls + self.cls_pos\n",
        "\n",
        "            for cab in self.cls_attn_blocks:\n",
        "                cls = cab(cls, x_tok)               # [B,1,C]\n",
        "\n",
        "            cls_vec = cls.squeeze(1)                # [B,C]\n",
        "            cls_vec = self.norm_feat(cls_vec)\n",
        "\n",
        "            if self.pooling == \"cls\":\n",
        "                feat = cls_vec\n",
        "                return self.head(feat)\n",
        "\n",
        "            # pooling == \"cli\": mezcla CLS con mean(tokens) normalizado\n",
        "            tok_mean = self.norm(x_tok).mean(dim=1)  # [B,C]\n",
        "            feat = self.cli_pool(cls_vec, tok_mean)\n",
        "            feat = self.norm_feat(feat)\n",
        "            return self.head(feat)\n",
        "\n",
        "        else:\n",
        "            # ---- Hierarchical backbone (solo mean) ----\n",
        "            x_map = x_tok.view(B, Hp, Wp, C0)\n",
        "            x_last, (Hf, Wf) = self.pyramid(x_map)      # x_last: [B, Nf, C_last]\n",
        "\n",
        "            x_last = self.norm(x_last)\n",
        "            feat = x_last.mean(dim=1)\n",
        "            feat = self.norm_feat(feat)\n",
        "            return self.head(feat)\n"
      ],
      "metadata": {
        "id": "SECkRyzH0IwB"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def test_volo_classifier_flat():\n",
        "    torch.manual_seed(0)\n",
        "    model = VOLOClassifier(\n",
        "        num_classes=100,\n",
        "        img_size=64,\n",
        "        patch_size=4,\n",
        "        hierarchical=False,\n",
        "        embed_dim=192,\n",
        "        outlooker_depth=2,\n",
        "        transformer_depth=2,\n",
        "        outlooker_heads=6,\n",
        "        transformer_heads=6,\n",
        "        pooling=\"mean\")\n",
        "\n",
        "    x = torch.randn(2, 3, 64, 64)\n",
        "    y = model(x)\n",
        "    print(\"[OK] flat logits:\", y.shape)\n",
        "    assert y.shape == (2, 100)\n",
        "\n",
        "def test_volo_classifier_hier():\n",
        "    torch.manual_seed(0)\n",
        "    model = VOLOClassifier(\n",
        "        num_classes=100,\n",
        "        img_size=64,\n",
        "        patch_size=4,\n",
        "        hierarchical=True,\n",
        "        downsample_kind=\"map\",\n",
        "        dims=(192, 256, 384),\n",
        "        outlooker_depths=(2, 2, 0),\n",
        "        outlooker_heads_list=(6, 8, 12),\n",
        "        transformer_depths=(0, 2, 2),\n",
        "        transformer_heads_list=(6, 8, 12),\n",
        "        pooling=\"mean\",)\n",
        "\n",
        "    x = torch.randn(2, 3, 64, 64)\n",
        "    y = model(x)\n",
        "    print(\"[OK] hier logits:\", y.shape)\n",
        "    assert y.shape == (2, 100)\n",
        "\n",
        "test_volo_classifier_flat()\n",
        "test_volo_classifier_hier()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mE2wYcCg032h",
        "outputId": "05df5d57-bbf5-425c-e4c4-d652098709ec"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[OK] flat logits: torch.Size([2, 100])\n",
            "[OK] hier logits: torch.Size([2, 100])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def _fmt_out(output):\n",
        "    if isinstance(output, (tuple, list)):\n",
        "        shapes = []\n",
        "        for o in output:\n",
        "            if hasattr(o, \"shape\"):\n",
        "                shapes.append(tuple(o.shape))\n",
        "            else:\n",
        "                shapes.append(type(o).__name__)\n",
        "        return shapes\n",
        "    if hasattr(output, \"shape\"):\n",
        "        return tuple(output.shape)\n",
        "    return type(output).__name__\n",
        "\n",
        "\n",
        "def attach_shape_hooks_volo(model: nn.Module, verbose: bool = True):\n",
        "    hooks = []\n",
        "\n",
        "    def add_hook(mod: nn.Module, name: str):\n",
        "        if mod is None:\n",
        "            return\n",
        "        def hook(_m, _inp, out):\n",
        "            print(f\"{name:35s} -> {_fmt_out(out)}\")\n",
        "        hooks.append(mod.register_forward_hook(hook))\n",
        "\n",
        "    # Top-level components\n",
        "    add_hook(getattr(model, \"patch_embed\", None), \"patch_embed\")\n",
        "    add_hook(getattr(model, \"local_stage\", None), \"local_stage (outlooker)\")\n",
        "    add_hook(getattr(model, \"pyramid\", None), \"pyramid (top)\")\n",
        "    add_hook(getattr(model, \"norm\", None), \"norm\")\n",
        "    add_hook(getattr(model, \"head\", None), \"head\")\n",
        "\n",
        "    # Global blocks (flat)\n",
        "    if hasattr(model, \"global_blocks\"):\n",
        "        for i, blk in enumerate(model.global_blocks):\n",
        "            add_hook(blk, f\"global_block[{i}]\")\n",
        "\n",
        "    # Pyramid internals (hierarchical)\n",
        "    pyr = getattr(model, \"pyramid\", None)\n",
        "    if pyr is not None:\n",
        "        if hasattr(pyr, \"levels\"):\n",
        "            for i, lvl in enumerate(pyr.levels):\n",
        "                # lvl es nn.ModuleDict: NO tiene .get\n",
        "                loc = lvl[\"local\"] if \"local\" in lvl else None\n",
        "                glob = lvl[\"global\"] if \"global\" in lvl else None\n",
        "                add_hook(loc,  f\"pyr.level[{i}].local\")\n",
        "                add_hook(glob, f\"pyr.level[{i}].global\")\n",
        "\n",
        "        if hasattr(pyr, \"downsamples\"):\n",
        "            for i, ds in enumerate(pyr.downsamples):\n",
        "                add_hook(ds, f\"pyr.down[{i}]\")\n",
        "\n",
        "    return hooks\n",
        "\n",
        "def remove_hooks(hooks):\n",
        "    for h in hooks:\n",
        "        h.remove()\n",
        "\n",
        "@torch.no_grad()\n",
        "def debug_forward_shapes(model: nn.Module, img_size: int, device: str = \"cpu\", batch_size: int = 2):\n",
        "    model = model.to(device).eval()\n",
        "    hooks = attach_shape_hooks_volo(model)\n",
        "\n",
        "    x = torch.randn(batch_size, 3, img_size, img_size, device=device)\n",
        "    print(f\"\\n=== Forward debug | img_size={img_size} | model={model.__class__.__name__} ===\")\n",
        "    y = model(x)\n",
        "    print(f\"{'OUTPUT logits':35s} -> {tuple(y.shape)}\")\n",
        "\n",
        "    remove_hooks(hooks)\n"
      ],
      "metadata": {
        "id": "2Ke7ncx_69Rq"
      },
      "execution_count": 45,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_flat64 = VOLOClassifier(\n",
        "    num_classes=100,\n",
        "    img_size=64,\n",
        "    patch_size=4,\n",
        "    hierarchical=False,\n",
        "    embed_dim=192,\n",
        "    outlooker_depth=2,\n",
        "    outlooker_heads=6,\n",
        "    transformer_depth=2,\n",
        "    transformer_heads=6,\n",
        "    pooling=\"mean\",\n",
        "    use_pos_embed=True,)\n",
        "\n",
        "debug_forward_shapes(model_flat64, img_size=64, device=\"cpu\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aaORyp_F7EsC",
        "outputId": "31dc82fc-2fbf-4ed8-9d0e-a66eb400740a"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attached hooks to:\n",
            "\n",
            "=== Forward debug | img_size=64 | model=VOLOClassifier ===\n",
            "patch_embed                         -> [(2, 16, 16, 192), 'tuple', (2, 256, 192), 'tuple']\n",
            "local_stage (outlooker)             -> (2, 16, 16, 192)\n",
            "global_block[0]                     -> (2, 256, 192)\n",
            "global_block[1]                     -> (2, 256, 192)\n",
            "norm                                -> (2, 256, 192)\n",
            "head                                -> (2, 100)\n",
            "OUTPUT logits                       -> (2, 100)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_hier64 = VOLOClassifier(\n",
        "    num_classes=100,\n",
        "    img_size=64,\n",
        "    patch_size=4,\n",
        "    hierarchical=True,\n",
        "    downsample_kind=\"map\",\n",
        "    dims=(192, 256, 384),\n",
        "    outlooker_depths=(2, 2, 0),\n",
        "    outlooker_heads_list=(6, 8, 12),\n",
        "    transformer_depths=(0, 2, 2),\n",
        "    transformer_heads_list=(6, 8, 12),\n",
        "    pooling=\"mean\",\n",
        "    use_pos_embed=True,)\n",
        "\n",
        "debug_forward_shapes(model_hier64, img_size=64, device=\"cpu\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FEDhIpY47KHP",
        "outputId": "2fca77d2-53d5-45e8-d878-bb39841672d9"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Forward debug | img_size=64 | model=VOLOClassifier ===\n",
            "patch_embed                         -> [(2, 16, 16, 192), 'tuple', (2, 256, 192), 'tuple']\n",
            "pyr.level[0].local                  -> (2, 16, 16, 192)\n",
            "pyr.down[0]                         -> (2, 8, 8, 256)\n",
            "pyr.level[1].local                  -> (2, 8, 8, 256)\n",
            "pyr.level[1].global                 -> (2, 64, 256)\n",
            "pyr.down[1]                         -> (2, 4, 4, 384)\n",
            "pyr.level[2].global                 -> (2, 16, 384)\n",
            "pyramid (top)                       -> [(2, 16, 384), 'tuple']\n",
            "norm                                -> (2, 16, 384)\n",
            "head                                -> (2, 100)\n",
            "OUTPUT logits                       -> (2, 100)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_hier64_tok = VOLOClassifier(\n",
        "    num_classes=100,\n",
        "    img_size=64,\n",
        "    patch_size=4,\n",
        "    hierarchical=True,\n",
        "    downsample_kind=\"token\",\n",
        "    dims=(192, 256, 384),\n",
        "    outlooker_depths=(2, 2, 0),\n",
        "    outlooker_heads_list=(6, 8, 12),\n",
        "    transformer_depths=(0, 2, 2),\n",
        "    transformer_heads_list=(6, 8, 12),\n",
        "    pooling=\"mean\",\n",
        "    use_pos_embed=True,)\n",
        "\n",
        "debug_forward_shapes(model_hier64_tok, img_size=64, device=\"cpu\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A77K81Kn7NO3",
        "outputId": "57075d87-388a-4569-b494-c78b7a5ff121"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Forward debug | img_size=64 | model=VOLOClassifier ===\n",
            "patch_embed                         -> [(2, 16, 16, 192), 'tuple', (2, 256, 192), 'tuple']\n",
            "pyr.level[0].local                  -> (2, 16, 16, 192)\n",
            "pyr.down[0]                         -> [(2, 64, 256), 'tuple']\n",
            "pyr.level[1].local                  -> (2, 8, 8, 256)\n",
            "pyr.level[1].global                 -> (2, 64, 256)\n",
            "pyr.down[1]                         -> [(2, 16, 384), 'tuple']\n",
            "pyr.level[2].global                 -> (2, 16, 384)\n",
            "pyramid (top)                       -> [(2, 16, 384), 'tuple']\n",
            "norm                                -> (2, 16, 384)\n",
            "head                                -> (2, 100)\n",
            "OUTPUT logits                       -> (2, 100)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "---"
      ],
      "metadata": {
        "id": "d1DRvX0A4caq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os, math, random, inspect\n",
        "from contextlib import contextmanager, nullcontext\n",
        "from typing import Dict\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def seed_everything(seed: int = 0, deterministic: bool = False):\n",
        "    random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    if deterministic:\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "    else:\n",
        "        torch.backends.cudnn.benchmark = True\n",
        "\n",
        "_DTYPE_MAP = {\n",
        "    \"bf16\": torch.bfloat16, \"bfloat16\": torch.bfloat16,\n",
        "    \"fp16\": torch.float16,  \"float16\": torch.float16,\n",
        "    \"fp32\": torch.float32,  \"float32\": torch.float32,}\n",
        "\n",
        "def _cuda_dtype_supported(dtype: torch.dtype) -> bool:\n",
        "    if not torch.cuda.is_available():\n",
        "        return False\n",
        "    return dtype in (torch.float16, torch.bfloat16)\n",
        "\n",
        "def make_grad_scaler(device: str = \"cuda\", enabled: bool = True):\n",
        "    if not enabled:\n",
        "        return None\n",
        "\n",
        "    if hasattr(torch, \"amp\") and hasattr(torch.amp, \"GradScaler\"):\n",
        "        try:\n",
        "            sig = inspect.signature(torch.amp.GradScaler)\n",
        "            if len(sig.parameters) >= 1:\n",
        "                return torch.amp.GradScaler(device if device in (\"cuda\", \"cpu\") else \"cuda\")\n",
        "            return torch.amp.GradScaler()\n",
        "        except Exception:\n",
        "            pass\n",
        "\n",
        "    if hasattr(torch.cuda, \"amp\") and hasattr(torch.cuda.amp, \"GradScaler\"):\n",
        "        return torch.cuda.amp.GradScaler()\n",
        "    return None\n",
        "\n",
        "\n",
        "@contextmanager\n",
        "def autocast_ctx(\n",
        "    device: str = \"cuda\",\n",
        "    enabled: bool = True,\n",
        "    dtype: str = \"fp16\",\n",
        "    cache_enabled: bool = True,):\n",
        "    \"\"\"\n",
        "    Context manager de autocast:\n",
        "      - cuda: fp16 por defecto (ideal en T4)\n",
        "      - cpu: bfloat16 si está disponible\n",
        "    \"\"\"\n",
        "    if not enabled:\n",
        "        with nullcontext():\n",
        "            yield\n",
        "        return\n",
        "\n",
        "    if device == \"cuda\":\n",
        "        want = _DTYPE_MAP.get(dtype.lower(), torch.float16)\n",
        "        use = want if _cuda_dtype_supported(want) else torch.float16\n",
        "        with torch.amp.autocast(device_type=\"cuda\", dtype=use, cache_enabled=cache_enabled):\n",
        "            yield\n",
        "        return\n",
        "\n",
        "    if device == \"cpu\":\n",
        "        try:\n",
        "            with torch.amp.autocast(device_type=\"cpu\", dtype=torch.bfloat16, cache_enabled=cache_enabled):\n",
        "                yield\n",
        "        except Exception:\n",
        "            with nullcontext():\n",
        "                yield\n",
        "        return\n",
        "\n",
        "    with nullcontext():\n",
        "        yield"
      ],
      "metadata": {
        "id": "qQU0oPtG4dmV"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def build_param_groups_no_wd(model: nn.Module, weight_decay: float):\n",
        "    decay, no_decay = [], []\n",
        "    for name, p in model.named_parameters():\n",
        "        if not p.requires_grad:\n",
        "            continue\n",
        "\n",
        "        name_l = name.lower()\n",
        "        # no decay for biases + norms + positional/class tokens\n",
        "        if (\n",
        "            name.endswith(\".bias\")\n",
        "            or (\"norm\" in name_l)\n",
        "            or (\"bn\" in name_l)\n",
        "            or (\"ln\" in name_l)\n",
        "            or (\"pos\" in name_l)         # pos_embed / pos\n",
        "            or (\"cls_token\" in name_l)\n",
        "        ):\n",
        "            no_decay.append(p)\n",
        "        else:\n",
        "            decay.append(p)\n",
        "\n",
        "    return [\n",
        "        {\"params\": decay, \"weight_decay\": weight_decay},\n",
        "        {\"params\": no_decay, \"weight_decay\": 0.0}]\n",
        "\n",
        "\n",
        "class WarmupCosineLR:\n",
        "    \"\"\"Warmup linear for warmup_steps, then cosine to min_lr. Step-based.\"\"\"\n",
        "    def __init__(self, optimizer, total_steps: int, warmup_steps: int, min_lr: float = 0.0):\n",
        "        self.optimizer = optimizer\n",
        "        self.total_steps = int(total_steps)\n",
        "        self.warmup_steps = int(warmup_steps)\n",
        "        self.min_lr = float(min_lr)\n",
        "        self.base_lrs = [g[\"lr\"] for g in optimizer.param_groups]\n",
        "        self.step_num = 0\n",
        "\n",
        "    def step(self):\n",
        "        self.step_num += 1\n",
        "        t = self.step_num\n",
        "\n",
        "        for i, group in enumerate(self.optimizer.param_groups):\n",
        "            base = self.base_lrs[i]\n",
        "            if t <= self.warmup_steps and self.warmup_steps > 0:\n",
        "                lr = base * (t / self.warmup_steps)\n",
        "            else:\n",
        "                tt = min(t, self.total_steps)\n",
        "                denom = max(1, self.total_steps - self.warmup_steps)\n",
        "                progress = (tt - self.warmup_steps) / denom\n",
        "                cosine = 0.5 * (1.0 + math.cos(math.pi * progress))\n",
        "                lr = self.min_lr + (base - self.min_lr) * cosine\n",
        "            group[\"lr\"] = lr\n",
        "\n",
        "    def state_dict(self):\n",
        "        return {\"step_num\": self.step_num}\n",
        "\n",
        "    def load_state_dict(self, d):\n",
        "        self.step_num = int(d.get(\"step_num\", 0))"
      ],
      "metadata": {
        "id": "IOvy-NJH5gZG"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def save_checkpoint(\n",
        "    path: str,\n",
        "    model,\n",
        "    optimizer,\n",
        "    scheduler,\n",
        "    scaler,\n",
        "    epoch: int,\n",
        "    best_top1: float,\n",
        "    extra: dict | None = None,):\n",
        "\n",
        "    ckpt = {\n",
        "        \"model\": model.state_dict(),\n",
        "        \"optimizer\": optimizer.state_dict() if optimizer is not None else None,\n",
        "        \"scheduler\": scheduler.state_dict() if scheduler is not None else None,\n",
        "        \"scaler\": scaler.state_dict() if scaler is not None else None,\n",
        "        \"epoch\": epoch,\n",
        "        \"best_top1\": best_top1,\n",
        "        \"extra\": extra or {},}\n",
        "    torch.save(ckpt, path)\n",
        "\n",
        "\n",
        "def load_checkpoint(\n",
        "    path: str,\n",
        "    model,\n",
        "    optimizer=None,\n",
        "    scheduler=None,\n",
        "    scaler=None,\n",
        "    map_location=\"cpu\",\n",
        "    strict: bool = True,):\n",
        "    ckpt = torch.load(path, map_location=map_location)\n",
        "    model.load_state_dict(ckpt[\"model\"], strict=strict)\n",
        "\n",
        "    if optimizer is not None and ckpt.get(\"optimizer\") is not None:\n",
        "        optimizer.load_state_dict(ckpt[\"optimizer\"])\n",
        "    if scheduler is not None and ckpt.get(\"scheduler\") is not None:\n",
        "        scheduler.load_state_dict(ckpt[\"scheduler\"])\n",
        "    if scaler is not None and ckpt.get(\"scaler\") is not None:\n",
        "        scaler.load_state_dict(ckpt[\"scaler\"])\n",
        "    return ckpt"
      ],
      "metadata": {
        "id": "CPrZy1aU5nYv"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------\n",
        "# Mixup / CutMix + Loss\n",
        "# -------------------------\n",
        "def _one_hot(targets: torch.Tensor, num_classes: int) -> torch.Tensor:\n",
        "    return F.one_hot(targets, num_classes=num_classes).float()\n",
        "\n",
        "\n",
        "def soft_target_cross_entropy(logits: torch.Tensor, targets_soft: torch.Tensor) -> torch.Tensor:\n",
        "    logp = F.log_softmax(logits, dim=1)\n",
        "    return -(targets_soft * logp).sum(dim=1).mean()\n",
        "\n",
        "\n",
        "def apply_mixup_cutmix(\n",
        "    images: torch.Tensor,\n",
        "    targets: torch.Tensor,\n",
        "    num_classes: int,\n",
        "    mixup_alpha: float = 0.0,\n",
        "    cutmix_alpha: float = 0.0,\n",
        "    prob: float = 1.0,):\n",
        "    \"\"\"\n",
        "    Returns:\n",
        "      images_aug: [B,3,H,W]\n",
        "      targets_soft: [B,K]\n",
        "    \"\"\"\n",
        "    if prob <= 0.0 or (mixup_alpha <= 0.0 and cutmix_alpha <= 0.0):\n",
        "        return images, _one_hot(targets, num_classes)\n",
        "\n",
        "    if random.random() > prob:\n",
        "        return images, _one_hot(targets, num_classes)\n",
        "\n",
        "    use_cutmix = (cutmix_alpha > 0.0) and (mixup_alpha <= 0.0 or random.random() < 0.5)\n",
        "    B, _, H, W = images.shape\n",
        "    perm = torch.randperm(B, device=images.device)\n",
        "\n",
        "    y1 = _one_hot(targets, num_classes)\n",
        "    y2 = _one_hot(targets[perm], num_classes)\n",
        "\n",
        "    if use_cutmix:\n",
        "        lam = torch.distributions.Beta(cutmix_alpha, cutmix_alpha).sample().item()\n",
        "        cut_w = int(W * math.sqrt(1.0 - lam))\n",
        "        cut_h = int(H * math.sqrt(1.0 - lam))\n",
        "        cx = random.randint(0, W - 1)\n",
        "        cy = random.randint(0, H - 1)\n",
        "\n",
        "        x1 = max(cx - cut_w // 2, 0)\n",
        "        x2 = min(cx + cut_w // 2, W)\n",
        "        y1b = max(cy - cut_h // 2, 0)\n",
        "        y2b = min(cy + cut_h // 2, H)\n",
        "\n",
        "        images_aug = images.clone()\n",
        "        images_aug[:, :, y1b:y2b, x1:x2] = images[perm, :, y1b:y2b, x1:x2]\n",
        "\n",
        "        # adjust lambda based on actual area swapped\n",
        "        area = (x2 - x1) * (y2b - y1b)\n",
        "        lam = 1.0 - area / float(W * H)\n",
        "    else:\n",
        "        lam = torch.distributions.Beta(mixup_alpha, mixup_alpha).sample().item()\n",
        "        images_aug = images * lam + images[perm] * (1.0 - lam)\n",
        "\n",
        "    targets_soft = y1 * lam + y2 * (1.0 - lam)\n",
        "    return images_aug, targets_soft"
      ],
      "metadata": {
        "id": "zpVyEsL75qJW"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# -------------------------\n",
        "# Metrics\n",
        "# -------------------------\n",
        "@torch.no_grad()\n",
        "def accuracy_topk(logits: torch.Tensor, targets: torch.Tensor, ks=(1, 3, 5)) -> Dict[int, float]:\n",
        "    \"\"\"\n",
        "    targets can be:\n",
        "      - int64 class indices [B]\n",
        "      - soft targets [B, num_classes] (we'll argmax for accuracy reporting)\n",
        "    \"\"\"\n",
        "    if targets.ndim == 2:\n",
        "        targets = targets.argmax(dim=1)\n",
        "\n",
        "    max_k = max(ks)\n",
        "    B = targets.size(0)\n",
        "    _, pred = torch.topk(logits, k=max_k, dim=1)\n",
        "    correct = pred.eq(targets.view(-1, 1).expand_as(pred))\n",
        "    out = {}\n",
        "    for k in ks:\n",
        "        out[k] = 100.0 * correct[:, :k].any(dim=1).float().sum().item() / B\n",
        "    return out"
      ],
      "metadata": {
        "id": "1X9ksaTi5tNl"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from typing import Optional\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "def train_one_epoch(\n",
        "    model: nn.Module,\n",
        "    dataloader,\n",
        "    optimizer: torch.optim.Optimizer,\n",
        "    scheduler,\n",
        "    device: str = \"cuda\",\n",
        "    scaler=None,\n",
        "    autocast_dtype: str = \"fp16\",\n",
        "    use_amp: bool = True,\n",
        "    grad_clip_norm: Optional[float] = 1.0,\n",
        "    label_smoothing: float = 0.1,\n",
        "    mixup_alpha: float = 0.0,\n",
        "    cutmix_alpha: float = 0.0,\n",
        "    mix_prob: float = 1.0,\n",
        "    num_classes: int = 100,\n",
        "    channels_last: bool = False,\n",
        "    print_every: int = 100):\n",
        "    model.train()\n",
        "\n",
        "    use_scaler = (scaler is not None) and use_amp and autocast_dtype.lower() in (\"fp16\", \"float16\")\n",
        "\n",
        "    running_loss = 0.0\n",
        "    total = 0\n",
        "    c1 = c3 = c5 = 0.0\n",
        "\n",
        "    t0 = time.time()\n",
        "    for step, (images, targets) in enumerate(dataloader, start=1):\n",
        "        images = images.to(device, non_blocking=True)\n",
        "        targets = targets.to(device, non_blocking=True)\n",
        "\n",
        "        if channels_last:\n",
        "            images = images.contiguous(memory_format=torch.channels_last)\n",
        "\n",
        "        B = targets.size(0)\n",
        "\n",
        "        # mixup/cutmix => soft targets\n",
        "        images_aug, targets_soft = apply_mixup_cutmix(\n",
        "            images, targets,\n",
        "            num_classes=num_classes,\n",
        "            mixup_alpha=mixup_alpha,\n",
        "            cutmix_alpha=cutmix_alpha,\n",
        "            prob=mix_prob)\n",
        "\n",
        "        use_mix = (mixup_alpha > 0.0) or (cutmix_alpha > 0.0)\n",
        "        targets_for_acc = targets_soft if use_mix else targets\n",
        "\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "\n",
        "        with autocast_ctx(device=device, enabled=use_amp, dtype=autocast_dtype, cache_enabled=True):\n",
        "            logits = model(images_aug)  # [B, K]\n",
        "\n",
        "        # loss in fp32\n",
        "        if use_mix:\n",
        "            loss = soft_target_cross_entropy(logits.float(), targets_soft)\n",
        "        else:\n",
        "            loss = F.cross_entropy(logits.float(), targets, label_smoothing=label_smoothing)\n",
        "\n",
        "        if use_scaler:\n",
        "            scaler.scale(loss).backward()\n",
        "            if grad_clip_norm is not None:\n",
        "                scaler.unscale_(optimizer)\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip_norm)\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "        else:\n",
        "            loss.backward()\n",
        "            if grad_clip_norm is not None:\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip_norm)\n",
        "            optimizer.step()\n",
        "\n",
        "        if scheduler is not None:\n",
        "            scheduler.step()\n",
        "\n",
        "        # metrics\n",
        "        running_loss += loss.item() * B\n",
        "        total += B\n",
        "        accs = accuracy_topk(logits.detach(), targets_for_acc, ks=(1, 3, 5))\n",
        "        c1 += accs[1] * B / 100.0\n",
        "        c3 += accs[3] * B / 100.0\n",
        "        c5 += accs[5] * B / 100.0\n",
        "\n",
        "        if print_every and (step % print_every == 0):\n",
        "            dt = time.time() - t0\n",
        "            imgs_sec = total / max(dt, 1e-9)\n",
        "            print(\n",
        "                f\"[train step {step}/{len(dataloader)}] \"\n",
        "                f\"loss {running_loss/total:.4f} | \"\n",
        "                f\"top1 {100*c1/total:.2f}% | top3 {100*c3/total:.2f}% | top5 {100*c5/total:.2f}% | \"\n",
        "                f\"{imgs_sec:.1f} img/s | lr {optimizer.param_groups[0]['lr']:.2e}\")\n",
        "\n",
        "    avg_loss = running_loss / total\n",
        "    metrics = {\n",
        "        \"top1\": 100.0 * c1 / total,\n",
        "        \"top3\": 100.0 * c3 / total,\n",
        "        \"top5\": 100.0 * c5 / total,}\n",
        "\n",
        "    return avg_loss, metrics"
      ],
      "metadata": {
        "id": "oDbIUA_65uZi"
      },
      "execution_count": 51,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "@torch.no_grad()\n",
        "def evaluate_one_epoch(\n",
        "    model: nn.Module,\n",
        "    dataloader,\n",
        "    device: str = \"cuda\",\n",
        "    autocast_dtype: str = \"fp16\",\n",
        "    use_amp: bool = True,\n",
        "    label_smoothing: float = 0.0,\n",
        "    channels_last: bool = False):\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    running_loss = 0.0\n",
        "    total = 0\n",
        "    c1 = c3 = c5 = 0.0\n",
        "\n",
        "    for images, targets in dataloader:\n",
        "        images = images.to(device, non_blocking=True)\n",
        "        targets = targets.to(device, non_blocking=True)\n",
        "        if channels_last:\n",
        "            images = images.contiguous(memory_format=torch.channels_last)\n",
        "\n",
        "        B = targets.size(0)\n",
        "\n",
        "        with autocast_ctx(device=device, enabled=use_amp, dtype=autocast_dtype, cache_enabled=True):\n",
        "            logits = model(images)\n",
        "\n",
        "        loss = F.cross_entropy(logits.float(), targets, label_smoothing=label_smoothing)\n",
        "\n",
        "        running_loss += loss.item() * B\n",
        "        total += B\n",
        "\n",
        "        accs = accuracy_topk(logits, targets, ks=(1, 3, 5))\n",
        "        c1 += accs[1] * B / 100.0\n",
        "        c3 += accs[3] * B / 100.0\n",
        "        c5 += accs[5] * B / 100.0\n",
        "\n",
        "    avg_loss = running_loss / total\n",
        "    metrics = {\n",
        "        \"top1\": 100.0 * c1 / total,\n",
        "        \"top3\": 100.0 * c3 / total,\n",
        "        \"top5\": 100.0 * c5 / total,}\n",
        "\n",
        "    return avg_loss, metrics"
      ],
      "metadata": {
        "id": "ASZKG0qq6GC8"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "def train_model(\n",
        "    model: nn.Module,\n",
        "    train_loader,\n",
        "    epochs: int,\n",
        "    val_loader=None,\n",
        "    device: str = \"cuda\",\n",
        "    lr: float = 5e-4,\n",
        "    weight_decay: float = 0.05,\n",
        "    autocast_dtype: str = \"fp16\",\n",
        "    use_amp: bool = True,\n",
        "    grad_clip_norm: float | None = 1.0,\n",
        "    warmup_ratio: float = 0.05,\n",
        "    min_lr: float = 0.0,\n",
        "    label_smoothing: float = 0.1,\n",
        "    print_every: int = 100,\n",
        "    save_path: str = \"best_model.pt\",\n",
        "    last_path: str = \"last_model.pt\",\n",
        "    resume_path: str | None = None,\n",
        "\n",
        "    mixup_alpha: float = 0.0,\n",
        "    cutmix_alpha: float = 0.0,\n",
        "    mix_prob: float = 1.0,\n",
        "    num_classes: int = 100,\n",
        "    channels_last: bool = False,\n",
        "\n",
        "    # Early stopping\n",
        "    early_stop: bool = True,\n",
        "    early_stop_metric: str = \"top1\",   # \"top1\" (max) o \"loss\" (min)\n",
        "    early_stop_patience: int = 10,\n",
        "    early_stop_min_delta: float = 0.0,\n",
        "    early_stop_require_monotonic: bool = False,):\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    # Optimizer\n",
        "    param_groups = build_param_groups_no_wd(model, weight_decay=weight_decay)\n",
        "    optimizer = torch.optim.AdamW(param_groups, lr=lr, betas=(0.9, 0.999), eps=1e-8)\n",
        "\n",
        "    # Scheduler warmup + cosine (step-based)\n",
        "    total_steps = epochs * len(train_loader)\n",
        "    warmup_steps = int(total_steps * warmup_ratio)\n",
        "    scheduler = WarmupCosineLR(\n",
        "        optimizer,\n",
        "        total_steps=total_steps,\n",
        "        warmup_steps=warmup_steps,\n",
        "        min_lr=min_lr,)\n",
        "\n",
        "    # AMP scaler (solo fp16)\n",
        "    scaler = None\n",
        "    if use_amp and autocast_dtype.lower() in (\"fp16\", \"float16\"):\n",
        "        scaler = make_grad_scaler(device=device, enabled=True)\n",
        "\n",
        "    # Resume\n",
        "    start_epoch = 0\n",
        "    best_val_top1 = -float(\"inf\")\n",
        "    best_val_loss = float(\"inf\")\n",
        "    best_epoch = 0\n",
        "\n",
        "    if resume_path is not None:\n",
        "        ckpt = load_checkpoint(\n",
        "            resume_path, model,\n",
        "            optimizer=optimizer, scheduler=scheduler, scaler=scaler,\n",
        "            map_location=device,\n",
        "            strict=True,)\n",
        "\n",
        "        start_epoch = int(ckpt.get(\"epoch\", 0))\n",
        "        best_val_top1 = float(ckpt.get(\"best_top1\", best_val_top1))\n",
        "        extra = ckpt.get(\"extra\", {}) or {}\n",
        "        best_val_loss = float(extra.get(\"best_val_loss\", best_val_loss))\n",
        "        best_epoch = int(extra.get(\"best_epoch\", best_epoch))\n",
        "        print(f\"Resumed from {resume_path} at epoch {start_epoch} | best_top1 {best_val_top1:.2f}% | best_loss {best_val_loss:.4f}\")\n",
        "\n",
        "    history = {\n",
        "        \"train_loss\": [], \"train_top1\": [], \"train_top3\": [], \"train_top5\": [],\n",
        "        \"val_loss\": [], \"val_top1\": [], \"val_top3\": [], \"val_top5\": [],\n",
        "        \"lr\": []}\n",
        "\n",
        "    # Early stop state\n",
        "    metric = early_stop_metric.lower()\n",
        "    assert metric in (\"top1\", \"loss\"), \"early_stop_metric must be 'top1' or 'loss'\"\n",
        "    patience = int(early_stop_patience)\n",
        "\n",
        "    if metric == \"top1\":\n",
        "        best_metric = best_val_top1\n",
        "        mode = \"max\"\n",
        "    else:\n",
        "        best_metric = best_val_loss\n",
        "        mode = \"min\"\n",
        "\n",
        "    bad_epochs = 0\n",
        "    last_vals = []  # para monotonic window\n",
        "\n",
        "    def _is_improvement(curr: float, best: float) -> bool:\n",
        "        d = float(early_stop_min_delta)\n",
        "        if mode == \"max\":\n",
        "            return curr > (best + d)\n",
        "        else:\n",
        "            return curr < (best - d)\n",
        "\n",
        "    def _monotonic_ok(vals: list[float]) -> bool:\n",
        "        if not early_stop_require_monotonic or len(vals) < 2:\n",
        "            return True\n",
        "        if mode == \"max\":\n",
        "            # permitimos fluctuación hacia arriba si es monotonic? -> aquí interpretamos monotonic como no empeorar\n",
        "            return all(vals[i] >= vals[i + 1] for i in range(len(vals) - 1)) is False  # no aplica bien\n",
        "        else:\n",
        "            return all(vals[i] <= vals[i + 1] for i in range(len(vals) - 1)) is False\n",
        "\n",
        "    # Nota: monotonic estricta suele ser mala idea; mejor dejamos require_monotonic=False.\n",
        "    # Si lo quieres, te lo dejo implementado como \"degradación monotónica\" en ventana.\n",
        "    def _degradation_monotonic(vals: list[float]) -> bool:\n",
        "        if not early_stop_require_monotonic or len(vals) < 2:\n",
        "            return True\n",
        "        if mode == \"max\":\n",
        "            return all(vals[i] >= vals[i + 1] for i in range(len(vals) - 1))  # cae monotónicamente\n",
        "        else:\n",
        "            return all(vals[i] <= vals[i + 1] for i in range(len(vals) - 1))  # sube monotónicamente\n",
        "\n",
        "    for epoch in range(start_epoch + 1, epochs + 1):\n",
        "        print(f\"\\n=== Epoch {epoch}/{epochs} ===\")\n",
        "        t_epoch = time.time()\n",
        "\n",
        "        # --- Train ---\n",
        "        tr_loss, tr_m = train_one_epoch(\n",
        "            model=model,\n",
        "            dataloader=train_loader,\n",
        "            optimizer=optimizer,\n",
        "            scheduler=scheduler,\n",
        "            device=device,\n",
        "            scaler=scaler,\n",
        "            autocast_dtype=autocast_dtype,\n",
        "            use_amp=use_amp,\n",
        "            grad_clip_norm=grad_clip_norm,\n",
        "            label_smoothing=label_smoothing,\n",
        "            mixup_alpha=mixup_alpha,\n",
        "            cutmix_alpha=cutmix_alpha,\n",
        "            mix_prob=mix_prob,\n",
        "            num_classes=num_classes,\n",
        "            channels_last=channels_last,\n",
        "            print_every=print_every,)\n",
        "\n",
        "        history[\"train_loss\"].append(tr_loss)\n",
        "        history[\"train_top1\"].append(tr_m[\"top1\"])\n",
        "        history[\"train_top3\"].append(tr_m[\"top3\"])\n",
        "        history[\"train_top5\"].append(tr_m[\"top5\"])\n",
        "        history[\"lr\"].append(optimizer.param_groups[0][\"lr\"])\n",
        "\n",
        "        print(f\"[Train] loss {tr_loss:.4f} | top1 {tr_m['top1']:.2f}% | top3 {tr_m['top3']:.2f}% | top5 {tr_m['top5']:.2f}% | lr {optimizer.param_groups[0]['lr']:.2e}\")\n",
        "\n",
        "        # guardar \"last\" siempre\n",
        "        save_checkpoint(\n",
        "            last_path, model, optimizer, scheduler, scaler,\n",
        "            epoch=epoch, best_top1=best_val_top1,\n",
        "            extra={\n",
        "                \"autocast_dtype\": autocast_dtype,\n",
        "                \"use_amp\": use_amp,\n",
        "                \"best_val_loss\": best_val_loss,\n",
        "                \"best_epoch\": best_epoch,\n",
        "                \"early_stop_metric\": metric,\n",
        "                \"early_stop_patience\": patience,\n",
        "                \"early_stop_min_delta\": float(early_stop_min_delta),},)\n",
        "\n",
        "        # --- Val ---\n",
        "        if val_loader is not None:\n",
        "            va_loss, va_m = evaluate_one_epoch(\n",
        "                model=model,\n",
        "                dataloader=val_loader,\n",
        "                device=device,\n",
        "                autocast_dtype=autocast_dtype,\n",
        "                use_amp=use_amp,\n",
        "                label_smoothing=0.0,\n",
        "                channels_last=channels_last,)\n",
        "\n",
        "            history[\"val_loss\"].append(va_loss)\n",
        "            history[\"val_top1\"].append(va_m[\"top1\"])\n",
        "            history[\"val_top3\"].append(va_m[\"top3\"])\n",
        "            history[\"val_top5\"].append(va_m[\"top5\"])\n",
        "\n",
        "            print(f\"[Val]   loss {va_loss:.4f} | top1 {va_m['top1']:.2f}% | top3 {va_m['top3']:.2f}% | top5 {va_m['top5']:.2f}%\")\n",
        "\n",
        "            # Guardar best por top1\n",
        "            if va_m[\"top1\"] > best_val_top1:\n",
        "                best_val_top1 = va_m[\"top1\"]\n",
        "                # También trackea best loss por si early_stop usa loss\n",
        "                if va_loss < best_val_loss:\n",
        "                    best_val_loss = va_loss\n",
        "                    best_epoch = epoch\n",
        "\n",
        "                save_checkpoint(\n",
        "                    save_path, model, optimizer, scheduler, scaler,\n",
        "                    epoch=epoch, best_top1=best_val_top1,\n",
        "                    extra={\n",
        "                        \"autocast_dtype\": autocast_dtype,\n",
        "                        \"use_amp\": use_amp,\n",
        "                        \"best_val_loss\": best_val_loss,\n",
        "                        \"best_epoch\": best_epoch,},)\n",
        "\n",
        "                print(f\"Best saved to {save_path} (val top1 {best_val_top1:.2f}%)\")\n",
        "\n",
        "            # Early stop robusto ---\n",
        "            if early_stop:\n",
        "                curr_metric = va_m[\"top1\"] if metric == \"top1\" else va_loss\n",
        "\n",
        "                # ventana para monotonic degradation (opcional)\n",
        "                last_vals.append(float(curr_metric))\n",
        "                if len(last_vals) > patience:\n",
        "                    last_vals = last_vals[-patience:]\n",
        "\n",
        "                if _is_improvement(curr_metric, best_metric):\n",
        "                    best_metric = float(curr_metric)\n",
        "                    bad_epochs = 0\n",
        "                else:\n",
        "                    bad_epochs += 1\n",
        "\n",
        "                if bad_epochs >= patience:\n",
        "                    ok = _degradation_monotonic(last_vals)\n",
        "                    if ok:\n",
        "                        print(\n",
        "                            f\"Early-stop: no improvement on val_{metric} for {patience} epochs. \"\n",
        "                            f\"Best={best_metric:.4f if metric=='loss' else best_metric:.2f} | \"\n",
        "                            f\"Last={curr_metric:.4f if metric=='loss' else curr_metric:.2f}\")\n",
        "\n",
        "                        break\n",
        "\n",
        "        dt = time.time() - t_epoch\n",
        "        print(f\"Epoch time: {dt/60:.2f} min\")\n",
        "\n",
        "    return history, model"
      ],
      "metadata": {
        "id": "o9ZlcH5I6JL2"
      },
      "execution_count": 49,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "\n",
        "# Training"
      ],
      "metadata": {
        "id": "VagXinWMCV-t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "seed_everything(7)\n",
        "\n",
        "model_volo_medium = VOLOClassifier(\n",
        "    num_classes=100,\n",
        "    img_size=32,\n",
        "    patch_size=4,\n",
        "    hierarchical=False,\n",
        "\n",
        "    embed_dim=320,\n",
        "    outlooker_depth=5,\n",
        "    outlooker_heads=10,\n",
        "    transformer_depth=10,\n",
        "    transformer_heads=10,\n",
        "\n",
        "    kernel_size=3,\n",
        "    mlp_ratio=4.0,\n",
        "    dropout=0.12,\n",
        "    attn_dropout=0.05,\n",
        "    drop_path_rate=0.20,\n",
        "\n",
        "    pooling=\"cls\",\n",
        "    cls_attn_depth=2,\n",
        "    use_pos_embed=True,\n",
        "    use_cls_pos=True,\n",
        ").to(\"cuda\")\n",
        "\n",
        "\n",
        "history_splus, best_splus = train_model(\n",
        "    model=model_volo_medium,\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    epochs=50,\n",
        "    device=\"cuda\",\n",
        "\n",
        "    lr=5e-4,\n",
        "    weight_decay=0.08,      # ↑ un poco para generalización\n",
        "    autocast_dtype=\"fp16\",\n",
        "    use_amp=True,\n",
        "    grad_clip_norm=1.0,\n",
        "    warmup_ratio=0.06,\n",
        "    min_lr=1e-6,\n",
        "\n",
        "    label_smoothing=0.0,    # CutMix => 0\n",
        "    save_path=\"volo_splus_best.pt\",\n",
        "    last_path=\"volo_splus_last.pt\",\n",
        "\n",
        "    mixup_alpha=0.0,\n",
        "    cutmix_alpha=0.8,       # ↓ respecto a 1.0 para CLS stability\n",
        "    mix_prob=1.0,\n",
        "    num_classes=100,\n",
        "\n",
        "    early_stop=True,\n",
        "    early_stop_metric=\"top1\",\n",
        "    early_stop_patience=5,\n",
        "    early_stop_min_delta=0.05)\n",
        "\n",
        "\n",
        "print(\"Done. Best val top1:\", max(history_splus[\"val_top1\"]) if len(history_splus[\"val_top1\"]) else None)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2Wk2-Bfm7dfp",
        "outputId": "40633c6a-cf35-4f2f-ac75-652dbfe7ff55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Epoch 1/50 ===\n",
            "[train step 100/352] loss 4.6378 | top1 2.02% | top3 5.22% | top5 8.29% | 586.2 img/s | lr 4.73e-05\n",
            "[train step 200/352] loss 4.5721 | top1 2.39% | top3 6.52% | top5 10.00% | 589.2 img/s | lr 9.47e-05\n",
            "[train step 300/352] loss 4.5235 | top1 2.93% | top3 7.76% | top5 11.75% | 585.3 img/s | lr 1.42e-04\n",
            "[Train] loss 4.4993 | top1 3.23% | top3 8.47% | top5 12.66% | lr 1.67e-04\n",
            "[Val]   loss 4.1517 | top1 6.72% | top3 16.30% | top5 22.86%\n",
            "Best saved to volo_splus_best.pt (val top1 6.72%)\n",
            "Epoch time: 1.37 min\n",
            "\n",
            "=== Epoch 2/50 ===\n",
            "[train step 100/352] loss 4.3465 | top1 5.35% | top3 13.09% | top5 19.01% | 585.6 img/s | lr 2.14e-04\n",
            "[train step 200/352] loss 4.3302 | top1 5.67% | top3 13.49% | top5 19.38% | 591.5 img/s | lr 2.61e-04\n",
            "[train step 300/352] loss 4.3152 | top1 5.82% | top3 13.85% | top5 19.95% | 598.5 img/s | lr 3.09e-04\n",
            "[Train] loss 4.3096 | top1 5.86% | top3 13.95% | top5 20.12% | lr 3.33e-04\n",
            "[Val]   loss 3.9565 | top1 8.62% | top3 20.42% | top5 29.56%\n",
            "Best saved to volo_splus_best.pt (val top1 8.62%)\n",
            "Epoch time: 1.37 min\n",
            "\n",
            "=== Epoch 3/50 ===\n",
            "[train step 100/352] loss 4.2478 | top1 7.12% | top3 16.02% | top5 22.66% | 607.1 img/s | lr 3.81e-04\n",
            "[train step 200/352] loss 4.2299 | top1 7.30% | top3 16.59% | top5 23.39% | 601.2 img/s | lr 4.28e-04\n",
            "[train step 300/352] loss 4.2216 | top1 7.26% | top3 16.80% | top5 23.78% | 599.0 img/s | lr 4.75e-04\n",
            "[Train] loss 4.2119 | top1 7.38% | top3 17.04% | top5 24.16% | lr 5.00e-04\n",
            "[Val]   loss 3.7978 | top1 10.72% | top3 24.76% | top5 33.78%\n",
            "Best saved to volo_splus_best.pt (val top1 10.72%)\n",
            "Epoch time: 1.66 min\n",
            "\n",
            "=== Epoch 4/50 ===\n",
            "[train step 100/352] loss 4.1246 | top1 8.86% | top3 19.68% | top5 27.26% | 587.1 img/s | lr 5.00e-04\n",
            "[train step 200/352] loss 4.1252 | top1 8.88% | top3 19.87% | top5 27.56% | 597.0 img/s | lr 5.00e-04\n",
            "[train step 300/352] loss 4.0844 | top1 9.55% | top3 21.04% | top5 28.96% | 596.2 img/s | lr 5.00e-04\n",
            "[Train] loss 4.0688 | top1 9.77% | top3 21.47% | top5 29.52% | lr 4.99e-04\n",
            "[Val]   loss 3.4956 | top1 15.74% | top3 32.66% | top5 42.70%\n",
            "Best saved to volo_splus_best.pt (val top1 15.74%)\n",
            "Epoch time: 1.56 min\n",
            "\n",
            "=== Epoch 5/50 ===\n",
            "[train step 100/352] loss 4.0277 | top1 11.02% | top3 23.81% | top5 31.84% | 588.6 img/s | lr 4.99e-04\n",
            "[train step 200/352] loss 4.0153 | top1 10.90% | top3 23.75% | top5 31.98% | 588.2 img/s | lr 4.99e-04\n",
            "[train step 300/352] loss 3.9781 | top1 11.67% | top3 24.79% | top5 33.23% | 596.6 img/s | lr 4.98e-04\n",
            "[Train] loss 3.9600 | top1 12.07% | top3 25.34% | top5 33.85% | lr 4.98e-04\n",
            "[Val]   loss 3.2701 | top1 21.26% | top3 40.46% | top5 50.48%\n",
            "Best saved to volo_splus_best.pt (val top1 21.26%)\n",
            "Epoch time: 1.45 min\n",
            "\n",
            "=== Epoch 6/50 ===\n",
            "[train step 100/352] loss 3.8868 | top1 14.03% | top3 28.62% | top5 37.82% | 589.2 img/s | lr 4.97e-04\n",
            "[train step 200/352] loss 3.9264 | top1 13.31% | top3 27.16% | top5 36.30% | 598.0 img/s | lr 4.96e-04\n",
            "[train step 300/352] loss 3.8962 | top1 13.86% | top3 27.86% | top5 36.94% | 598.1 img/s | lr 4.95e-04\n",
            "[Train] loss 3.8929 | top1 13.94% | top3 27.95% | top5 36.96% | lr 4.95e-04\n",
            "[Val]   loss 3.2294 | top1 21.76% | top3 40.08% | top5 50.56%\n",
            "Best saved to volo_splus_best.pt (val top1 21.76%)\n",
            "Epoch time: 1.55 min\n",
            "\n",
            "=== Epoch 7/50 ===\n",
            "[train step 100/352] loss 3.7800 | top1 15.70% | top3 31.28% | top5 40.38% | 590.1 img/s | lr 4.94e-04\n",
            "[train step 200/352] loss 3.7917 | top1 15.86% | top3 31.28% | top5 40.30% | 590.5 img/s | lr 4.93e-04\n",
            "[train step 300/352] loss 3.7960 | top1 15.90% | top3 31.23% | top5 40.40% | 598.1 img/s | lr 4.92e-04\n",
            "[Train] loss 3.7780 | top1 16.32% | top3 31.78% | top5 41.09% | lr 4.91e-04\n",
            "[Val]   loss 3.1190 | top1 23.10% | top3 42.76% | top5 53.52%\n",
            "Best saved to volo_splus_best.pt (val top1 23.10%)\n",
            "Epoch time: 1.45 min\n",
            "\n",
            "=== Epoch 8/50 ===\n",
            "[train step 100/352] loss 3.7453 | top1 17.18% | top3 32.18% | top5 41.22% | 587.8 img/s | lr 4.90e-04\n",
            "[train step 200/352] loss 3.7296 | top1 17.43% | top3 32.83% | top5 42.06% | 596.2 img/s | lr 4.88e-04\n",
            "[train step 300/352] loss 3.7190 | top1 17.44% | top3 32.92% | top5 42.09% | 596.7 img/s | lr 4.87e-04\n",
            "[Train] loss 3.7156 | top1 17.38% | top3 33.02% | top5 42.30% | lr 4.86e-04\n",
            "[Val]   loss 2.9770 | top1 25.12% | top3 46.12% | top5 56.74%\n",
            "Best saved to volo_splus_best.pt (val top1 25.12%)\n",
            "Epoch time: 1.47 min\n",
            "\n",
            "=== Epoch 9/50 ===\n",
            "[train step 100/352] loss 3.6002 | top1 19.66% | top3 37.62% | top5 47.25% | 586.9 img/s | lr 4.85e-04\n",
            "[train step 200/352] loss 3.6402 | top1 19.06% | top3 36.44% | top5 45.75% | 597.2 img/s | lr 4.83e-04\n",
            "[train step 300/352] loss 3.6392 | top1 19.16% | top3 36.29% | top5 45.68% | 596.5 img/s | lr 4.81e-04\n",
            "[Train] loss 3.6434 | top1 19.18% | top3 36.17% | top5 45.50% | lr 4.80e-04\n",
            "[Val]   loss 2.8360 | top1 28.84% | top3 50.32% | top5 60.70%\n",
            "Best saved to volo_splus_best.pt (val top1 28.84%)\n",
            "Epoch time: 1.47 min\n",
            "\n",
            "=== Epoch 10/50 ===\n",
            "[train step 100/352] loss 3.6038 | top1 19.49% | top3 36.14% | top5 45.77% | 586.3 img/s | lr 4.78e-04\n",
            "[train step 200/352] loss 3.5910 | top1 20.28% | top3 37.06% | top5 46.89% | 597.0 img/s | lr 4.76e-04\n",
            "[train step 300/352] loss 3.5766 | top1 20.84% | top3 37.70% | top5 47.39% | 596.1 img/s | lr 4.74e-04\n",
            "[Train] loss 3.5739 | top1 20.88% | top3 37.82% | top5 47.57% | lr 4.73e-04\n",
            "[Val]   loss 2.7527 | top1 31.62% | top3 52.34% | top5 62.64%\n",
            "Best saved to volo_splus_best.pt (val top1 31.62%)\n",
            "Epoch time: 1.48 min\n",
            "\n",
            "=== Epoch 11/50 ===\n",
            "[train step 100/352] loss 3.5714 | top1 21.55% | top3 38.62% | top5 47.80% | 601.2 img/s | lr 4.71e-04\n",
            "[train step 200/352] loss 3.5723 | top1 21.41% | top3 38.77% | top5 48.23% | 599.7 img/s | lr 4.69e-04\n",
            "[train step 300/352] loss 3.5477 | top1 21.55% | top3 39.08% | top5 48.45% | 598.4 img/s | lr 4.66e-04\n",
            "[Train] loss 3.5555 | top1 21.39% | top3 38.95% | top5 48.31% | lr 4.65e-04\n",
            "[Val]   loss 2.6667 | top1 33.86% | top3 54.36% | top5 64.10%\n",
            "Best saved to volo_splus_best.pt (val top1 33.86%)\n",
            "Epoch time: 1.57 min\n",
            "\n",
            "=== Epoch 12/50 ===\n",
            "[train step 100/352] loss 3.4300 | top1 23.48% | top3 41.73% | top5 52.13% | 591.4 img/s | lr 4.63e-04\n",
            "[train step 200/352] loss 3.4662 | top1 23.32% | top3 41.56% | top5 51.56% | 597.7 img/s | lr 4.60e-04\n",
            "[train step 300/352] loss 3.4800 | top1 22.96% | top3 40.74% | top5 50.73% | 598.0 img/s | lr 4.58e-04\n",
            "[Train] loss 3.4771 | top1 23.06% | top3 40.89% | top5 50.77% | lr 4.56e-04\n",
            "[Val]   loss 2.6053 | top1 34.58% | top3 56.10% | top5 66.06%\n",
            "Best saved to volo_splus_best.pt (val top1 34.58%)\n",
            "Epoch time: 1.54 min\n",
            "\n",
            "=== Epoch 13/50 ===\n",
            "[train step 100/352] loss 3.3941 | top1 25.33% | top3 44.06% | top5 53.81% | 586.5 img/s | lr 4.53e-04\n",
            "[train step 200/352] loss 3.4389 | top1 24.75% | top3 42.83% | top5 52.62% | 589.6 img/s | lr 4.51e-04\n",
            "[train step 300/352] loss 3.4473 | top1 24.53% | top3 42.60% | top5 52.21% | 597.3 img/s | lr 4.48e-04\n",
            "[Train] loss 3.4600 | top1 24.14% | top3 42.18% | top5 51.77% | lr 4.46e-04\n",
            "[Val]   loss 2.5400 | top1 35.94% | top3 58.20% | top5 67.76%\n",
            "Best saved to volo_splus_best.pt (val top1 35.94%)\n",
            "Epoch time: 1.45 min\n",
            "\n",
            "=== Epoch 14/50 ===\n",
            "[train step 100/352] loss 3.4046 | top1 24.98% | top3 43.69% | top5 53.24% | 583.8 img/s | lr 4.43e-04\n",
            "[train step 200/352] loss 3.3762 | top1 25.86% | top3 44.93% | top5 54.58% | 595.2 img/s | lr 4.40e-04\n",
            "[train step 300/352] loss 3.3869 | top1 25.67% | top3 44.64% | top5 54.28% | 595.4 img/s | lr 4.37e-04\n",
            "[Train] loss 3.3850 | top1 25.70% | top3 44.59% | top5 54.13% | lr 4.36e-04\n",
            "[Val]   loss 2.4611 | top1 37.60% | top3 59.78% | top5 70.00%\n",
            "Best saved to volo_splus_best.pt (val top1 37.60%)\n",
            "Epoch time: 1.55 min\n",
            "\n",
            "=== Epoch 15/50 ===\n",
            "[train step 100/352] loss 3.3712 | top1 26.08% | top3 44.84% | top5 54.78% | 588.9 img/s | lr 4.32e-04\n",
            "[train step 200/352] loss 3.3604 | top1 26.23% | top3 45.20% | top5 54.89% | 588.6 img/s | lr 4.29e-04\n",
            "[train step 300/352] loss 3.3516 | top1 26.61% | top3 45.93% | top5 55.57% | 596.7 img/s | lr 4.26e-04\n",
            "[Train] loss 3.3514 | top1 26.57% | top3 45.67% | top5 55.43% | lr 4.24e-04\n",
            "[Val]   loss 2.4121 | top1 38.06% | top3 60.70% | top5 70.58%\n",
            "Best saved to volo_splus_best.pt (val top1 38.06%)\n",
            "Epoch time: 1.45 min\n",
            "\n",
            "=== Epoch 16/50 ===\n",
            "[train step 100/352] loss 3.3711 | top1 26.12% | top3 44.58% | top5 54.71% | 590.2 img/s | lr 4.21e-04\n",
            "[train step 200/352] loss 3.3439 | top1 26.80% | top3 45.61% | top5 55.59% | 598.0 img/s | lr 4.17e-04\n",
            "[train step 300/352] loss 3.3320 | top1 27.23% | top3 45.95% | top5 55.78% | 597.4 img/s | lr 4.13e-04\n",
            "[Train] loss 3.3236 | top1 27.28% | top3 46.05% | top5 55.80% | lr 4.12e-04\n",
            "[Val]   loss 2.2997 | top1 41.44% | top3 62.86% | top5 72.82%\n",
            "Best saved to volo_splus_best.pt (val top1 41.44%)\n",
            "Epoch time: 1.55 min\n",
            "\n",
            "=== Epoch 17/50 ===\n",
            "[train step 100/352] loss 3.2732 | top1 29.36% | top3 48.36% | top5 58.09% | 587.4 img/s | lr 4.08e-04\n",
            "[train step 200/352] loss 3.3060 | top1 28.10% | top3 47.19% | top5 56.73% | 589.4 img/s | lr 4.04e-04\n",
            "[train step 300/352] loss 3.3039 | top1 28.04% | top3 47.29% | top5 56.91% | 596.3 img/s | lr 4.00e-04\n",
            "[Train] loss 3.3024 | top1 28.21% | top3 47.54% | top5 57.16% | lr 3.98e-04\n",
            "[Val]   loss 2.2911 | top1 40.84% | top3 63.14% | top5 72.22%\n",
            "Epoch time: 1.44 min\n",
            "\n",
            "=== Epoch 18/50 ===\n",
            "[train step 100/352] loss 3.3369 | top1 27.80% | top3 45.86% | top5 55.47% | 588.5 img/s | lr 3.95e-04\n",
            "[train step 200/352] loss 3.2137 | top1 30.00% | top3 49.27% | top5 58.63% | 598.3 img/s | lr 3.91e-04\n",
            "[train step 300/352] loss 3.2332 | top1 29.58% | top3 48.96% | top5 58.40% | 597.2 img/s | lr 3.87e-04\n",
            "[Train] loss 3.2237 | top1 29.77% | top3 48.92% | top5 58.39% | lr 3.85e-04\n",
            "[Val]   loss 2.2207 | top1 42.64% | top3 64.76% | top5 74.68%\n",
            "Best saved to volo_splus_best.pt (val top1 42.64%)\n",
            "Epoch time: 1.46 min\n",
            "\n",
            "=== Epoch 19/50 ===\n",
            "[train step 100/352] loss 3.1836 | top1 30.77% | top3 50.02% | top5 59.76% | 603.4 img/s | lr 3.81e-04\n",
            "[train step 200/352] loss 3.1619 | top1 31.04% | top3 50.32% | top5 59.77% | 598.3 img/s | lr 3.77e-04\n",
            "[train step 300/352] loss 3.1844 | top1 30.77% | top3 49.98% | top5 59.32% | 596.5 img/s | lr 3.73e-04\n",
            "[Train] loss 3.1735 | top1 30.96% | top3 50.22% | top5 59.52% | lr 3.70e-04\n",
            "[Val]   loss 2.1479 | top1 43.48% | top3 65.32% | top5 75.12%\n",
            "Best saved to volo_splus_best.pt (val top1 43.48%)\n",
            "Epoch time: 1.45 min\n",
            "\n",
            "=== Epoch 20/50 ===\n",
            "[train step 100/352] loss 3.1530 | top1 31.25% | top3 50.41% | top5 59.98% | 585.6 img/s | lr 3.66e-04\n",
            "[train step 200/352] loss 3.1601 | top1 30.84% | top3 49.67% | top5 59.41% | 595.1 img/s | lr 3.62e-04\n",
            "[train step 300/352] loss 3.1656 | top1 30.91% | top3 49.91% | top5 59.39% | 596.4 img/s | lr 3.58e-04\n",
            "[Train] loss 3.1742 | top1 30.90% | top3 50.00% | top5 59.50% | lr 3.56e-04\n",
            "[Val]   loss 2.1374 | top1 44.76% | top3 67.40% | top5 76.24%\n",
            "Best saved to volo_splus_best.pt (val top1 44.76%)\n",
            "Epoch time: 1.45 min\n",
            "\n",
            "=== Epoch 21/50 ===\n",
            "[train step 100/352] loss 3.2068 | top1 31.20% | top3 50.23% | top5 59.84% | 583.5 img/s | lr 3.51e-04\n",
            "[train step 200/352] loss 3.1896 | top1 30.94% | top3 50.40% | top5 59.94% | 595.9 img/s | lr 3.47e-04\n",
            "[train step 300/352] loss 3.1766 | top1 31.30% | top3 50.72% | top5 60.45% | 595.3 img/s | lr 3.42e-04\n",
            "[Train] loss 3.1766 | top1 31.38% | top3 50.77% | top5 60.46% | lr 3.40e-04\n",
            "[Val]   loss 2.1099 | top1 45.52% | top3 66.68% | top5 75.94%\n",
            "Best saved to volo_splus_best.pt (val top1 45.52%)\n",
            "Epoch time: 1.47 min\n",
            "\n",
            "=== Epoch 22/50 ===\n",
            "[train step 100/352] loss 3.1224 | top1 33.44% | top3 53.20% | top5 63.09% | 604.1 img/s | lr 3.36e-04\n",
            "[train step 200/352] loss 3.0644 | top1 34.12% | top3 54.30% | top5 63.57% | 596.8 img/s | lr 3.31e-04\n",
            "[train step 300/352] loss 3.0726 | top1 33.68% | top3 53.68% | top5 63.16% | 594.9 img/s | lr 3.27e-04\n",
            "[Train] loss 3.0791 | top1 33.58% | top3 53.68% | top5 63.12% | lr 3.24e-04\n",
            "[Val]   loss 2.0274 | top1 47.04% | top3 69.32% | top5 77.88%\n",
            "Best saved to volo_splus_best.pt (val top1 47.04%)\n",
            "Epoch time: 1.62 min\n",
            "\n",
            "=== Epoch 23/50 ===\n",
            "[train step 100/352] loss 3.0508 | top1 34.27% | top3 54.23% | top5 63.81% | 548.2 img/s | lr 3.20e-04\n",
            "[train step 200/352] loss 3.1035 | top1 32.71% | top3 52.25% | top5 61.59% | 568.8 img/s | lr 3.15e-04\n",
            "[train step 300/352] loss 3.1231 | top1 32.25% | top3 51.74% | top5 61.14% | 572.4 img/s | lr 3.11e-04\n",
            "[Train] loss 3.1190 | top1 32.34% | top3 51.90% | top5 61.22% | lr 3.08e-04\n",
            "[Val]   loss 2.0368 | top1 46.78% | top3 67.80% | top5 76.96%\n",
            "Epoch time: 1.39 min\n",
            "\n",
            "=== Epoch 24/50 ===\n",
            "[train step 100/352] loss 3.1293 | top1 32.98% | top3 53.37% | top5 62.59% | 538.8 img/s | lr 3.04e-04\n",
            "[train step 200/352] loss 3.1317 | top1 32.78% | top3 52.66% | top5 61.96% | 567.9 img/s | lr 2.99e-04\n",
            "[train step 300/352] loss 3.0874 | top1 33.51% | top3 53.56% | top5 62.86% | 578.8 img/s | lr 2.94e-04\n",
            "[Train] loss 3.0889 | top1 33.63% | top3 53.62% | top5 62.83% | lr 2.92e-04\n",
            "[Val]   loss 1.9669 | top1 48.82% | top3 70.08% | top5 78.44%\n",
            "Best saved to volo_splus_best.pt (val top1 48.82%)\n",
            "Epoch time: 1.39 min\n",
            "\n",
            "=== Epoch 25/50 ===\n",
            "[train step 100/352] loss 2.9479 | top1 36.27% | top3 56.58% | top5 65.53% | 593.7 img/s | lr 2.87e-04\n",
            "[train step 200/352] loss 2.9976 | top1 35.68% | top3 55.49% | top5 64.41% | 588.0 img/s | lr 2.83e-04\n",
            "[train step 300/352] loss 3.0129 | top1 35.48% | top3 55.18% | top5 64.06% | 592.2 img/s | lr 2.78e-04\n",
            "[Train] loss 3.0086 | top1 35.38% | top3 55.11% | top5 64.00% | lr 2.75e-04\n",
            "[Val]   loss 1.9174 | top1 49.36% | top3 71.44% | top5 79.58%\n",
            "Best saved to volo_splus_best.pt (val top1 49.36%)\n",
            "Epoch time: 1.37 min\n",
            "\n",
            "=== Epoch 26/50 ===\n",
            "[train step 100/352] loss 3.0033 | top1 34.67% | top3 53.92% | top5 62.92% | 578.0 img/s | lr 2.71e-04\n",
            "[train step 200/352] loss 3.0383 | top1 34.44% | top3 53.84% | top5 62.94% | 594.4 img/s | lr 2.66e-04\n",
            "[train step 300/352] loss 3.0440 | top1 34.42% | top3 53.89% | top5 62.94% | 593.9 img/s | lr 2.61e-04\n",
            "[Train] loss 3.0371 | top1 34.83% | top3 54.30% | top5 63.46% | lr 2.59e-04\n",
            "[Val]   loss 1.9247 | top1 50.42% | top3 71.44% | top5 78.88%\n",
            "Best saved to volo_splus_best.pt (val top1 50.42%)\n",
            "Epoch time: 1.36 min\n",
            "\n",
            "=== Epoch 27/50 ===\n",
            "[train step 100/352] loss 2.9904 | top1 36.41% | top3 56.31% | top5 65.13% | 587.4 img/s | lr 2.54e-04\n",
            "[train step 200/352] loss 2.9905 | top1 35.95% | top3 55.74% | top5 64.51% | 590.1 img/s | lr 2.49e-04\n",
            "[train step 300/352] loss 2.9856 | top1 35.95% | top3 56.00% | top5 64.98% | 595.8 img/s | lr 2.45e-04\n",
            "[Train] loss 2.9870 | top1 36.00% | top3 56.15% | top5 65.22% | lr 2.42e-04\n",
            "[Val]   loss 1.8641 | top1 51.40% | top3 71.98% | top5 80.42%\n",
            "Best saved to volo_splus_best.pt (val top1 51.40%)\n",
            "Epoch time: 1.46 min\n",
            "\n",
            "=== Epoch 28/50 ===\n",
            "[train step 100/352] loss 3.0146 | top1 35.89% | top3 55.75% | top5 65.16% | 583.8 img/s | lr 2.37e-04\n",
            "[train step 200/352] loss 2.9854 | top1 36.53% | top3 56.50% | top5 65.61% | 595.9 img/s | lr 2.33e-04\n",
            "[train step 300/352] loss 2.9118 | top1 38.06% | top3 58.18% | top5 67.12% | 595.1 img/s | lr 2.28e-04\n",
            "[Train] loss 2.9059 | top1 38.15% | top3 58.34% | top5 67.23% | lr 2.26e-04\n",
            "[Val]   loss 1.8323 | top1 52.72% | top3 73.08% | top5 80.94%\n",
            "Best saved to volo_splus_best.pt (val top1 52.72%)\n",
            "Epoch time: 1.47 min\n",
            "\n",
            "=== Epoch 29/50 ===\n",
            "[train step 100/352] loss 2.8929 | top1 38.63% | top3 58.34% | top5 67.61% | 585.4 img/s | lr 2.21e-04\n",
            "[train step 200/352] loss 2.8661 | top1 39.02% | top3 58.84% | top5 67.98% | 596.5 img/s | lr 2.16e-04\n",
            "[train step 300/352] loss 2.8931 | top1 38.62% | top3 58.53% | top5 67.57% | 595.4 img/s | lr 2.11e-04\n",
            "[Train] loss 2.9034 | top1 38.22% | top3 58.08% | top5 67.15% | lr 2.09e-04\n",
            "[Val]   loss 1.8439 | top1 52.02% | top3 72.90% | top5 80.80%\n",
            "Epoch time: 1.47 min\n",
            "\n",
            "=== Epoch 30/50 ===\n",
            "[train step 100/352] loss 2.8788 | top1 38.95% | top3 58.96% | top5 67.24% | 594.0 img/s | lr 2.04e-04\n",
            "[train step 200/352] loss 2.8537 | top1 39.51% | top3 59.45% | top5 68.12% | 597.5 img/s | lr 2.00e-04\n",
            "[train step 300/352] loss 2.8965 | top1 38.45% | top3 58.20% | top5 67.11% | 596.9 img/s | lr 1.95e-04\n",
            "[Train] loss 2.8939 | top1 38.69% | top3 58.44% | top5 67.34% | lr 1.93e-04\n",
            "[Val]   loss 1.8011 | top1 53.02% | top3 73.84% | top5 81.30%\n",
            "Best saved to volo_splus_best.pt (val top1 53.02%)\n",
            "Epoch time: 1.35 min\n",
            "\n",
            "=== Epoch 31/50 ===\n",
            "[train step 100/352] loss 2.8625 | top1 38.91% | top3 59.40% | top5 68.51% | 587.9 img/s | lr 1.88e-04\n",
            "[train step 200/352] loss 2.8318 | top1 39.72% | top3 59.84% | top5 68.84% | 600.4 img/s | lr 1.83e-04\n",
            "[train step 300/352] loss 2.8428 | top1 39.34% | top3 59.34% | top5 68.27% | 600.0 img/s | lr 1.79e-04\n",
            "[Train] loss 2.8470 | top1 39.26% | top3 59.31% | top5 68.27% | lr 1.77e-04\n",
            "[Val]   loss 1.7616 | top1 53.68% | top3 74.44% | top5 81.66%\n",
            "Best saved to volo_splus_best.pt (val top1 53.68%)\n",
            "Epoch time: 1.45 min\n",
            "\n",
            "=== Epoch 32/50 ===\n",
            "[train step 100/352] loss 2.8421 | top1 39.09% | top3 59.09% | top5 67.88% | 588.4 img/s | lr 1.72e-04\n",
            "[train step 200/352] loss 2.8282 | top1 40.41% | top3 60.28% | top5 69.29% | 597.7 img/s | lr 1.68e-04\n",
            "[train step 300/352] loss 2.8411 | top1 40.20% | top3 60.33% | top5 69.22% | 597.3 img/s | lr 1.63e-04\n",
            "[Train] loss 2.8473 | top1 40.08% | top3 60.33% | top5 69.22% | lr 1.61e-04\n",
            "[Val]   loss 1.7398 | top1 54.66% | top3 74.90% | top5 82.52%\n",
            "Best saved to volo_splus_best.pt (val top1 54.66%)\n",
            "Epoch time: 1.48 min\n",
            "\n",
            "=== Epoch 33/50 ===\n",
            "[train step 100/352] loss 2.7109 | top1 42.53% | top3 62.86% | top5 71.53% | 593.8 img/s | lr 1.56e-04\n",
            "[train step 200/352] loss 2.8006 | top1 41.00% | top3 61.18% | top5 69.82% | 597.4 img/s | lr 1.52e-04\n",
            "[train step 300/352] loss 2.8285 | top1 40.43% | top3 60.59% | top5 69.24% | 596.3 img/s | lr 1.48e-04\n",
            "[Train] loss 2.8326 | top1 40.37% | top3 60.43% | top5 69.12% | lr 1.45e-04\n",
            "[Val]   loss 1.7185 | top1 54.76% | top3 74.80% | top5 82.22%\n",
            "Best saved to volo_splus_best.pt (val top1 54.76%)\n",
            "Epoch time: 1.37 min\n",
            "\n",
            "=== Epoch 34/50 ===\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_loss, test_m = evaluate_one_epoch(\n",
        "    model=model_volo_medium,\n",
        "    dataloader=test_loader,\n",
        "    device=\"cuda\",\n",
        "    autocast_dtype=\"fp16\",\n",
        "    use_amp=True,\n",
        "    label_smoothing=0.0,)\n",
        "\n",
        "print(\"[Test VOLO paper-like] loss\", test_loss, \"|\", test_m)"
      ],
      "metadata": {
        "id": "VDMZKIM0_P83"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}