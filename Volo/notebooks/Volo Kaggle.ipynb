{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU","kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":28376,"sourceType":"datasetVersion","datasetId":22090},{"sourceId":285982,"sourceType":"datasetVersion","datasetId":6057}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nfrom torch.utils.data import DataLoader, random_split\nfrom torchvision import datasets, transforms\nfrom torchvision.transforms import RandAugment\n\ndef get_cifar100_datasets(\n    data_dir: str = \"./data\",\n    val_split: float = 0.0,\n    ra_num_ops: int = 2,\n    ra_magnitude: int = 7,\n    random_erasing_p: float = 0.25,\n    erasing_scale=(0.02, 0.20),\n    erasing_ratio=(0.3, 3.3),\n    img_size: int = 32,):\n\n    \"\"\"\n    CIFAR-100 datasets con augmentations \"mix-friendly\":\n    diseñadas para complementar Mixup/CutMix (en el loop) sin pasarse.\n\n    img_size:\n      - 32 (default): CIFAR nativo.\n      - >32: upsample (p.ej. 64) para experimentos (más tokens/compute).\n    \"\"\"\n    if img_size < 32:\n        raise ValueError(f\"img_size must be >= 32 for CIFAR-100. Got {img_size}.\")\n\n    cifar100_mean = (0.5071, 0.4867, 0.4408)\n    cifar100_std  = (0.2675, 0.2565, 0.2761)\n\n    # Si subimos resolución, primero hacemos resize y adaptamos crop/padding.\n    # Padding recomendado proporcional: 32->4, 64->8, etc.\n\n    crop_padding = max(4, img_size // 8)\n\n    train_ops = []\n    if img_size != 32:\n        train_ops.append(transforms.Resize(img_size, interpolation=transforms.InterpolationMode.BICUBIC))\n\n    train_ops += [\n        transforms.RandomCrop(img_size, padding=crop_padding),\n        transforms.RandomHorizontalFlip(),\n        RandAugment(num_ops=ra_num_ops, magnitude=ra_magnitude),\n        transforms.ToTensor(),\n        transforms.Normalize(cifar100_mean, cifar100_std),\n        transforms.RandomErasing(\n            p=random_erasing_p,\n            scale=erasing_scale,\n            ratio=erasing_ratio,\n            value=\"random\",),]\n\n    train_transform = transforms.Compose(train_ops)\n\n    test_ops = []\n    if img_size != 32:\n        test_ops.append(transforms.Resize(img_size, interpolation=transforms.InterpolationMode.BICUBIC))\n\n    test_ops += [\n        transforms.ToTensor(),\n        transforms.Normalize(cifar100_mean, cifar100_std),]\n\n    test_transform = transforms.Compose(test_ops)\n\n    full_train_dataset = datasets.CIFAR100(\n        root=data_dir, train=True, download=True, transform=train_transform)\n\n    test_dataset = datasets.CIFAR100(\n        root=data_dir, train=False, download=True, transform=test_transform)\n\n    if val_split > 0.0:\n        n_total = len(full_train_dataset)\n        n_val = int(n_total * val_split)\n        n_train = n_total - n_val\n        train_dataset, val_dataset = random_split(\n            full_train_dataset,\n            [n_train, n_val],\n            generator=torch.Generator().manual_seed(7),)\n\n    else:\n        train_dataset = full_train_dataset\n        val_dataset = None\n\n    return train_dataset, val_dataset, test_dataset\n\n\ndef get_cifar100_dataloaders(\n    batch_size: int = 128,\n    data_dir: str = \"./data\",\n    num_workers: int = 2,\n    val_split: float = 0.0,\n    pin_memory: bool = True,\n    ra_num_ops: int = 2,\n    ra_magnitude: int = 7,\n    random_erasing_p: float = 0.25,\n    img_size: int = 32,):\n    \"\"\"\n    Dataloaders CIFAR-100 listos para entrenar con Mixup/CutMix en el loop.\n    Augmentations no tan agresivas.\n\n    img_size:\n      - 32 (default): CIFAR nativo.\n      - 64: experimento de upsample (ojo: más compute).\n    \"\"\"\n    train_ds, val_ds, test_ds = get_cifar100_datasets(\n        data_dir=data_dir,\n        val_split=val_split,\n        ra_num_ops=ra_num_ops,\n        ra_magnitude=ra_magnitude,\n        random_erasing_p=random_erasing_p,\n        img_size=img_size,)\n\n    train_loader = DataLoader(\n        train_ds,\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=num_workers,\n        pin_memory=pin_memory,\n        persistent_workers=(num_workers > 0),)\n\n    val_loader = None\n    if val_ds is not None:\n        val_loader = DataLoader(\n            val_ds,\n            batch_size=batch_size,\n            shuffle=False,\n            num_workers=num_workers,\n            pin_memory=pin_memory,\n            persistent_workers=(num_workers > 0),)\n\n    test_loader = DataLoader(\n        test_ds,\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=num_workers,\n        pin_memory=pin_memory,\n        persistent_workers=(num_workers > 0),)\n\n    return train_loader, val_loader, test_loader","metadata":{"id":"JBBQVf2br3RQ","trusted":true,"execution":{"iopub.status.busy":"2025-12-31T02:54:26.202802Z","iopub.execute_input":"2025-12-31T02:54:26.203125Z","iopub.status.idle":"2025-12-31T02:54:34.854026Z","shell.execute_reply.started":"2025-12-31T02:54:26.203100Z","shell.execute_reply":"2025-12-31T02:54:34.853179Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"def _ddp_is_on():\n    return dist.is_available() and dist.is_initialized()\n\ndef _ddp_rank():\n    return dist.get_rank() if _ddp_is_on() else 0\n\ndef _ddp_barrier():\n    if _ddp_is_on():\n        dist.barrier()\n\ndef get_cifar100_datasets(\n    data_dir: str = \"./data\",\n    val_split: float = 0.0,\n    ra_num_ops: int = 2,\n    ra_magnitude: int = 7,\n    random_erasing_p: float = 0.25,\n    erasing_scale=(0.02, 0.20),\n    erasing_ratio=(0.3, 3.3),\n    img_size: int = 32,\n    seed: int = 7,\n    ddp_safe_download: bool = True):\n    \"\"\"\n    CIFAR-100 datasets con aug 'mix-friendly' y soporte DDP:\n      - Descarga segura: solo rank0 descarga, luego barrier.\n      - Split determinista: train/val indices iguales en todos los ranks.\n      - Val usa test_transform (SIN aug estocásticos).\n    \"\"\"\n    if img_size < 32:\n        raise ValueError(f\"img_size must be >= 32 for CIFAR-100. Got {img_size}.\")\n\n    cifar100_mean = (0.5071, 0.4867, 0.4408)\n    cifar100_std  = (0.2675, 0.2565, 0.2761)\n\n    crop_padding = max(4, img_size // 8)\n\n    train_ops = []\n    if img_size != 32:\n        train_ops.append(transforms.Resize(img_size, interpolation=transforms.InterpolationMode.BICUBIC))\n    train_ops += [\n        transforms.RandomCrop(img_size, padding=crop_padding),\n        transforms.RandomHorizontalFlip(),\n        RandAugment(num_ops=ra_num_ops, magnitude=ra_magnitude),\n        transforms.ToTensor(),\n        transforms.Normalize(cifar100_mean, cifar100_std),\n        transforms.RandomErasing(\n            p=random_erasing_p,\n            scale=erasing_scale,\n            ratio=erasing_ratio,\n            value=\"random\",\n        ),\n    ]\n    train_transform = transforms.Compose(train_ops)\n\n    test_ops = []\n    if img_size != 32:\n        test_ops.append(transforms.Resize(img_size, interpolation=transforms.InterpolationMode.BICUBIC))\n    test_ops += [\n        transforms.ToTensor(),\n        transforms.Normalize(cifar100_mean, cifar100_std),]\n    \n    test_transform = transforms.Compose(test_ops)\n\n    #  DDP-safe download \n    if ddp_safe_download and _ddp_is_on():\n        if _ddp_rank() == 0:\n            datasets.CIFAR100(root=data_dir, train=True, download=True)\n            datasets.CIFAR100(root=data_dir, train=False, download=True)\n        _ddp_barrier()\n        download_flag = False\n    else:\n        download_flag = True\n\n    # Base datasets (dos versiones: train aug y eval clean)\n    full_train_aug = datasets.CIFAR100(root=data_dir, train=True, download=download_flag, transform=train_transform)\n    full_train_eval = datasets.CIFAR100(root=data_dir, train=True, download=False, transform=test_transform)\n    test_dataset = datasets.CIFAR100(root=data_dir, train=False, download=download_flag, transform=test_transform)\n\n    if val_split > 0.0:\n        n_total = len(full_train_aug)\n        n_val = int(n_total * val_split)\n        n_train = n_total - n_val\n\n        g = torch.Generator().manual_seed(seed)\n        perm = torch.randperm(n_total, generator=g).tolist()\n        train_idx = perm[:n_train]\n        val_idx = perm[n_train:]\n\n        train_dataset = Subset(full_train_aug, train_idx)\n        val_dataset = Subset(full_train_eval, val_idx)   \n    else:\n        train_dataset = full_train_aug\n        val_dataset = None\n\n    return train_dataset, val_dataset, test_dataset","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T03:11:07.370621Z","iopub.execute_input":"2025-12-31T03:11:07.370939Z","iopub.status.idle":"2025-12-31T03:11:07.382886Z","shell.execute_reply.started":"2025-12-31T03:11:07.370911Z","shell.execute_reply":"2025-12-31T03:11:07.382205Z"}},"outputs":[],"execution_count":55},{"cell_type":"code","source":"train_loader, val_loader, test_loader = get_cifar100_dataloaders(\n    batch_size=256,\n    data_dir=\"./data/cifar100\",\n    num_workers=2,\n    val_split=0.1,\n    img_size=32)","metadata":{"id":"nq5hFdSnsarL","trusted":true,"execution":{"iopub.status.busy":"2025-12-31T03:04:02.851460Z","iopub.execute_input":"2025-12-31T03:04:02.851756Z","iopub.status.idle":"2025-12-31T03:04:04.605287Z","shell.execute_reply.started":"2025-12-31T03:04:02.851732Z","shell.execute_reply":"2025-12-31T03:04:04.604514Z"}},"outputs":[],"execution_count":46},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass PatchEmbeddingConv(nn.Module):\n    \"\"\"\n    Patch embedding estilo Swin.\n\n    - Conv2d con kernel=stride=patch_size para convertir imagen -> grilla de patches.\n    - Devuelve el mapa 2D en formato canal-al-final: [B, Hp, Wp, D],\n      (más cómodo para window partition).\n    - Opcionalmente devuelve tokens [B, N, D].\n    - Opcional padding automático si H/W no son divisibles por patch_size.\n    \"\"\"\n\n    def __init__(\n        self,\n        patch_size: int | tuple[int, int] = 4,\n        in_chans: int = 3,\n        embed_dim: int = 192,\n        norm_layer: type[nn.Module] | None = nn.LayerNorm,\n        pad_if_needed: bool = True,\n        return_tokens: bool = True):\n\n        super().__init__()\n\n        if isinstance(patch_size, int):\n            patch_size = (patch_size, patch_size)\n\n        self.patch_size = patch_size  # (Ph, Pw)\n        self.in_chans = in_chans\n        self.embed_dim = embed_dim\n        self.pad_if_needed = pad_if_needed\n        self.return_tokens = return_tokens\n\n        # [B, C, H, W] -> [B, D, Hp, Wp]\n        self.proj = nn.Conv2d(\n            in_channels=in_chans,\n            out_channels=embed_dim,\n            kernel_size=patch_size,\n            stride=patch_size,\n            bias=True,)\n\n        # En Swin normalmente LayerNorm sobre la última dimensión\n        self.norm = norm_layer(embed_dim) if norm_layer is not None else None\n\n    def forward(self, x: torch.Tensor):\n        \"\"\"\n        Args:\n            x: [B, C, H, W]\n\n        Returns:\n            x_map:    [B, Hp, Wp, D]\n            (Hp, Wp): tamaño espacial en patches\n            x_tokens (opcional): [B, N, D]\n            pad_hw (opcional): (pad_h, pad_w) aplicados a la imagen\n        \"\"\"\n        B, C, H, W = x.shape\n        Ph, Pw = self.patch_size\n\n        pad_h = (Ph - (H % Ph)) % Ph\n        pad_w = (Pw - (W % Pw)) % Pw\n\n        if (pad_h != 0 or pad_w != 0):\n            if not self.pad_if_needed:\n                raise AssertionError(\n                    f\"Image size ({H}x{W}) no es divisible por patch_size {self.patch_size} \"\n                    f\"y pad_if_needed=False.\")\n\n            x = F.pad(x, (0, pad_w, 0, pad_h))\n\n        # [B, D, Hp, Wp]\n        x = self.proj(x)\n        Hp, Wp = x.shape[2], x.shape[3]\n\n        # canal al final -> [B, Hp, Wp, D]\n        x_map = x.permute(0, 2, 3, 1).contiguous()\n\n        if self.norm is not None:\n            x_map = self.norm(x_map)\n\n        if self.return_tokens:\n            x_tokens = x_map.view(B, Hp * Wp, self.embed_dim)\n            return x_map, (Hp, Wp), x_tokens, (pad_h, pad_w)\n\n        return x_map, (Hp, Wp), (pad_h, pad_w)\n\n","metadata":{"id":"bsqDnkuOs2kr","trusted":true,"execution":{"iopub.status.busy":"2025-12-31T02:54:59.967114Z","iopub.execute_input":"2025-12-31T02:54:59.967925Z","iopub.status.idle":"2025-12-31T02:54:59.977858Z","shell.execute_reply.started":"2025-12-31T02:54:59.967896Z","shell.execute_reply":"2025-12-31T02:54:59.977014Z"}},"outputs":[],"execution_count":6},{"cell_type":"code","source":"def test_patch_embedding_conv():\n    torch.manual_seed(0)\n\n    #  tamaño divisible (64 con patch=4)\n    B, C, H, W = 2, 3, 64, 64\n    x = torch.randn(B, C, H, W)\n\n    pe = PatchEmbeddingConv(\n        patch_size=4,\n        in_chans=3,\n        embed_dim=192,\n        norm_layer=torch.nn.LayerNorm,\n        pad_if_needed=True,\n        return_tokens=True,)\n\n    x_map, (Hp, Wp), x_tok, (pad_h, pad_w) = pe(x)\n\n    assert x_map.shape == (B, Hp, Wp, 192)\n    assert x_tok.shape == (B, Hp * Wp, 192)\n    assert (pad_h, pad_w) == (0, 0)\n    assert (Hp, Wp) == (H // 4, W // 4)\n\n    print(\"[OK] PatchEmbeddingConv divisible:\",\n          \"x_map\", tuple(x_map.shape),\n          \"| x_tok\", tuple(x_tok.shape),\n          \"| pad\", (pad_h, pad_w))\n\n    # tamaño NO divisible (65x63 con patch=4) -> debería paddear\n    H2, W2 = 65, 63\n    x2 = torch.randn(B, C, H2, W2)\n\n    x_map2, (Hp2, Wp2), x_tok2, (pad_h2, pad_w2) = pe(x2)\n\n    assert (H2 + pad_h2) % 4 == 0\n    assert (W2 + pad_w2) % 4 == 0\n    assert x_map2.shape == (B, Hp2, Wp2, 192)\n    assert x_tok2.shape == (B, Hp2 * Wp2, 192)\n\n    print(\"[OK] PatchEmbeddingConv non-divisible:\",\n          \"input\", (H2, W2),\n          \"| padded by\", (pad_h2, pad_w2),\n          \"| patches\", (Hp2, Wp2),\n          \"| x_map\", tuple(x_map2.shape))\n\ntest_patch_embedding_conv()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WwDktYWGxF5U","outputId":"fdfe21dd-4b2e-4d04-d1eb-ee5fe0581be5","trusted":true,"execution":{"iopub.status.busy":"2025-12-31T02:55:03.092968Z","iopub.execute_input":"2025-12-31T02:55:03.093265Z","iopub.status.idle":"2025-12-31T02:55:03.181364Z","shell.execute_reply.started":"2025-12-31T02:55:03.093226Z","shell.execute_reply":"2025-12-31T02:55:03.180689Z"}},"outputs":[{"name":"stdout","text":"[OK] PatchEmbeddingConv divisible: x_map (2, 16, 16, 192) | x_tok (2, 256, 192) | pad (0, 0)\n[OK] PatchEmbeddingConv non-divisible: input (65, 63) | padded by (3, 1) | patches (17, 16) | x_map (2, 17, 16, 192)\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"class OutlookAttention(nn.Module):\n    \"\"\"\n    Outlook Attention (VOLO): agregación local dinámica sobre ventanas.\n\n    Entrada:  x_map [B, H, W, C]  (channel-last)\n    Salida:   y_map [B, H, W, C]\n\n    Parámetros:\n      - dim: canales C\n      - kernel_size: k (vecindario k×k)\n      - stride: s (si s>1 hace downsample tipo \"outlook pooling\"; para CIFAR típicamente s=1)\n      - num_heads: h (partimos canales en cabezas, como MHSA)\n    \"\"\"\n\n    def __init__(\n        self,\n        dim: int,\n        num_heads: int = 6,\n        kernel_size: int = 3,\n        stride: int = 1,\n        attn_drop: float = 0.0,\n        proj_drop: float = 0.0,):\n\n        super().__init__()\n        assert dim % num_heads == 0, \"dim must be divisible by num_heads\"\n\n        self.dim = dim\n        self.num_heads = num_heads\n        self.head_dim = dim // num_heads\n\n        self.kernel_size = kernel_size\n        self.stride = stride\n\n        # Genera pesos de atención por posición: [B, H, W, heads * k*k]\n        self.attn = nn.Linear(dim, num_heads * kernel_size * kernel_size, bias=True)\n\n        # Proyección para values (antes de unfold)\n        self.v = nn.Linear(dim, dim, bias=True)\n\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim, bias=True)\n        self.proj_drop = nn.Dropout(proj_drop)\n\n    def forward(self, x_map: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        x_map: [B, H, W, C]\n        \"\"\"\n        B, H, W, C = x_map.shape\n        k = self.kernel_size\n        s = self.stride\n        heads = self.num_heads\n        hd = self.head_dim\n\n        # attention weights\n        a = self.attn(x_map)\n        # si stride>1, la atención se evalúa en posiciones downsampled\n        if s > 1:\n            # downsample espacialmente (simple avg pool sobre channel-last)\n            a = a.permute(0, 3, 1, 2)                       # [B, heads*k*k, H, W]\n            a = F.avg_pool2d(a, kernel_size=s, stride=s)    # [B, heads*k*k, Hs, Ws]\n            a = a.permute(0, 2, 3, 1).contiguous()          # [B, Hs, Ws, heads*k*k]\n\n        Hs, Ws = a.shape[1], a.shape[2]\n        a = a.view(B, Hs * Ws, heads, k * k)\n        a = F.softmax(a, dim=-1)\n        a = self.attn_drop(a)\n\n        # values map\n        v = self.v(x_map)\n        v = v.permute(0, 3, 1, 2).contiguous()\n\n        # unfold extrae vecindarios k×k para cada posición (con padding para \"same\")\n        pad = k // 2\n        v_unf = F.unfold(v, kernel_size=k, padding=pad, stride=s)\n        v_unf = v_unf.view(B, C, k * k, Hs * Ws).permute(0, 3, 1, 2).contiguous()\n        v_unf = v_unf.view(B, Hs * Ws, heads, hd, k * k)\n\n        # apply attention: weighted sum over neighborhood\n        # a:     [B, Hs*Ws, heads, k*k]\n        # v_unf: [B, Hs*Ws, heads, hd, k*k]\n        y = (v_unf * a.unsqueeze(3)).sum(dim=-1)\n        y = y.reshape(B, Hs * Ws, C)              # concat heads\n\n        # fold back to spatial map\n        y_map = y.view(B, Hs, Ws, C)\n\n        y_map = self.proj(y_map)\n        y_map = self.proj_drop(y_map)\n        return y_map","metadata":{"id":"HOk76_dwwpZq","trusted":true,"execution":{"iopub.status.busy":"2025-12-31T02:55:05.510069Z","iopub.execute_input":"2025-12-31T02:55:05.510621Z","iopub.status.idle":"2025-12-31T02:55:05.520452Z","shell.execute_reply.started":"2025-12-31T02:55:05.510596Z","shell.execute_reply":"2025-12-31T02:55:05.519577Z"}},"outputs":[],"execution_count":8},{"cell_type":"code","source":"def test_outlook_attention_stride1():\n    torch.manual_seed(0)\n\n    B, H, W, C = 2, 16, 16, 192\n    x_map = torch.randn(B, H, W, C, requires_grad=True)\n\n    oa = OutlookAttention(\n        dim=C,\n        num_heads=6,\n        kernel_size=3,\n        stride=1,\n        attn_drop=0.0,\n        proj_drop=0.0)\n\n    y = oa(x_map)\n    assert y.shape == x_map.shape, f\"Expected {x_map.shape}, got {y.shape}\"\n\n    loss = y.mean()\n    loss.backward()\n\n    assert x_map.grad is not None, \"No gradient flowed to input!\"\n    assert torch.isfinite(x_map.grad).all(), \"Non-finite grads!\"\n\n    print(\"[OK] OutlookAttention stride=1:\",\n          \"in\", tuple(x_map.shape),\n          \"| out\", tuple(y.shape),\n          \"| grad mean\", float(x_map.grad.abs().mean()))\n\ntest_outlook_attention_stride1()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pPAUVyTQxM5d","outputId":"40dc70c1-7cfe-46ae-bd5a-906ded533d72","trusted":true,"execution":{"iopub.status.busy":"2025-12-31T02:55:07.416601Z","iopub.execute_input":"2025-12-31T02:55:07.417133Z","iopub.status.idle":"2025-12-31T02:55:07.558339Z","shell.execute_reply.started":"2025-12-31T02:55:07.417107Z","shell.execute_reply":"2025-12-31T02:55:07.557494Z"}},"outputs":[{"name":"stdout","text":"[OK] OutlookAttention stride=1: in (2, 16, 16, 192) | out (2, 16, 16, 192) | grad mean 2.7283142571832286e-06\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"def test_outlook_attention_stride2():\n    torch.manual_seed(0)\n\n    B, H, W, C = 2, 16, 16, 192\n    x_map = torch.randn(B, H, W, C, requires_grad=True)\n\n    oa = OutlookAttention(\n        dim=C,\n        num_heads=6,\n        kernel_size=3,\n        stride=2,\n        attn_drop=0.0,\n        proj_drop=0.0)\n\n    y = oa(x_map)\n\n    assert y.shape[0] == B and y.shape[-1] == C\n    assert y.shape[1] == H // 2 and y.shape[2] == W // 2, f\"Got {y.shape[1:3]}\"\n\n    loss = y.mean()\n    loss.backward()\n    assert x_map.grad is not None\n    assert torch.isfinite(x_map.grad).all()\n\n    print(\"[OK] OutlookAttention stride=2:\",\n          \"in\", (B, H, W, C),\n          \"| out\", tuple(y.shape))\n\ntest_outlook_attention_stride2()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tGRO09XqxQMJ","outputId":"dd5b47dd-f474-4f03-bda5-411b8318baf7","trusted":true,"execution":{"iopub.status.busy":"2025-12-31T02:55:08.968349Z","iopub.execute_input":"2025-12-31T02:55:08.968881Z","iopub.status.idle":"2025-12-31T02:55:08.990113Z","shell.execute_reply.started":"2025-12-31T02:55:08.968857Z","shell.execute_reply":"2025-12-31T02:55:08.989239Z"}},"outputs":[{"name":"stdout","text":"[OK] OutlookAttention stride=2: in (2, 16, 16, 192) | out (2, 8, 8, 192)\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"class DropPath(nn.Module):\n    def __init__(self, drop_prob: float = 0.0):\n        super().__init__()\n        self.drop_prob = float(drop_prob)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        if self.drop_prob == 0.0 or not self.training:\n            return x\n\n        keep_prob = 1.0 - self.drop_prob\n        shape = (x.shape[0],) + (1,) * (x.ndim - 1)\n        random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n        random_tensor.floor_()\n        return x.div(keep_prob) * random_tensor\n\n\nclass MLP(nn.Module):\n    def __init__(self, dim: int, hidden_dim: int, drop: float = 0.0):\n        super().__init__()\n        self.fc1 = nn.Linear(dim, hidden_dim)\n        self.act = nn.GELU()\n        self.fc2 = nn.Linear(hidden_dim, dim)\n        self.drop = nn.Dropout(drop)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.drop(x)\n        x = self.fc2(x)\n        x = self.drop(x)\n        return x\n\nclass OutlookerBlock(nn.Module):\n    \"\"\"\n    Bloque VOLO Outlooker:\n      x -> LN -> OutlookAttention -> DropPath + residual\n        -> LN -> MLP -> DropPath + residual\n\n    Input/Output: [B, H, W, C]\n    \"\"\"\n    def __init__(\n        self,\n        dim: int,\n        num_heads: int,\n        kernel_size: int = 3,\n        stride: int = 1,\n        mlp_ratio: float = 4.0,\n        attn_drop: float = 0.0,\n        proj_drop: float = 0.0,\n        drop_path: float = 0.0,\n        mlp_drop: float = 0.0):\n\n        super().__init__()\n        self.norm1 = nn.LayerNorm(dim)\n\n        self.attn = OutlookAttention(\n            dim=dim,\n            num_heads=num_heads,\n            kernel_size=kernel_size,\n            stride=stride,\n            attn_drop=attn_drop,\n            proj_drop=proj_drop,)\n\n        self.drop_path1 = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n\n        self.norm2 = nn.LayerNorm(dim)\n        hidden_dim = int(dim * mlp_ratio)\n\n        self.mlp = MLP(dim=dim, hidden_dim=hidden_dim, drop=mlp_drop)\n        self.drop_path2 = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n\n    def forward(self, x_map: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        x_map: tensor de forma (B, C, H, W) o (B, N, C), según el bloque.\n        \"\"\"\n\n        # Primer sub-bloque: Norm -> Atención -> DropPath -> Residual\n\n        # Normalización del input\n        x_norm_1 = self.norm1(x_map)\n\n        # Atención\n        attn_out = self.attn(x_norm_1)\n        attn_out = self.drop_path1(attn_out)\n\n        # Suma residual\n        x_map = x_map + attn_out\n\n        # Segundo sub-bloque: Norm -> MLP -> DropPath -> Residual ---\n\n        x_norm_2 = self.norm2(x_map)\n\n        # MLP\n        mlp_out = self.mlp(x_norm_2)\n        mlp_out = self.drop_path2(mlp_out)\n\n        # Segunda suma residual\n        x_out = x_map + mlp_out\n\n        return x_out","metadata":{"id":"eZ5e4lAMxT00","trusted":true,"execution":{"iopub.status.busy":"2025-12-31T02:55:10.186127Z","iopub.execute_input":"2025-12-31T02:55:10.186812Z","iopub.status.idle":"2025-12-31T02:55:10.197233Z","shell.execute_reply.started":"2025-12-31T02:55:10.186788Z","shell.execute_reply":"2025-12-31T02:55:10.196529Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"def test_outlooker_block():\n    torch.manual_seed(0)\n\n    B, H, W, C = 2, 16, 16, 192\n    x_map = torch.randn(B, H, W, C, requires_grad=True)\n\n    blk = OutlookerBlock(\n        dim=C,\n        num_heads=6,\n        kernel_size=3,\n        stride=1,\n        mlp_ratio=4.0,\n        attn_drop=0.0,\n        proj_drop=0.0,\n        drop_path=0.0,\n        mlp_drop=0.0,)\n\n    y = blk(x_map)\n    assert y.shape == x_map.shape\n\n    y.mean().backward()\n    assert x_map.grad is not None\n    assert torch.isfinite(x_map.grad).all()\n\n    print(\"[OK] OutlookerBlock:\",\n          \"in/out\", tuple(y.shape),\n          \"| grad mean\", float(x_map.grad.abs().mean()))\n\ntest_outlooker_block()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ncbCj-egxW5v","outputId":"f9922edc-927c-40f7-86d6-bc8cf0dd8ec8","trusted":true,"execution":{"iopub.status.busy":"2025-12-31T02:55:12.329081Z","iopub.execute_input":"2025-12-31T02:55:12.329385Z","iopub.status.idle":"2025-12-31T02:55:12.378994Z","shell.execute_reply.started":"2025-12-31T02:55:12.329362Z","shell.execute_reply":"2025-12-31T02:55:12.378236Z"}},"outputs":[{"name":"stdout","text":"[OK] OutlookerBlock: in/out (2, 16, 16, 192) | grad mean 1.0187762200075667e-05\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"def test_embed_then_outlook(img_size=64, patch_size=4, dim=192, heads=6):\n    torch.manual_seed(0)\n\n    B = 2\n    x = torch.randn(B, 3, img_size, img_size, requires_grad=True)\n\n    pe = PatchEmbeddingConv(\n        patch_size=patch_size,\n        in_chans=3,\n        embed_dim=dim,\n        norm_layer=torch.nn.LayerNorm,\n        pad_if_needed=True,\n        return_tokens=True,)\n\n    blk = OutlookerBlock(\n        dim=dim,\n        num_heads=heads,\n        kernel_size=3,\n        stride=1,\n        mlp_ratio=4.0,\n        drop_path=0.0,)\n\n    x_map, (Hp, Wp), x_tok, pad_hw = pe(x)\n    y_map = blk(x_map)\n\n    assert y_map.shape == x_map.shape == (B, Hp, Wp, dim)\n\n    # grad\n    y_map.mean().backward()\n    assert x.grad is not None and torch.isfinite(x.grad).all()\n\n    print(\"[OK] Embed->Outlook:\",\n          \"img\", (img_size, img_size),\n          \"| patches\", (Hp, Wp),\n          \"| map\", tuple(y_map.shape),\n          \"| pad\", pad_hw)\n\ntest_embed_then_outlook(img_size=32)\ntest_embed_then_outlook(img_size=64)","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IcrvTt48x2gj","outputId":"12405db1-f6e4-4409-83d7-c9365bf00583","trusted":true,"execution":{"iopub.status.busy":"2025-12-31T02:55:14.609890Z","iopub.execute_input":"2025-12-31T02:55:14.610596Z","iopub.status.idle":"2025-12-31T02:55:14.682657Z","shell.execute_reply.started":"2025-12-31T02:55:14.610570Z","shell.execute_reply":"2025-12-31T02:55:14.681830Z"}},"outputs":[{"name":"stdout","text":"[OK] Embed->Outlook: img (32, 32) | patches (8, 8) | map (2, 8, 8, 192) | pad (0, 0)\n[OK] Embed->Outlook: img (64, 64) | patches (16, 16) | map (2, 16, 16, 192) | pad (0, 0)\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"class VOLOStage(nn.Module):\n    \"\"\"\n    Un stage VOLO basado en OutlookerBlocks.\n\n    Mantiene el formato channel-last:\n      Input:  [B, H, W, C]\n      Output: [B, H, W, C]  (si stride=1)\n    Si quisieras un stage que haga downsample, usa stride>1 en los bloques\n    (pero en CIFAR te recomiendo stride=1 en el stage inicial).\n    \"\"\"\n\n    def __init__(\n        self,\n        dim: int,\n        depth: int,\n        num_heads: int,\n        kernel_size: int = 3,\n        stride: int = 1,\n        mlp_ratio: float = 4.0,\n        attn_drop: float = 0.0,\n        proj_drop: float = 0.0,\n        drop_path: float | list[float] = 0.0,\n        mlp_drop: float = 0.0,):\n\n        super().__init__()\n\n        if isinstance(drop_path, float):\n            dpr = [drop_path] * depth\n        else:\n            assert len(drop_path) == depth, \"drop_path list must have length=depth\"\n            dpr = drop_path\n\n        self.blocks = nn.ModuleList([\n            OutlookerBlock(\n                dim=dim,\n                num_heads=num_heads,\n                kernel_size=kernel_size,\n                stride=stride,\n                mlp_ratio=mlp_ratio,\n                attn_drop=attn_drop,\n                proj_drop=proj_drop,\n                drop_path=dpr[i],\n                mlp_drop=mlp_drop,) for i in range(depth)])\n\n    def forward(self, x_map: torch.Tensor) -> torch.Tensor:\n        for blk in self.blocks:\n            x_map = blk(x_map)\n        return x_map","metadata":{"id":"whzUQ5eEx_Mo","trusted":true,"execution":{"iopub.status.busy":"2025-12-31T02:55:16.084387Z","iopub.execute_input":"2025-12-31T02:55:16.084866Z","iopub.status.idle":"2025-12-31T02:55:16.091229Z","shell.execute_reply.started":"2025-12-31T02:55:16.084841Z","shell.execute_reply":"2025-12-31T02:55:16.090291Z"}},"outputs":[],"execution_count":14},{"cell_type":"code","source":"def test_volo_stage():\n    torch.manual_seed(0)\n\n    B, H, W, C = 2, 16, 16, 192\n    x = torch.randn(B, H, W, C, requires_grad=True)\n\n    stage = VOLOStage(\n        dim=C,\n        depth=3,\n        num_heads=6,\n        kernel_size=3,\n        stride=1,\n        drop_path=[0.0, 0.05, 0.1])\n\n    y = stage(x)\n    assert y.shape == x.shape\n    y.mean().backward()\n    assert x.grad is not None and torch.isfinite(x.grad).all()\n\n    print(\"[OK] VOLOStage:\", tuple(y.shape), \"| grad mean\", float(x.grad.abs().mean()))\n\ntest_volo_stage()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7iSxXS3XyIwb","outputId":"61d8fd2c-3449-4a46-e77f-1f480b4d9f8c","trusted":true,"execution":{"iopub.status.busy":"2025-12-31T02:55:17.732413Z","iopub.execute_input":"2025-12-31T02:55:17.732954Z","iopub.status.idle":"2025-12-31T02:55:17.811773Z","shell.execute_reply.started":"2025-12-31T02:55:17.732928Z","shell.execute_reply":"2025-12-31T02:55:17.810904Z"}},"outputs":[{"name":"stdout","text":"[OK] VOLOStage: (2, 16, 16, 192) | grad mean 1.0873188330151606e-05\n","output_type":"stream"}],"execution_count":15},{"cell_type":"markdown","source":"## Attention","metadata":{"id":"r7w_cBO4yzYY"}},{"cell_type":"code","source":"def scaled_dot_product_attention(q, k, v, mask=None, attn_dropout_p: float = 0.0, training: bool = True):\n    \"\"\"\n    q: (B, H, Lq, d)\n    k: (B, H, Lk, d)\n    v: (B, H, Lk, d)\n    mask: broadcastable a (B, H, Lq, Lk)\n          - bool: True = BLOQUEAR (poner -inf)\n          - float: 1.0 = permitir, 0.0 = bloquear\n    \"\"\"\n    scores = torch.matmul(q, k.transpose(-2, -1))\n    dk = q.size(-1)\n    scores = scores / (dk ** 0.5)\n\n    if mask is not None:\n        if mask.dtype == torch.bool:\n            scores = scores.masked_fill(mask, float(\"-inf\"))\n        else:\n            scores = scores.masked_fill(mask <= 0, float(\"-inf\"))\n\n    attn = F.softmax(scores, dim=-1)\n    if attn_dropout_p > 0.0:\n        attn = F.dropout(attn, p=attn_dropout_p, training=training)\n\n    output = torch.matmul(attn, v)\n    return output, attn","metadata":{"id":"LY-tnHuuyvbW","trusted":true,"execution":{"iopub.status.busy":"2025-12-31T02:55:19.217969Z","iopub.execute_input":"2025-12-31T02:55:19.218526Z","iopub.status.idle":"2025-12-31T02:55:19.223487Z","shell.execute_reply.started":"2025-12-31T02:55:19.218501Z","shell.execute_reply":"2025-12-31T02:55:19.222835Z"}},"outputs":[],"execution_count":16},{"cell_type":"code","source":"class MultiHeadAttention(nn.Module):\n    def __init__(self, d_model: int, num_heads: int, dropout: float = 0.1):\n        super().__init__()\n        assert d_model % num_heads == 0, \"d_model debe ser múltiplo de num_heads\"\n\n        self.num_heads = num_heads\n        self.d_head = d_model // num_heads\n        self.d_model = d_model\n\n        self.w_q = nn.Linear(d_model, d_model)\n        self.w_k = nn.Linear(d_model, d_model)\n        self.w_v = nn.Linear(d_model, d_model)\n        self.w_o = nn.Linear(d_model, d_model)\n\n        # \"dropout\" lo usaremos como dropout de atención (sobre attn)\n        self.attn_dropout_p = dropout\n        # y también dejamos dropout de salida si quieres (común en ViT)\n        self.out_dropout = nn.Dropout(dropout)\n\n    def _split_heads(self, x):\n        B, L, _ = x.shape\n        return x.view(B, L, self.num_heads, self.d_head).transpose(1, 2)\n\n    def _combine_heads(self, x):\n        B, H, L, D = x.shape\n        return x.transpose(1, 2).contiguous().view(B, L, H * D)\n\n    def forward(self, x_q, x_kv, mask=None):\n        q = self._split_heads(self.w_q(x_q))\n        k = self._split_heads(self.w_k(x_kv))\n        v = self._split_heads(self.w_v(x_kv))\n\n        if mask is not None:\n            if mask.dim() == 2:\n                mask = mask[:, None, None, :]\n            elif mask.dim() == 3:\n                mask = mask[:, None, :, :]\n            elif mask.dim() == 4:\n                pass\n            else:\n                raise ValueError(f\"Máscara con dims no soportadas: {mask.shape}\")\n\n            if mask.dtype != torch.bool:\n                mask = (mask <= 0)\n\n        attn_out, _ = scaled_dot_product_attention(\n            q, k, v,\n            mask=mask,\n            attn_dropout_p=self.attn_dropout_p,\n            training=self.training)\n\n        attn_out = self._combine_heads(attn_out)\n\n        attn_out = self.w_o(attn_out)\n        attn_out = self.out_dropout(attn_out)\n        return attn_out","metadata":{"id":"Qip2P-Jby1h7","trusted":true,"execution":{"iopub.status.busy":"2025-12-31T02:55:20.729569Z","iopub.execute_input":"2025-12-31T02:55:20.730134Z","iopub.status.idle":"2025-12-31T02:55:20.738123Z","shell.execute_reply.started":"2025-12-31T02:55:20.730107Z","shell.execute_reply":"2025-12-31T02:55:20.737299Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"class FeedForward(nn.Module):\n    def __init__(self, dim: int, hidden_dim: int, dropout: float = 0.1):\n        super().__init__()\n        self.fc1 = nn.Linear(dim, hidden_dim)\n        self.act = nn.GELU()\n        self.fc2 = nn.Linear(hidden_dim, dim)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.dropout(x)\n        x = self.fc2(x)\n        x = self.dropout(x)\n        return x\n\n\nclass TransformerBlock(nn.Module):\n    \"\"\"\n    Bloque encoder para ViT (pre-norm):\n    x -> LN -> MHA -> DropPath -> +residual\n       -> LN -> MLP -> DropPath -> +residual\n    \"\"\"\n    def __init__(\n        self,\n        dim: int,\n        num_heads: int,\n        mlp_ratio: float = 4.0,\n        attn_dropout: float = 0.0,\n        dropout: float = 0.1,\n        drop_path: float = 0.0):\n\n        super().__init__()\n        self.norm1 = nn.LayerNorm(dim)\n        self.attn = MultiHeadAttention(d_model=dim, num_heads=num_heads, dropout=attn_dropout)\n        self.drop_path1 = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n\n        self.norm2 = nn.LayerNorm(dim)\n        hidden_dim = int(dim * mlp_ratio)\n        self.mlp = FeedForward(dim, hidden_dim, dropout=dropout)\n        self.drop_path2 = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n\n    def forward(self, x):\n        x = x + self.drop_path1(self.attn(self.norm1(x), self.norm1(x), mask=None))\n        x = x + self.drop_path2(self.mlp(self.norm2(x)))\n        return x\n","metadata":{"id":"EQqNseVTy7Lp","trusted":true,"execution":{"iopub.status.busy":"2025-12-31T02:55:22.634114Z","iopub.execute_input":"2025-12-31T02:55:22.634646Z","iopub.status.idle":"2025-12-31T02:55:22.641956Z","shell.execute_reply.started":"2025-12-31T02:55:22.634622Z","shell.execute_reply":"2025-12-31T02:55:22.641143Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"class TransformerStack(nn.Module):\n    \"\"\"Stack simple de TransformerBlock sobre tokens [B, N, C].\"\"\"\n    def __init__(self, dim: int, depth: int, num_heads: int, mlp_ratio=4.0,\n                 attn_dropout=0.0, dropout=0.1, drop_path: float | list[float] = 0.0):\n        super().__init__()\n        if isinstance(drop_path, float):\n            dpr = [drop_path] * depth\n        else:\n            assert len(drop_path) == depth\n            dpr = drop_path\n\n        self.blocks = nn.ModuleList([\n            TransformerBlock(\n                dim=dim,\n                num_heads=num_heads,\n                mlp_ratio=mlp_ratio,\n                attn_dropout=attn_dropout,\n                dropout=dropout,\n                drop_path=dpr[i] if \"drop_path\" in TransformerBlock.__init__.__code__.co_varnames else 0.0) for i in range(depth)])\n\n    def forward(self, x_tok: torch.Tensor) -> torch.Tensor:\n        for blk in self.blocks:\n            x_tok = blk(x_tok)\n        return x_tok","metadata":{"id":"qDcU6lW4zkjU","trusted":true,"execution":{"iopub.status.busy":"2025-12-31T02:55:24.397093Z","iopub.execute_input":"2025-12-31T02:55:24.397393Z","iopub.status.idle":"2025-12-31T02:55:24.403206Z","shell.execute_reply.started":"2025-12-31T02:55:24.397370Z","shell.execute_reply":"2025-12-31T02:55:24.402536Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"def test_transformer_block():\n    torch.manual_seed(0)\n    B, N, C = 2, 256, 192\n    x = torch.randn(B, N, C, requires_grad=True)\n\n    blk = TransformerBlock(dim=C, num_heads=6, mlp_ratio=4.0, attn_dropout=0.0, dropout=0.1, drop_path=0.0)\n    y = blk(x)\n    assert y.shape == x.shape\n    y.mean().backward()\n    assert x.grad is not None and torch.isfinite(x.grad).all()\n    print(\"[OK] TransformerBlock:\", tuple(y.shape), \"grad\", float(x.grad.abs().mean()))\n\ntest_transformer_block()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IODXdVf0y81n","outputId":"19eeb4eb-042f-4d95-e796-52220afa2d51","trusted":true,"execution":{"iopub.status.busy":"2025-12-31T02:55:26.805077Z","iopub.execute_input":"2025-12-31T02:55:26.805594Z","iopub.status.idle":"2025-12-31T02:55:26.850521Z","shell.execute_reply.started":"2025-12-31T02:55:26.805569Z","shell.execute_reply":"2025-12-31T02:55:26.849744Z"}},"outputs":[{"name":"stdout","text":"[OK] TransformerBlock: (2, 256, 192) grad 1.018048442347208e-05\n","output_type":"stream"}],"execution_count":20},{"cell_type":"markdown","source":"# Hiratical","metadata":{"id":"gmtkfE3VzVmw"}},{"cell_type":"code","source":"class MapDownsample(nn.Module):\n    \"\"\"\n    Downsample para mapas channel-last: [B, H, W, C_in] -> [B, H/2, W/2, C_out]\n    usando conv2d stride=2 en formato channel-first internamente.\n    \"\"\"\n    def __init__(self, dim_in: int, dim_out: int, kernel_size: int = 3, norm_layer=nn.LayerNorm):\n        super().__init__()\n        pad = kernel_size // 2\n        self.conv = nn.Conv2d(dim_in, dim_out, kernel_size=kernel_size, stride=2, padding=pad, bias=True)\n        self.norm = norm_layer(dim_out) if norm_layer is not None else None\n\n    def forward(self, x_map: torch.Tensor):\n        # x_map: [B, H, W, C_in]\n        B, H, W, C = x_map.shape\n        x = x_map.permute(0, 3, 1, 2).contiguous()     # [B, C, H, W]\n        x = self.conv(x)                               # [B, C_out, H2, W2]\n        x_map = x.permute(0, 2, 3, 1).contiguous()     # [B, H2, W2, C_out]\n        if self.norm is not None:\n            x_map = self.norm(x_map)\n        return x_map","metadata":{"id":"LIn3ZrvLzXgV","trusted":true,"execution":{"iopub.status.busy":"2025-12-31T02:55:28.311096Z","iopub.execute_input":"2025-12-31T02:55:28.311609Z","iopub.status.idle":"2025-12-31T02:55:28.317006Z","shell.execute_reply.started":"2025-12-31T02:55:28.311583Z","shell.execute_reply":"2025-12-31T02:55:28.316211Z"}},"outputs":[],"execution_count":21},{"cell_type":"code","source":"class PoolingLayer(nn.Module):\n    \"\"\"\n    Pooling jerárquico para ViT:\n\n    - Toma tokens [B, N, D_in] + grid_size (H, W)\n    - Los reinterpreta como feature map [B, D_in, H, W]\n    - Aplica:\n        depthwise conv (3x3, stride=2, padding=1)\n        pointwise conv (1x1) para cambiar D_in -> D_out\n    - Devuelve:\n        tokens [B, N_out, D_out] y nuevo grid_size (H_out, W_out)\n    \"\"\"\n\n    def __init__(self,\n        dim_in: int,\n        dim_out: int,\n        kernel_size: int = 3,\n        stride: int = 2,\n        norm_layer: type[nn.Module] | None = nn.LayerNorm):\n\n        super().__init__()\n        padding = kernel_size // 2\n\n        # Depthwise conv: cada canal se filtra por separado\n        self.depthwise_conv = nn.Conv2d(\n            in_channels=dim_in,\n            out_channels=dim_in,\n            kernel_size=kernel_size,\n            stride=stride,\n            padding=padding,\n            groups=dim_in)\n\n        # Pointwise conv: mezcla canales y cambia dim\n        self.pointwise_conv = nn.Conv2d(\n            in_channels=dim_in,\n            out_channels=dim_out,\n            kernel_size=1,\n            stride=1,\n            padding=0)\n\n        self.norm = norm_layer(dim_out) if norm_layer is not None else None\n\n        self.dim_in = dim_in\n        self.dim_out = dim_out\n        self.stride = stride\n\n    def forward(self, x: torch.Tensor, grid_size: tuple[int, int]):\n        \"\"\"\n        Args:\n            x: tokens [B, N, D_in]\n            grid_size: (H, W) tal que H*W = N\n\n        Returns:\n            x_out: tokens [B, N_out, D_out]\n            new_grid: (H_out, W_out)\n        \"\"\"\n        B, N, D_in = x.shape\n        H, W = grid_size\n\n        assert D_in == self.dim_in, f\"dim_in {D_in} != {self.dim_in}\"\n        assert H * W == N, f\"H*W={H*W} no coincide con N={N}\"\n\n        # [B, N, D_in] -> [B, D_in, H, W]\n        x = x.view(B, H, W, D_in).permute(0, 3, 1, 2)\n\n        # Depthwise + pointwise\n        x = self.depthwise_conv(x)\n        x = self.pointwise_conv(x)\n\n        B, D_out, H_out, W_out = x.shape\n        N_out = H_out * W_out\n\n        # Volver a tokens: [B, D_out, H_out, W_out] -> [B, N_out, D_out]\n        x = x.flatten(2).transpose(1, 2)\n\n        if self.norm is not None:\n            x = self.norm(x)\n\n        new_grid = (H_out, W_out)\n        return x, new_grid","metadata":{"id":"-m016eqBzwAL","trusted":true,"execution":{"iopub.status.busy":"2025-12-31T02:55:29.826463Z","iopub.execute_input":"2025-12-31T02:55:29.827132Z","iopub.status.idle":"2025-12-31T02:55:29.834314Z","shell.execute_reply.started":"2025-12-31T02:55:29.827104Z","shell.execute_reply":"2025-12-31T02:55:29.833720Z"}},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":"# VOLO BackBone","metadata":{"id":"qqlP5kIAzm8a"}},{"cell_type":"code","source":"def map_to_tokens(x_map: torch.Tensor) -> torch.Tensor:\n    B, H, W, C = x_map.shape\n    return x_map.view(B, H * W, C)\n\ndef tokens_to_map(x_tok: torch.Tensor, H: int, W: int) -> torch.Tensor:\n    B, N, C = x_tok.shape\n    assert N == H * W\n    return x_tok.view(B, H, W, C)\n\nclass VOLOPyramid(nn.Module):\n    \"\"\"\n    Backbone jerárquico para VOLO (sin classifier head aún).\n    - Local: VOLOStage (Outlooker)\n    - Global: TransformerStack (opcional)\n    - Downsample: map-space (recomendado) o token-space (PoolingLayer tuyo)\n    \"\"\"\n    def __init__(\n        self,\n        dims: tuple[int, ...],                 # ej (192, 256, 384)\n        outlooker_depths: tuple[int, ...],     # ej (4, 2, 0)  (0 si no hay outlooker en ese nivel)\n        outlooker_heads: tuple[int, ...],      # ej (6, 8, 12)\n        transformer_depths: tuple[int, ...],   # ej (0, 4, 6)\n        transformer_heads: tuple[int, ...],    # ej (6, 8, 12)\n        kernel_size: int = 3,\n        mlp_ratio: float = 4.0,\n        downsample_kind: str = \"map\",          # \"map\" o \"token\"\n        drop_path_rate: float = 0.0):\n\n        super().__init__()\n        L = len(dims)\n\n        assert len(outlooker_depths) == L\n        assert len(outlooker_heads) == L\n        assert len(transformer_depths) == L\n        assert len(transformer_heads) == L\n\n        # schedule lineal de droppath a través de todos los bloques (local+global)\n        total_blocks = sum(outlooker_depths) + sum(transformer_depths)\n        dpr = torch.linspace(0, drop_path_rate, total_blocks).tolist() if total_blocks > 0 else []\n        dp_i = 0\n\n        self.levels = nn.ModuleList()\n        self.downsamples = nn.ModuleList()\n        self.downsample_kind = downsample_kind\n\n        for i in range(L):\n            dim = dims[i]\n\n            # Local stage (Outlooker)\n            local = None\n            if outlooker_depths[i] > 0:\n                local_dpr = dpr[dp_i: dp_i + outlooker_depths[i]]\n                dp_i += outlooker_depths[i]\n                local = VOLOStage(\n                    dim=dim,\n                    depth=outlooker_depths[i],\n                    num_heads=outlooker_heads[i],\n                    kernel_size=kernel_size,\n                    stride=1,\n                    mlp_ratio=mlp_ratio,\n                    drop_path=local_dpr)\n\n            # Global stage (Transformer)\n            global_ = None\n            if transformer_depths[i] > 0:\n                glob_dpr = dpr[dp_i: dp_i + transformer_depths[i]]\n                dp_i += transformer_depths[i]\n\n                global_ = TransformerStack(\n                    dim=dim,\n                    depth=transformer_depths[i],\n                    num_heads=transformer_heads[i],\n                    mlp_ratio=mlp_ratio,\n                    attn_dropout=0.0,\n                    dropout=0.1,\n                    drop_path=glob_dpr,)\n\n            self.levels.append(nn.ModuleDict({\"local\": local, \"global\": global_}))\n\n            # Downsample para pasar dim_i -> dim_{i+1} (si no es el último nivel)\n            if i < L - 1:\n                if downsample_kind == \"map\":\n                    self.downsamples.append(MapDownsample(dim_in=dim, dim_out=dims[i + 1], kernel_size=3))\n                elif downsample_kind == \"token\":\n                    # reusar PoolingLayer\n                    self.downsamples.append(PoolingLayer(dim_in=dim, dim_out=dims[i + 1], kernel_size=3, stride=2))\n                else:\n                    raise ValueError(f\"downsample_kind must be 'map' or 'token'. Got {downsample_kind}\")\n\n        assert dp_i == total_blocks\n\n    def forward(self, x_map: torch.Tensor):\n        \"\"\"\n        x_map: [B, H, W, C0]\n        returns:\n          x_final_tokens: [B, N_last, C_last]\n          last_grid: (H_last, W_last)\n        \"\"\"\n        B, H, W, C = x_map.shape\n\n        for i, lvl in enumerate(self.levels):\n            # local stage en map\n            if lvl[\"local\"] is not None:\n                x_map = lvl[\"local\"](x_map)\n\n            # global stage en tokens (si existe)\n            if lvl[\"global\"] is not None:\n                x_tok = map_to_tokens(x_map)\n                x_tok = lvl[\"global\"](x_tok)\n                x_map = tokens_to_map(x_tok, H, W)\n\n            # downsample (si aplica)\n            if i < len(self.downsamples):\n                ds = self.downsamples[i]\n                if self.downsample_kind == \"map\":\n                    x_map = ds(x_map)\n                    H, W = x_map.shape[1], x_map.shape[2]\n                else:\n                    # token downsample: necesita grid\n                    x_tok = map_to_tokens(x_map)\n                    x_tok, (H, W) = ds(x_tok, (H, W))\n                    x_map = tokens_to_map(x_tok, H, W)\n\n        x_final = map_to_tokens(x_map)\n        return x_final, (H, W)","metadata":{"id":"t_EUv_hqzfvv","trusted":true,"execution":{"iopub.status.busy":"2025-12-31T02:55:34.116296Z","iopub.execute_input":"2025-12-31T02:55:34.117007Z","iopub.status.idle":"2025-12-31T02:55:34.129615Z","shell.execute_reply.started":"2025-12-31T02:55:34.116982Z","shell.execute_reply":"2025-12-31T02:55:34.128776Z"}},"outputs":[],"execution_count":24},{"cell_type":"code","source":"def test_volo_pyramid_map():\n    torch.manual_seed(0)\n    B = 2\n    H = W = 16\n    x_map = torch.randn(B, H, W, 192)\n\n    pyr = VOLOPyramid(\n        dims=(192, 256, 384),\n        outlooker_depths=(2, 2, 0),\n        outlooker_heads=(6, 8, 12),\n        transformer_depths=(0, 2, 2),\n        transformer_heads=(6, 8, 12),\n        downsample_kind=\"map\",\n        drop_path_rate=0.1,)\n\n    x_tok, (Hf, Wf) = pyr(x_map)\n    print(\"[OK] Pyramid-map:\", x_tok.shape, \"grid\", (Hf, Wf))\n    assert x_tok.shape[0] == B\n    assert x_tok.shape[2] == 384\n    assert Hf * Wf == x_tok.shape[1]\n\ntest_volo_pyramid_map()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"fO9L_Fabz4SY","outputId":"6870ea2e-35e6-480f-c7b0-fc404da1363d","trusted":true,"execution":{"iopub.status.busy":"2025-12-31T02:55:36.533756Z","iopub.execute_input":"2025-12-31T02:55:36.534602Z","iopub.status.idle":"2025-12-31T02:55:36.647655Z","shell.execute_reply.started":"2025-12-31T02:55:36.534577Z","shell.execute_reply":"2025-12-31T02:55:36.646964Z"}},"outputs":[{"name":"stdout","text":"[OK] Pyramid-map: torch.Size([2, 16, 384]) grid (4, 4)\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"def test_volo_pyramid_token():\n    torch.manual_seed(0)\n    B = 2\n    H = W = 16\n    x_map = torch.randn(B, H, W, 192)\n\n    pyr = VOLOPyramid(\n        dims=(192, 256, 384),\n        outlooker_depths=(2, 2, 0),\n        outlooker_heads=(6, 8, 12),\n        transformer_depths=(0, 2, 2),\n        transformer_heads=(6, 8, 12),\n        downsample_kind=\"token\",\n        drop_path_rate=0.1)\n\n    x_tok, (Hf, Wf) = pyr(x_map)\n    print(\"[OK] Pyramid-token:\", x_tok.shape, \"grid\", (Hf, Wf))\n    assert x_tok.shape[2] == 384\n    assert Hf * Wf == x_tok.shape[1]\n\ntest_volo_pyramid_token()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Nr-zTpnMz73Y","outputId":"3a419435-ee86-468d-c336-06e3a80ca0bf","trusted":true,"execution":{"iopub.status.busy":"2025-12-31T02:55:38.111351Z","iopub.execute_input":"2025-12-31T02:55:38.112021Z","iopub.status.idle":"2025-12-31T02:55:38.215297Z","shell.execute_reply.started":"2025-12-31T02:55:38.111989Z","shell.execute_reply":"2025-12-31T02:55:38.214559Z"}},"outputs":[{"name":"stdout","text":"[OK] Pyramid-token: torch.Size([2, 16, 384]) grid (4, 4)\n","output_type":"stream"}],"execution_count":26},{"cell_type":"code","source":"class ClassAttention(nn.Module):\n    \"\"\"\n    Class Attention: sólo el CLS atiende al conjunto [CLS | tokens].\n    Inputs:\n      cls:    [B, 1, C]\n      tokens: [B, N, C]\n    Output:\n      cls_out: [B, 1, C]\n    \"\"\"\n    def __init__(self, dim: int, num_heads: int, attn_dropout: float = 0.0, proj_dropout: float = 0.0):\n        super().__init__()\n        self.attn = MultiHeadAttention(d_model=dim, num_heads=num_heads, dropout=attn_dropout)\n        self.proj_drop = nn.Dropout(proj_dropout)\n\n    def forward(self, cls: torch.Tensor, tokens: torch.Tensor) -> torch.Tensor:\n        kv = torch.cat([cls, tokens], dim=1)        # [B, 1+N, C]\n        cls_out = self.attn(cls, kv, mask=None) # [B, 1, C] (solo CLS sale actualizado)\n        return self.proj_drop(cls_out)\n\n\nclass ClassAttentionBlock(nn.Module):\n    \"\"\"\n    Pre-norm (CaiT-style):\n      cls -> LN -> ClassAttn(cls, [cls|tokens]) -> +res\n          -> LN -> MLP -> +res\n    Nota: tokens NO se actualizan.\n    \"\"\"\n    def __init__(\n        self,\n        dim: int,\n        num_heads: int,\n        mlp_ratio: float = 4.0,\n        attn_dropout: float = 0.0,\n        dropout: float = 0.0):\n\n        super().__init__()\n        self.norm_cls = nn.LayerNorm(dim)\n        self.norm_tok = nn.LayerNorm(dim)\n        self.ca = ClassAttention(dim, num_heads, attn_dropout=attn_dropout, proj_dropout=dropout)\n\n        self.norm2 = nn.LayerNorm(dim)\n        self.mlp = FeedForward(dim, int(dim * mlp_ratio), dropout=dropout)\n\n    def forward(self, cls: torch.Tensor, tokens: torch.Tensor) -> torch.Tensor:\n        # Class attention update (solo CLS)\n        cls_norm = self.norm_cls(cls)\n        tok_norm = self.norm_tok(tokens)\n        cls = cls + self.ca(cls_norm, tok_norm)\n\n        # MLP update (solo CLS)\n        cls = cls + self.mlp(self.norm2(cls))\n        return cls","metadata":{"id":"Nzbf97Ut9lSQ","trusted":true,"execution":{"iopub.status.busy":"2025-12-31T02:58:33.052973Z","iopub.execute_input":"2025-12-31T02:58:33.053322Z","iopub.status.idle":"2025-12-31T02:58:33.062289Z","shell.execute_reply.started":"2025-12-31T02:58:33.053280Z","shell.execute_reply":"2025-12-31T02:58:33.061592Z"}},"outputs":[],"execution_count":44},{"cell_type":"code","source":"class CLIPool(nn.Module):\n    \"\"\"\n    \"CLI\" style pooling: mezcla aprendible entre CLS y mean(tokens).\n      z = alpha * cls + (1-alpha) * mean\n    \"\"\"\n    def __init__(self, init_alpha: float = 0.5):\n        super().__init__()\n        # parametriza alpha en logits para mantenerlo en (0,1)\n        init_alpha = float(init_alpha)\n        init_alpha = min(max(init_alpha, 1e-4), 1 - 1e-4)\n        logit = math.log(init_alpha / (1 - init_alpha))\n        self.alpha_logit = nn.Parameter(torch.tensor([logit], dtype=torch.float32))\n\n    def forward(self, cls_vec: torch.Tensor, tok_mean: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        cls_vec:  [B, C]\n        tok_mean: [B, C]\n        \"\"\"\n        alpha = torch.sigmoid(self.alpha_logit)  # scalar in (0,1)\n        return alpha * cls_vec + (1.0 - alpha) * tok_mean","metadata":{"id":"w-uPdjcF9n5L"},"outputs":[],"execution_count":66},{"cell_type":"markdown","source":"# VOLO","metadata":{"id":"jg1a84bu0E2D"}},{"cell_type":"code","source":"import math\n\ndef trunc_normal_(tensor, mean=0., std=1.):\n    with torch.no_grad():\n        return tensor.normal_(mean=mean, std=std)\n\nclass PosEmbed2D(nn.Module):\n    \"\"\"\n    Positional embedding aprendible para grilla (H, W) en tokens.\n\n    Guarda [1, H*W, C]. Si en forward llega otro (H,W), interpola.\n    \"\"\"\n    def __init__(self, H: int, W: int, dim: int):\n        super().__init__()\n        self.H0 = H\n        self.W0 = W\n        self.dim = dim\n        self.pos = nn.Parameter(torch.zeros(1, H * W, dim))\n        trunc_normal_(self.pos, std=0.02)\n\n    def forward(self, x_tok: torch.Tensor, grid: tuple[int, int]):\n        \"\"\"\n        x_tok: [B, N, C]\n        grid: (H, W)\n        \"\"\"\n        B, N, C = x_tok.shape\n        H, W = grid\n        if (H == self.H0) and (W == self.W0):\n            return x_tok + self.pos\n\n        # Interpola pos emb como mapa [1, C, H, W] -> nuevo tamaño\n        pos = self.pos.reshape(1, self.H0, self.W0, self.dim).permute(0, 3, 1, 2)  # [1,C,H0,W0]\n        pos = nn.functional.interpolate(pos, size=(H, W), mode=\"bicubic\", align_corners=False)\n        pos = pos.permute(0, 2, 3, 1).reshape(1, H * W, self.dim)\n        return x_tok + pos\n\nclass VOLOClassifier(nn.Module):\n    \"\"\"\n    VOLO para CIFAR-100 (y similares), con dos modos:\n      - flat: OutlookerStage -> TransformerStack (sin downsample)\n              pooling: mean | cls | cli (cls via class-attn final)\n      - hierarchical: pirámide con downsample (map o token)\n              pooling: SOLO mean (por ahora)\n\n    Flujo base:\n      x [B,3,H,W]\n        -> PatchEmbeddingConv -> x_tok [B, N, C0]\n        -> pos emb (opcional)\n        -> backbone (flat o pyramid)\n        -> pooling\n        -> head\n    \"\"\"\n\n    def __init__(\n        self,\n        num_classes: int = 100,\n        img_size: int = 32,\n        in_chans: int = 3,\n        patch_size: int = 4,\n\n        # mode\n        hierarchical: bool = False,\n        downsample_kind: str = \"map\",   # si hierarchical=True: \"map\" o \"token\"\n\n        # dims / depths (flat)\n        embed_dim: int = 192,\n        outlooker_depth: int = 4,\n        outlooker_heads: int = 6,\n        transformer_depth: int = 6,\n        transformer_heads: int = 6,\n\n        # hierarchical configs (si hierarchical=True)\n        dims: tuple[int, ...] = (192, 256, 384),\n        outlooker_depths: tuple[int, ...] = (2, 2, 0),\n        outlooker_heads_list: tuple[int, ...] = (6, 8, 12),\n        transformer_depths: tuple[int, ...] = (0, 2, 2),\n        transformer_heads_list: tuple[int, ...] = (6, 8, 12),\n\n        # block hyperparams\n        kernel_size: int = 3,\n        mlp_ratio: float = 4.0,\n        dropout: float = 0.1,\n        attn_dropout: float = 0.0,\n        drop_path_rate: float = 0.0,\n\n        # head / pooling\n        pooling: str = \"mean\",          # flat: \"mean\"|\"cls\"|\"cli\" ; hierarchical: \"mean\"\n        use_pos_embed: bool = True,\n\n        # cls refinamiento (flat)\n        cls_attn_depth: int = 2,        # # capas ClassAttentionBlock\n        cli_init_alpha: float = 0.5,    # init alpha para pooling=\"cli\"\n        use_cls_pos: bool = True):\n\n        super().__init__()\n\n        self.hierarchical = hierarchical\n        self.use_pos_embed = use_pos_embed\n\n        if self.hierarchical:\n            assert pooling == \"mean\", \"Por ahora hierarchical solo soporta pooling='mean'.\"\n        else:\n            assert pooling in [\"mean\", \"cls\", \"cli\"], \"pooling en flat debe ser 'mean', 'cls' o 'cli'.\"\n        self.pooling = pooling\n\n        # ---- Patch Embedding ----\n        C0 = (dims[0] if hierarchical else embed_dim)\n\n        self.patch_embed = PatchEmbeddingConv(\n            patch_size=patch_size,\n            in_chans=in_chans,\n            embed_dim=C0,\n            norm_layer=nn.LayerNorm,\n            pad_if_needed=True,\n            return_tokens=True,)\n\n        Hp0 = math.ceil(img_size / patch_size)\n        Wp0 = math.ceil(img_size / patch_size)\n\n        self.pos_embed = PosEmbed2D(Hp0, Wp0, C0) if use_pos_embed else None\n        self.pos_drop = nn.Dropout(dropout)\n\n        # ---- Backbone ----\n        if not hierarchical:\n            total = outlooker_depth + transformer_depth\n            dpr = torch.linspace(0, drop_path_rate, total).tolist() if total > 0 else []\n            dpr_local = dpr[:outlooker_depth]\n            dpr_glob = dpr[outlooker_depth:]\n\n            self.local_stage = VOLOStage(\n                dim=embed_dim,\n                depth=outlooker_depth,\n                num_heads=outlooker_heads,\n                kernel_size=kernel_size,\n                stride=1,\n                mlp_ratio=mlp_ratio,\n                attn_drop=attn_dropout,\n                proj_drop=dropout,\n                drop_path=dpr_local if len(dpr_local) else 0.0,\n                mlp_drop=dropout)\n\n            self.global_blocks = nn.ModuleList([\n                TransformerBlock(\n                    dim=embed_dim,\n                    num_heads=transformer_heads,\n                    mlp_ratio=mlp_ratio,\n                    attn_dropout=attn_dropout,\n                    dropout=dropout,\n                    drop_path=(dpr_glob[i] if len(dpr_glob) else 0.0),\n                ) for i in range(transformer_depth)])\n\n            # --- CLS  (solo si pooling usa cls/cli) ---\n            self.use_cls = (pooling in [\"cls\", \"cli\"])\n            if self.use_cls:\n                self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n                trunc_normal_(self.cls_token, std=0.02)\n\n                self.cls_pos = None\n                if use_cls_pos:\n                    self.cls_pos = nn.Parameter(torch.zeros(1, 1, embed_dim))\n                    trunc_normal_(self.cls_pos, std=0.02)\n\n                self.cls_attn_blocks = nn.ModuleList([\n                    ClassAttentionBlock(\n                        dim=embed_dim,\n                        num_heads=transformer_heads,\n                        mlp_ratio=mlp_ratio,\n                        attn_dropout=attn_dropout,\n                        dropout=dropout,) for _ in range(int(cls_attn_depth))])\n\n                self.cli_pool = CLIPool(init_alpha=cli_init_alpha) if pooling == \"cli\" else None\n            else:\n                self.cls_token = None\n                self.cls_pos = None\n                self.cls_attn_blocks = None\n                self.cli_pool = None\n\n\n            self.norm = nn.LayerNorm(embed_dim)\n            self.norm_feat = nn.LayerNorm(embed_dim)\n\n            self.head = nn.Linear(embed_dim, num_classes)\n\n        else:\n            self.pyramid = VOLOPyramid(\n                dims=dims,\n                outlooker_depths=outlooker_depths,\n                outlooker_heads=outlooker_heads_list,\n                transformer_depths=transformer_depths,\n                transformer_heads=transformer_heads_list,\n                kernel_size=kernel_size,\n                mlp_ratio=mlp_ratio,\n                downsample_kind=downsample_kind,\n                drop_path_rate=drop_path_rate,)\n\n\n            self.norm = nn.LayerNorm(dims[-1])\n            self.norm_feat = nn.LayerNorm(dims[-1])\n            self.head = nn.Linear(dims[-1], num_classes)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        # Patch embedding\n        x_map, (Hp, Wp), x_tok, _pad = self.patch_embed(x)   # x_tok [B,N,C0]\n        B, N, C0 = x_tok.shape\n\n        # Pos emb sobre tokens del grid\n        if self.use_pos_embed and (self.pos_embed is not None):\n            x_tok = self.pos_embed(x_tok, (Hp, Wp))\n        x_tok = self.pos_drop(x_tok)\n\n        if not self.hierarchical:\n            # ---- Flat backbone ----\n            # Outlooker trabaja en map (sin CLS)\n            x_map = x_tok.view(B, Hp, Wp, C0)\n            x_map = self.local_stage(x_map)\n            x_tok = x_map.view(B, Hp * Wp, C0)  # [B,N,C]\n\n            # Transformer global (tokens sin CLS)\n            for blk in self.global_blocks:\n                x_tok = blk(x_tok)\n\n            #  Pooling\n            if self.pooling == \"mean\":\n                # Normaliza tokens y promedia\n                x_tok_n = self.norm(x_tok)           # [B,N,C]\n                feat = x_tok_n.mean(dim=1)           # [B,C]\n                feat = self.norm_feat(feat)          # [B,C]\n                return self.head(feat)\n\n            # CLS refinado con class-attn final (CaiT-style)\n            cls = self.cls_token.expand(B, -1, -1)   # [B,1,C]\n            if self.cls_pos is not None:\n                cls = cls + self.cls_pos\n\n            for cab in self.cls_attn_blocks:\n                cls = cab(cls, x_tok)               # [B,1,C]\n\n            cls_vec = cls.squeeze(1)                # [B,C]\n            cls_vec = self.norm_feat(cls_vec)\n\n            if self.pooling == \"cls\":\n                feat = cls_vec\n                return self.head(feat)\n\n            # pooling == \"cli\": mezcla CLS con mean(tokens) normalizado\n            tok_mean = self.norm(x_tok).mean(dim=1)  # [B,C]\n            feat = self.cli_pool(cls_vec, tok_mean)\n            feat = self.norm_feat(feat)\n            return self.head(feat)\n\n        else:\n            # ---- Hierarchical backbone (solo mean) ----\n            x_map = x_tok.view(B, Hp, Wp, C0)\n            x_last, (Hf, Wf) = self.pyramid(x_map)      # x_last: [B, Nf, C_last]\n\n            x_last = self.norm(x_last)\n            feat = x_last.mean(dim=1)\n            feat = self.norm_feat(feat)\n            return self.head(feat)\n","metadata":{"id":"SECkRyzH0IwB","trusted":true,"execution":{"iopub.status.busy":"2025-12-31T02:55:53.828052Z","iopub.execute_input":"2025-12-31T02:55:53.828759Z","iopub.status.idle":"2025-12-31T02:55:53.849797Z","shell.execute_reply.started":"2025-12-31T02:55:53.828731Z","shell.execute_reply":"2025-12-31T02:55:53.849011Z"}},"outputs":[],"execution_count":28},{"cell_type":"code","source":"def test_volo_classifier_flat():\n    torch.manual_seed(0)\n    model = VOLOClassifier(\n        num_classes=100,\n        img_size=64,\n        patch_size=4,\n        hierarchical=False,\n        embed_dim=192,\n        outlooker_depth=2,\n        transformer_depth=2,\n        outlooker_heads=6,\n        transformer_heads=6,\n        pooling=\"mean\")\n\n    x = torch.randn(2, 3, 64, 64)\n    y = model(x)\n    print(\"[OK] flat logits:\", y.shape)\n    assert y.shape == (2, 100)\n\ndef test_volo_classifier_hier():\n    torch.manual_seed(0)\n    model = VOLOClassifier(\n        num_classes=100,\n        img_size=64,\n        patch_size=4,\n        hierarchical=True,\n        downsample_kind=\"map\",\n        dims=(192, 256, 384),\n        outlooker_depths=(2, 2, 0),\n        outlooker_heads_list=(6, 8, 12),\n        transformer_depths=(0, 2, 2),\n        transformer_heads_list=(6, 8, 12),\n        pooling=\"mean\",)\n\n    x = torch.randn(2, 3, 64, 64)\n    y = model(x)\n    print(\"[OK] hier logits:\", y.shape)\n    assert y.shape == (2, 100)\n\ntest_volo_classifier_flat()\ntest_volo_classifier_hier()","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"mE2wYcCg032h","outputId":"05df5d57-bbf5-425c-e4c4-d652098709ec","trusted":true,"execution":{"iopub.status.busy":"2025-12-31T02:55:57.297831Z","iopub.execute_input":"2025-12-31T02:55:57.298598Z","iopub.status.idle":"2025-12-31T02:55:57.456553Z","shell.execute_reply.started":"2025-12-31T02:55:57.298573Z","shell.execute_reply":"2025-12-31T02:55:57.455786Z"}},"outputs":[{"name":"stdout","text":"[OK] flat logits: torch.Size([2, 100])\n[OK] hier logits: torch.Size([2, 100])\n","output_type":"stream"}],"execution_count":29},{"cell_type":"code","source":"def _fmt_out(output):\n    if isinstance(output, (tuple, list)):\n        shapes = []\n        for o in output:\n            if hasattr(o, \"shape\"):\n                shapes.append(tuple(o.shape))\n            else:\n                shapes.append(type(o).__name__)\n        return shapes\n    if hasattr(output, \"shape\"):\n        return tuple(output.shape)\n    return type(output).__name__\n\n\ndef attach_shape_hooks_volo(model: nn.Module, verbose: bool = True):\n    hooks = []\n\n    def add_hook(mod: nn.Module, name: str):\n        if mod is None:\n            return\n        def hook(_m, _inp, out):\n            print(f\"{name:35s} -> {_fmt_out(out)}\")\n        hooks.append(mod.register_forward_hook(hook))\n\n    # Top-level components\n    add_hook(getattr(model, \"patch_embed\", None), \"patch_embed\")\n    add_hook(getattr(model, \"local_stage\", None), \"local_stage (outlooker)\")\n    add_hook(getattr(model, \"pyramid\", None), \"pyramid (top)\")\n    add_hook(getattr(model, \"norm\", None), \"norm\")\n    add_hook(getattr(model, \"head\", None), \"head\")\n\n    # Global blocks (flat)\n    if hasattr(model, \"global_blocks\"):\n        for i, blk in enumerate(model.global_blocks):\n            add_hook(blk, f\"global_block[{i}]\")\n\n    # Pyramid internals (hierarchical)\n    pyr = getattr(model, \"pyramid\", None)\n    if pyr is not None:\n        if hasattr(pyr, \"levels\"):\n            for i, lvl in enumerate(pyr.levels):\n                # lvl es nn.ModuleDict: NO tiene .get\n                loc = lvl[\"local\"] if \"local\" in lvl else None\n                glob = lvl[\"global\"] if \"global\" in lvl else None\n                add_hook(loc,  f\"pyr.level[{i}].local\")\n                add_hook(glob, f\"pyr.level[{i}].global\")\n\n        if hasattr(pyr, \"downsamples\"):\n            for i, ds in enumerate(pyr.downsamples):\n                add_hook(ds, f\"pyr.down[{i}]\")\n\n    return hooks\n\ndef remove_hooks(hooks):\n    for h in hooks:\n        h.remove()\n\n@torch.no_grad()\ndef debug_forward_shapes(model: nn.Module, img_size: int, device: str = \"cpu\", batch_size: int = 2):\n    model = model.to(device).eval()\n    hooks = attach_shape_hooks_volo(model)\n\n    x = torch.randn(batch_size, 3, img_size, img_size, device=device)\n    print(f\"\\n=== Forward debug | img_size={img_size} | model={model.__class__.__name__} ===\")\n    y = model(x)\n    print(f\"{'OUTPUT logits':35s} -> {tuple(y.shape)}\")\n\n    remove_hooks(hooks)\n","metadata":{"id":"2Ke7ncx_69Rq","trusted":true,"execution":{"iopub.status.busy":"2025-12-31T02:55:59.900843Z","iopub.execute_input":"2025-12-31T02:55:59.901374Z","iopub.status.idle":"2025-12-31T02:55:59.910828Z","shell.execute_reply.started":"2025-12-31T02:55:59.901350Z","shell.execute_reply":"2025-12-31T02:55:59.910100Z"}},"outputs":[],"execution_count":31},{"cell_type":"code","source":"model_flat64 = VOLOClassifier(\n    num_classes=100,\n    img_size=64,\n    patch_size=4,\n    hierarchical=False,\n    embed_dim=192,\n    outlooker_depth=2,\n    outlooker_heads=6,\n    transformer_depth=2,\n    transformer_heads=6,\n    pooling=\"mean\",\n    use_pos_embed=True,)\n\ndebug_forward_shapes(model_flat64, img_size=64, device=\"cpu\")\n","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"aaORyp_F7EsC","outputId":"31dc82fc-2fbf-4ed8-9d0e-a66eb400740a","trusted":true,"execution":{"iopub.status.busy":"2025-12-31T02:56:02.493933Z","iopub.execute_input":"2025-12-31T02:56:02.494689Z","iopub.status.idle":"2025-12-31T02:56:02.552914Z","shell.execute_reply.started":"2025-12-31T02:56:02.494663Z","shell.execute_reply":"2025-12-31T02:56:02.552144Z"}},"outputs":[{"name":"stdout","text":"\n=== Forward debug | img_size=64 | model=VOLOClassifier ===\npatch_embed                         -> [(2, 16, 16, 192), 'tuple', (2, 256, 192), 'tuple']\nlocal_stage (outlooker)             -> (2, 16, 16, 192)\nglobal_block[0]                     -> (2, 256, 192)\nglobal_block[1]                     -> (2, 256, 192)\nnorm                                -> (2, 256, 192)\nhead                                -> (2, 100)\nOUTPUT logits                       -> (2, 100)\n","output_type":"stream"}],"execution_count":32},{"cell_type":"code","source":"model_hier64 = VOLOClassifier(\n    num_classes=100,\n    img_size=64,\n    patch_size=4,\n    hierarchical=True,\n    downsample_kind=\"map\",\n    dims=(192, 256, 384),\n    outlooker_depths=(2, 2, 0),\n    outlooker_heads_list=(6, 8, 12),\n    transformer_depths=(0, 2, 2),\n    transformer_heads_list=(6, 8, 12),\n    pooling=\"mean\",\n    use_pos_embed=True,)\n\ndebug_forward_shapes(model_hier64, img_size=64, device=\"cpu\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"FEDhIpY47KHP","outputId":"2fca77d2-53d5-45e8-d878-bb39841672d9","trusted":true,"execution":{"iopub.status.busy":"2025-12-31T02:56:04.269460Z","iopub.execute_input":"2025-12-31T02:56:04.269724Z","iopub.status.idle":"2025-12-31T02:56:04.381348Z","shell.execute_reply.started":"2025-12-31T02:56:04.269706Z","shell.execute_reply":"2025-12-31T02:56:04.380452Z"}},"outputs":[{"name":"stdout","text":"\n=== Forward debug | img_size=64 | model=VOLOClassifier ===\npatch_embed                         -> [(2, 16, 16, 192), 'tuple', (2, 256, 192), 'tuple']\npyr.level[0].local                  -> (2, 16, 16, 192)\npyr.down[0]                         -> (2, 8, 8, 256)\npyr.level[1].local                  -> (2, 8, 8, 256)\npyr.level[1].global                 -> (2, 64, 256)\npyr.down[1]                         -> (2, 4, 4, 384)\npyr.level[2].global                 -> (2, 16, 384)\npyramid (top)                       -> [(2, 16, 384), 'tuple']\nnorm                                -> (2, 16, 384)\nhead                                -> (2, 100)\nOUTPUT logits                       -> (2, 100)\n","output_type":"stream"}],"execution_count":33},{"cell_type":"code","source":"model_hier64_tok = VOLOClassifier(\n    num_classes=100,\n    img_size=64,\n    patch_size=4,\n    hierarchical=True,\n    downsample_kind=\"token\",\n    dims=(192, 256, 384),\n    outlooker_depths=(2, 2, 0),\n    outlooker_heads_list=(6, 8, 12),\n    transformer_depths=(0, 2, 2),\n    transformer_heads_list=(6, 8, 12),\n    pooling=\"mean\",\n    use_pos_embed=True,)\n\ndebug_forward_shapes(model_hier64_tok, img_size=64, device=\"cpu\")","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A77K81Kn7NO3","outputId":"57075d87-388a-4569-b494-c78b7a5ff121"},"outputs":[{"output_type":"stream","name":"stdout","text":["\n","=== Forward debug | img_size=64 | model=VOLOClassifier ===\n","patch_embed                         -> [(2, 16, 16, 192), 'tuple', (2, 256, 192), 'tuple']\n","pyr.level[0].local                  -> (2, 16, 16, 192)\n","pyr.down[0]                         -> [(2, 64, 256), 'tuple']\n","pyr.level[1].local                  -> (2, 8, 8, 256)\n","pyr.level[1].global                 -> (2, 64, 256)\n","pyr.down[1]                         -> [(2, 16, 384), 'tuple']\n","pyr.level[2].global                 -> (2, 16, 384)\n","pyramid (top)                       -> [(2, 16, 384), 'tuple']\n","norm                                -> (2, 16, 384)\n","head                                -> (2, 100)\n","OUTPUT logits                       -> (2, 100)\n"]}],"execution_count":47},{"cell_type":"markdown","source":"---","metadata":{"id":"d1DRvX0A4caq"}},{"cell_type":"code","source":"import os, math, random, inspect\nfrom contextlib import contextmanager, nullcontext\nfrom typing import Dict\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\ndef seed_everything(seed: int = 0, deterministic: bool = False):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    if deterministic:\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n    else:\n        torch.backends.cudnn.benchmark = True\n\n_DTYPE_MAP = {\n    \"bf16\": torch.bfloat16, \"bfloat16\": torch.bfloat16,\n    \"fp16\": torch.float16,  \"float16\": torch.float16,\n    \"fp32\": torch.float32,  \"float32\": torch.float32,}\n\ndef _cuda_dtype_supported(dtype: torch.dtype) -> bool:\n    if not torch.cuda.is_available():\n        return False\n    return dtype in (torch.float16, torch.bfloat16)\n\ndef make_grad_scaler(device: str = \"cuda\", enabled: bool = True):\n    if not enabled:\n        return None\n\n    if hasattr(torch, \"amp\") and hasattr(torch.amp, \"GradScaler\"):\n        try:\n            sig = inspect.signature(torch.amp.GradScaler)\n            if len(sig.parameters) >= 1:\n                return torch.amp.GradScaler(device if device in (\"cuda\", \"cpu\") else \"cuda\")\n            return torch.amp.GradScaler()\n        except Exception:\n            pass\n\n    if hasattr(torch.cuda, \"amp\") and hasattr(torch.cuda.amp, \"GradScaler\"):\n        return torch.cuda.amp.GradScaler()\n    return None\n\n\n@contextmanager\ndef autocast_ctx(\n    device: str = \"cuda\",\n    enabled: bool = True,\n    dtype: str = \"fp16\",\n    cache_enabled: bool = True,):\n    \"\"\"\n    Context manager de autocast:\n      - cuda: fp16 por defecto (ideal en T4)\n      - cpu: bfloat16 si está disponible\n    \"\"\"\n    if not enabled:\n        with nullcontext():\n            yield\n        return\n\n    if device == \"cuda\":\n        want = _DTYPE_MAP.get(dtype.lower(), torch.float16)\n        use = want if _cuda_dtype_supported(want) else torch.float16\n        with torch.amp.autocast(device_type=\"cuda\", dtype=use, cache_enabled=cache_enabled):\n            yield\n        return\n\n    if device == \"cpu\":\n        try:\n            with torch.amp.autocast(device_type=\"cpu\", dtype=torch.bfloat16, cache_enabled=cache_enabled):\n                yield\n        except Exception:\n            with nullcontext():\n                yield\n        return\n\n    with nullcontext():\n        yield","metadata":{"id":"qQU0oPtG4dmV","trusted":true,"execution":{"iopub.status.busy":"2025-12-31T02:56:06.890188Z","iopub.execute_input":"2025-12-31T02:56:06.890720Z","iopub.status.idle":"2025-12-31T02:56:06.900659Z","shell.execute_reply.started":"2025-12-31T02:56:06.890699Z","shell.execute_reply":"2025-12-31T02:56:06.899961Z"}},"outputs":[],"execution_count":34},{"cell_type":"code","source":"def build_param_groups_no_wd(model: nn.Module, weight_decay: float):\n    decay, no_decay = [], []\n    for name, p in model.named_parameters():\n        if not p.requires_grad:\n            continue\n\n        name_l = name.lower()\n        # no decay for biases + norms + positional/class tokens\n        if (\n            name.endswith(\".bias\")\n            or (\"norm\" in name_l)\n            or (\"bn\" in name_l)\n            or (\"ln\" in name_l)\n            or (\"pos\" in name_l)         # pos_embed / pos\n            or (\"cls_token\" in name_l)\n        ):\n            no_decay.append(p)\n        else:\n            decay.append(p)\n\n    return [\n        {\"params\": decay, \"weight_decay\": weight_decay},\n        {\"params\": no_decay, \"weight_decay\": 0.0}]\n\n\nclass WarmupCosineLR:\n    \"\"\"Warmup linear for warmup_steps, then cosine to min_lr. Step-based.\"\"\"\n    def __init__(self, optimizer, total_steps: int, warmup_steps: int, min_lr: float = 0.0):\n        self.optimizer = optimizer\n        self.total_steps = int(total_steps)\n        self.warmup_steps = int(warmup_steps)\n        self.min_lr = float(min_lr)\n        self.base_lrs = [g[\"lr\"] for g in optimizer.param_groups]\n        self.step_num = 0\n\n    def step(self):\n        self.step_num += 1\n        t = self.step_num\n\n        for i, group in enumerate(self.optimizer.param_groups):\n            base = self.base_lrs[i]\n            if t <= self.warmup_steps and self.warmup_steps > 0:\n                lr = base * (t / self.warmup_steps)\n            else:\n                tt = min(t, self.total_steps)\n                denom = max(1, self.total_steps - self.warmup_steps)\n                progress = (tt - self.warmup_steps) / denom\n                cosine = 0.5 * (1.0 + math.cos(math.pi * progress))\n                lr = self.min_lr + (base - self.min_lr) * cosine\n            group[\"lr\"] = lr\n\n    def state_dict(self):\n        return {\"step_num\": self.step_num}\n\n    def load_state_dict(self, d):\n        self.step_num = int(d.get(\"step_num\", 0))","metadata":{"id":"IOvy-NJH5gZG","trusted":true,"execution":{"iopub.status.busy":"2025-12-31T02:56:10.716927Z","iopub.execute_input":"2025-12-31T02:56:10.717190Z","iopub.status.idle":"2025-12-31T02:56:10.725268Z","shell.execute_reply.started":"2025-12-31T02:56:10.717171Z","shell.execute_reply":"2025-12-31T02:56:10.724497Z"}},"outputs":[],"execution_count":36},{"cell_type":"code","source":"def save_checkpoint(\n    path: str,\n    model,\n    optimizer,\n    scheduler,\n    scaler,\n    epoch: int,\n    best_top1: float,\n    extra: dict | None = None,):\n\n    ckpt = {\n        \"model\": model.state_dict(),\n        \"optimizer\": optimizer.state_dict() if optimizer is not None else None,\n        \"scheduler\": scheduler.state_dict() if scheduler is not None else None,\n        \"scaler\": scaler.state_dict() if scaler is not None else None,\n        \"epoch\": epoch,\n        \"best_top1\": best_top1,\n        \"extra\": extra or {},}\n    torch.save(ckpt, path)\n\n\ndef load_checkpoint(\n    path: str,\n    model,\n    optimizer=None,\n    scheduler=None,\n    scaler=None,\n    map_location=\"cpu\",\n    strict: bool = True,):\n    ckpt = torch.load(path, map_location=map_location)\n    model.load_state_dict(ckpt[\"model\"], strict=strict)\n\n    if optimizer is not None and ckpt.get(\"optimizer\") is not None:\n        optimizer.load_state_dict(ckpt[\"optimizer\"])\n    if scheduler is not None and ckpt.get(\"scheduler\") is not None:\n        scheduler.load_state_dict(ckpt[\"scheduler\"])\n    if scaler is not None and ckpt.get(\"scaler\") is not None:\n        scaler.load_state_dict(ckpt[\"scaler\"])\n    return ckpt","metadata":{"id":"CPrZy1aU5nYv","trusted":true,"execution":{"iopub.status.busy":"2025-12-31T02:56:12.185750Z","iopub.execute_input":"2025-12-31T02:56:12.186547Z","iopub.status.idle":"2025-12-31T02:56:12.192753Z","shell.execute_reply.started":"2025-12-31T02:56:12.186518Z","shell.execute_reply":"2025-12-31T02:56:12.192064Z"}},"outputs":[],"execution_count":37},{"cell_type":"code","source":"# -------------------------\n# Mixup / CutMix + Loss\n# -------------------------\ndef _one_hot(targets: torch.Tensor, num_classes: int) -> torch.Tensor:\n    return F.one_hot(targets, num_classes=num_classes).float()\n\n\ndef soft_target_cross_entropy(logits: torch.Tensor, targets_soft: torch.Tensor) -> torch.Tensor:\n    logp = F.log_softmax(logits, dim=1)\n    return -(targets_soft * logp).sum(dim=1).mean()\n\n\ndef apply_mixup_cutmix(\n    images: torch.Tensor,\n    targets: torch.Tensor,\n    num_classes: int,\n    mixup_alpha: float = 0.0,\n    cutmix_alpha: float = 0.0,\n    prob: float = 1.0,):\n    \"\"\"\n    Returns:\n      images_aug: [B,3,H,W]\n      targets_soft: [B,K]\n    \"\"\"\n    if prob <= 0.0 or (mixup_alpha <= 0.0 and cutmix_alpha <= 0.0):\n        return images, _one_hot(targets, num_classes)\n\n    if random.random() > prob:\n        return images, _one_hot(targets, num_classes)\n\n    use_cutmix = (cutmix_alpha > 0.0) and (mixup_alpha <= 0.0 or random.random() < 0.5)\n    B, _, H, W = images.shape\n    perm = torch.randperm(B, device=images.device)\n\n    y1 = _one_hot(targets, num_classes)\n    y2 = _one_hot(targets[perm], num_classes)\n\n    if use_cutmix:\n        lam = torch.distributions.Beta(cutmix_alpha, cutmix_alpha).sample().item()\n        cut_w = int(W * math.sqrt(1.0 - lam))\n        cut_h = int(H * math.sqrt(1.0 - lam))\n        cx = random.randint(0, W - 1)\n        cy = random.randint(0, H - 1)\n\n        x1 = max(cx - cut_w // 2, 0)\n        x2 = min(cx + cut_w // 2, W)\n        y1b = max(cy - cut_h // 2, 0)\n        y2b = min(cy + cut_h // 2, H)\n\n        images_aug = images.clone()\n        images_aug[:, :, y1b:y2b, x1:x2] = images[perm, :, y1b:y2b, x1:x2]\n\n        # adjust lambda based on actual area swapped\n        area = (x2 - x1) * (y2b - y1b)\n        lam = 1.0 - area / float(W * H)\n    else:\n        lam = torch.distributions.Beta(mixup_alpha, mixup_alpha).sample().item()\n        images_aug = images * lam + images[perm] * (1.0 - lam)\n\n    targets_soft = y1 * lam + y2 * (1.0 - lam)\n    return images_aug, targets_soft","metadata":{"id":"zpVyEsL75qJW","trusted":true,"execution":{"iopub.status.busy":"2025-12-31T02:56:14.092956Z","iopub.execute_input":"2025-12-31T02:56:14.093674Z","iopub.status.idle":"2025-12-31T02:56:14.102092Z","shell.execute_reply.started":"2025-12-31T02:56:14.093647Z","shell.execute_reply":"2025-12-31T02:56:14.101432Z"}},"outputs":[],"execution_count":38},{"cell_type":"code","source":"# -------------------------\n# Metrics\n# -------------------------\n@torch.no_grad()\ndef accuracy_topk(logits: torch.Tensor, targets: torch.Tensor, ks=(1, 3, 5)) -> Dict[int, float]:\n    \"\"\"\n    targets can be:\n      - int64 class indices [B]\n      - soft targets [B, num_classes] (we'll argmax for accuracy reporting)\n    \"\"\"\n    if targets.ndim == 2:\n        targets = targets.argmax(dim=1)\n\n    max_k = max(ks)\n    B = targets.size(0)\n    _, pred = torch.topk(logits, k=max_k, dim=1)\n    correct = pred.eq(targets.view(-1, 1).expand_as(pred))\n    out = {}\n    for k in ks:\n        out[k] = 100.0 * correct[:, :k].any(dim=1).float().sum().item() / B\n    return out","metadata":{"id":"1X9ksaTi5tNl","trusted":true,"execution":{"iopub.status.busy":"2025-12-31T02:56:16.053281Z","iopub.execute_input":"2025-12-31T02:56:16.053555Z","iopub.status.idle":"2025-12-31T02:56:16.059125Z","shell.execute_reply.started":"2025-12-31T02:56:16.053537Z","shell.execute_reply":"2025-12-31T02:56:16.058546Z"}},"outputs":[],"execution_count":39},{"cell_type":"code","source":"import torch\nimport torch.distributed as dist\nfrom torch.utils.data.distributed import DistributedSampler\n\ndef ddp_is_on() -> bool:\n    return dist.is_available() and dist.is_initialized()\n\ndef ddp_rank() -> int:\n    return dist.get_rank() if ddp_is_on() else 0\n\ndef is_main_process() -> bool:\n    return (not ddp_is_on()) or ddp_rank() == 0\n\ndef ddp_sum_(tensor: torch.Tensor) -> torch.Tensor:\n    \"\"\"All-reduce SUM in-place and return tensor.\"\"\"\n    if ddp_is_on():\n        dist.all_reduce(tensor, op=dist.ReduceOp.SUM)\n    return tensor\n\ndef ddp_broadcast_bool(flag: bool, device: torch.device | str) -> bool:\n    \"\"\"Broadcast a stop flag from rank0 to all ranks.\"\"\"\n    t = torch.tensor([1 if flag else 0], device=device)\n    if ddp_is_on():\n        dist.broadcast(t, src=0)\n    return bool(t.item())","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T03:05:53.570691Z","iopub.execute_input":"2025-12-31T03:05:53.570982Z","iopub.status.idle":"2025-12-31T03:05:53.577193Z","shell.execute_reply.started":"2025-12-31T03:05:53.570962Z","shell.execute_reply":"2025-12-31T03:05:53.576578Z"}},"outputs":[],"execution_count":47},{"cell_type":"code","source":"from typing import Optional\nimport time\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\ndef train_one_epoch(\n    model: nn.Module,\n    dataloader,\n    optimizer: torch.optim.Optimizer,\n    scheduler,\n    device: str = \"cuda\",\n    scaler=None,\n    autocast_dtype: str = \"fp16\",\n    use_amp: bool = True,\n    grad_clip_norm: Optional[float] = 1.0,\n    label_smoothing: float = 0.1,\n    mixup_alpha: float = 0.0,\n    cutmix_alpha: float = 0.0,\n    mix_prob: float = 1.0,\n    num_classes: int = 100,\n    channels_last: bool = False,\n    print_every: int = 100,\n):\n    model.train()\n\n    use_scaler = (scaler is not None) and use_amp and autocast_dtype.lower() in (\"fp16\", \"float16\")\n\n    running_loss = 0.0\n    total = 0\n    c1 = c3 = c5 = 0.0  # “correct counts” acumulados (no %)\n\n    t0 = time.time()\n    for step, (images, targets) in enumerate(dataloader, start=1):\n        images = images.to(device, non_blocking=True)\n        targets = targets.to(device, non_blocking=True)\n\n        if channels_last:\n            images = images.contiguous(memory_format=torch.channels_last)\n\n        B = targets.size(0)\n\n        images_aug, targets_soft = apply_mixup_cutmix(\n            images, targets,\n            num_classes=num_classes,\n            mixup_alpha=mixup_alpha,\n            cutmix_alpha=cutmix_alpha,\n            prob=mix_prob\n        )\n\n        use_mix = (mixup_alpha > 0.0) or (cutmix_alpha > 0.0)\n        targets_for_acc = targets_soft if use_mix else targets\n\n        optimizer.zero_grad(set_to_none=True)\n\n        with autocast_ctx(device=device, enabled=use_amp, dtype=autocast_dtype, cache_enabled=True):\n            logits = model(images_aug)\n\n        if use_mix:\n            loss = soft_target_cross_entropy(logits.float(), targets_soft)\n        else:\n            loss = F.cross_entropy(logits.float(), targets, label_smoothing=label_smoothing)\n\n        if use_scaler:\n            scaler.scale(loss).backward()\n            if grad_clip_norm is not None:\n                scaler.unscale_(optimizer)\n                torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip_norm)\n            scaler.step(optimizer)\n            scaler.update()\n        else:\n            loss.backward()\n            if grad_clip_norm is not None:\n                torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip_norm)\n            optimizer.step()\n\n        if scheduler is not None:\n            scheduler.step()\n\n        # métricas “locales”\n        running_loss += loss.item() * B\n        total += B\n        accs = accuracy_topk(logits.detach(), targets_for_acc, ks=(1, 3, 5))\n        c1 += accs[1] * B / 100.0\n        c3 += accs[3] * B / 100.0\n        c5 += accs[5] * B / 100.0\n\n        # log solo en rank0 (si DDP)\n        if print_every and (step % print_every == 0) and is_main_process():\n            dt = time.time() - t0\n            imgs_sec = total / max(dt, 1e-9)\n            print(\n                f\"[train step {step}/{len(dataloader)}] \"\n                f\"loss {running_loss/total:.4f} | \"\n                f\"top1 {100*c1/total:.2f}% | top3 {100*c3/total:.2f}% | top5 {100*c5/total:.2f}% | \"\n                f\"{imgs_sec:.1f} img/s | lr {optimizer.param_groups[0]['lr']:.2e}\"\n            )\n\n    # ---- REDUCCIÓN GLOBAL (DDP) ----\n    stats = torch.tensor([running_loss, total, c1, c3, c5], device=device, dtype=torch.float64)\n    ddp_sum_(stats)\n    running_loss_g, total_g, c1_g, c3_g, c5_g = stats.tolist()\n\n    avg_loss = running_loss_g / max(total_g, 1e-12)\n    metrics = {\n        \"top1\": 100.0 * c1_g / max(total_g, 1e-12),\n        \"top3\": 100.0 * c3_g / max(total_g, 1e-12),\n        \"top5\": 100.0 * c5_g / max(total_g, 1e-12),}\n    \n    return avg_loss, metrics","metadata":{"id":"oDbIUA_65uZi","trusted":true,"execution":{"iopub.status.busy":"2025-12-31T03:06:18.463530Z","iopub.execute_input":"2025-12-31T03:06:18.464225Z","iopub.status.idle":"2025-12-31T03:06:18.476977Z","shell.execute_reply.started":"2025-12-31T03:06:18.464199Z","shell.execute_reply":"2025-12-31T03:06:18.476388Z"}},"outputs":[],"execution_count":48},{"cell_type":"code","source":"\n@torch.no_grad()\ndef evaluate_one_epoch(\n    model: nn.Module,\n    dataloader,\n    device: str = \"cuda\",\n    autocast_dtype: str = \"fp16\",\n    use_amp: bool = True,\n    label_smoothing: float = 0.0,\n    channels_last: bool = False,\n):\n    model.eval()\n\n    running_loss = 0.0\n    total = 0\n    c1 = c3 = c5 = 0.0\n\n    for images, targets in dataloader:\n        images = images.to(device, non_blocking=True)\n        targets = targets.to(device, non_blocking=True)\n\n        if channels_last:\n            images = images.contiguous(memory_format=torch.channels_last)\n\n        B = targets.size(0)\n\n        with autocast_ctx(device=device, enabled=use_amp, dtype=autocast_dtype, cache_enabled=True):\n            logits = model(images)\n\n        loss = F.cross_entropy(logits.float(), targets, label_smoothing=label_smoothing)\n\n        running_loss += loss.item() * B\n        total += B\n\n        accs = accuracy_topk(logits, targets, ks=(1, 3, 5))\n        c1 += accs[1] * B / 100.0\n        c3 += accs[3] * B / 100.0\n        c5 += accs[5] * B / 100.0\n\n    # ---- REDUCCIÓN GLOBAL (DDP) ----\n    stats = torch.tensor([running_loss, total, c1, c3, c5], device=device, dtype=torch.float64)\n    ddp_sum_(stats)\n    running_loss_g, total_g, c1_g, c3_g, c5_g = stats.tolist()\n\n    avg_loss = running_loss_g / max(total_g, 1e-12)\n    metrics = {\n        \"top1\": 100.0 * c1_g / max(total_g, 1e-12),\n        \"top3\": 100.0 * c3_g / max(total_g, 1e-12),\n        \"top5\": 100.0 * c5_g / max(total_g, 1e-12),}\n    \n    return avg_loss, metrics","metadata":{"id":"ASZKG0qq6GC8","trusted":true,"execution":{"iopub.status.busy":"2025-12-31T03:06:32.612338Z","iopub.execute_input":"2025-12-31T03:06:32.612611Z","iopub.status.idle":"2025-12-31T03:06:32.620029Z","shell.execute_reply.started":"2025-12-31T03:06:32.612590Z","shell.execute_reply":"2025-12-31T03:06:32.619415Z"}},"outputs":[],"execution_count":49},{"cell_type":"code","source":"import time\nimport torch\nimport torch.nn as nn\n\ndef train_model(\n    model: nn.Module,\n    train_loader,\n    epochs: int,\n    val_loader=None,\n    device: str = \"cuda\",\n    lr: float = 5e-4,\n    weight_decay: float = 0.05,\n    autocast_dtype: str = \"fp16\",\n    use_amp: bool = True,\n    grad_clip_norm: float | None = 1.0,\n    warmup_ratio: float = 0.05,\n    min_lr: float = 0.0,\n    label_smoothing: float = 0.1,\n    print_every: int = 100,\n    save_path: str = \"best_model.pt\",\n    last_path: str = \"last_model.pt\",\n    resume_path: str | None = None,\n\n    mixup_alpha: float = 0.0,\n    cutmix_alpha: float = 0.0,\n    mix_prob: float = 1.0,\n    num_classes: int = 100,\n    channels_last: bool = False,\n\n    early_stop: bool = True,\n    early_stop_metric: str = \"top1\",\n    early_stop_patience: int = 10,\n    early_stop_min_delta: float = 0.0,\n    early_stop_require_monotonic: bool = False):\n\n    model.to(device)\n\n    # Optimizer\n    param_groups = build_param_groups_no_wd(model, weight_decay=weight_decay)\n    optimizer = torch.optim.AdamW(param_groups, lr=lr, betas=(0.9, 0.999), eps=1e-8)\n\n    # Scheduler warmup + cosine (step-based)\n    total_steps = epochs * len(train_loader)\n    warmup_steps = int(total_steps * warmup_ratio)\n    scheduler = WarmupCosineLR(\n        optimizer,\n        total_steps=total_steps,\n        warmup_steps=warmup_steps,\n        min_lr=min_lr,\n    )\n\n    scaler = None\n    if use_amp and autocast_dtype.lower() in (\"fp16\", \"float16\"):\n        scaler = make_grad_scaler(device=device, enabled=True)\n\n    # Resume\n    start_epoch = 0\n    best_val_top1 = -float(\"inf\")\n    best_val_loss = float(\"inf\")\n    best_epoch = 0\n\n    if resume_path is not None:\n        ckpt = load_checkpoint(\n            resume_path, model,\n            optimizer=optimizer, scheduler=scheduler, scaler=scaler,\n            map_location=device,\n            strict=True,\n        )\n\n        start_epoch = int(ckpt.get(\"epoch\", 0))\n        best_val_top1 = float(ckpt.get(\"best_top1\", best_val_top1))\n        extra = ckpt.get(\"extra\", {}) or {}\n        best_val_loss = float(extra.get(\"best_val_loss\", best_val_loss))\n        best_epoch = int(extra.get(\"best_epoch\", best_epoch))\n\n        if is_main_process():\n            print(f\"Resumed from {resume_path} at epoch {start_epoch} | best_top1 {best_val_top1:.2f}% | best_loss {best_val_loss:.4f}\")\n\n    history = {\n        \"train_loss\": [], \"train_top1\": [], \"train_top3\": [], \"train_top5\": [],\n        \"val_loss\": [], \"val_top1\": [], \"val_top3\": [], \"val_top5\": [],\n        \"lr\": [],\n    } if is_main_process() else None  # <- solo rank0 guarda history\n\n    # Early stop state (solo rank0 lleva el estado)\n    metric = early_stop_metric.lower()\n    assert metric in (\"top1\", \"loss\")\n    patience = int(early_stop_patience)\n    mode = \"max\" if metric == \"top1\" else \"min\"\n    best_metric = best_val_top1 if metric == \"top1\" else best_val_loss\n    bad_epochs = 0\n    last_vals = []\n\n    def _is_improvement(curr: float, best: float) -> bool:\n        d = float(early_stop_min_delta)\n        return (curr > (best + d)) if mode == \"max\" else (curr < (best - d))\n\n    def _degradation_monotonic(vals: list[float]) -> bool:\n        if not early_stop_require_monotonic or len(vals) < 2:\n            return True\n        if mode == \"max\":\n            return all(vals[i] >= vals[i + 1] for i in range(len(vals) - 1))\n        else:\n            return all(vals[i] <= vals[i + 1] for i in range(len(vals) - 1))\n\n    for epoch in range(start_epoch + 1, epochs + 1):\n        if is_main_process():\n            print(f\"\\n=== Epoch {epoch}/{epochs} ===\")\n        t_epoch = time.time()\n\n        # ✅ DDP: reshuffle correcto por epoch\n        if hasattr(train_loader, \"sampler\") and isinstance(train_loader.sampler, DistributedSampler):\n            train_loader.sampler.set_epoch(epoch)\n        if val_loader is not None and hasattr(val_loader, \"sampler\") and isinstance(val_loader.sampler, DistributedSampler):\n            val_loader.sampler.set_epoch(epoch)\n\n        # --- Train ---\n        tr_loss, tr_m = train_one_epoch(\n            model=model,\n            dataloader=train_loader,\n            optimizer=optimizer,\n            scheduler=scheduler,\n            device=device,\n            scaler=scaler,\n            autocast_dtype=autocast_dtype,\n            use_amp=use_amp,\n            grad_clip_norm=grad_clip_norm,\n            label_smoothing=label_smoothing,\n            mixup_alpha=mixup_alpha,\n            cutmix_alpha=cutmix_alpha,\n            mix_prob=mix_prob,\n            num_classes=num_classes,\n            channels_last=channels_last,\n            print_every=print_every,\n        )\n\n        if is_main_process():\n            history[\"train_loss\"].append(tr_loss)\n            history[\"train_top1\"].append(tr_m[\"top1\"])\n            history[\"train_top3\"].append(tr_m[\"top3\"])\n            history[\"train_top5\"].append(tr_m[\"top5\"])\n            history[\"lr\"].append(optimizer.param_groups[0][\"lr\"])\n\n            print(f\"[Train] loss {tr_loss:.4f} | top1 {tr_m['top1']:.2f}% | top3 {tr_m['top3']:.2f}% | top5 {tr_m['top5']:.2f}% | lr {optimizer.param_groups[0]['lr']:.2e}\")\n\n            # ✅ guardar \"last\" SOLO en rank0\n            save_checkpoint(\n                last_path, model, optimizer, scheduler, scaler,\n                epoch=epoch, best_top1=best_val_top1,\n                extra={\n                    \"autocast_dtype\": autocast_dtype,\n                    \"use_amp\": use_amp,\n                    \"best_val_loss\": best_val_loss,\n                    \"best_epoch\": best_epoch,\n                    \"early_stop_metric\": metric,\n                    \"early_stop_patience\": patience,\n                    \"early_stop_min_delta\": float(early_stop_min_delta),\n                },\n            )\n\n        stop_now = False\n\n        # --- Val ---\n        if val_loader is not None:\n            va_loss, va_m = evaluate_one_epoch(\n                model=model,\n                dataloader=val_loader,\n                device=device,\n                autocast_dtype=autocast_dtype,\n                use_amp=use_amp,\n                label_smoothing=0.0,\n                channels_last=channels_last,\n            )\n\n            if is_main_process():\n                history[\"val_loss\"].append(va_loss)\n                history[\"val_top1\"].append(va_m[\"top1\"])\n                history[\"val_top3\"].append(va_m[\"top3\"])\n                history[\"val_top5\"].append(va_m[\"top5\"])\n\n                print(f\"[Val]   loss {va_loss:.4f} | top1 {va_m['top1']:.2f}% | top3 {va_m['top3']:.2f}% | top5 {va_m['top5']:.2f}%\")\n\n                # Best saved por top1\n                if va_m[\"top1\"] > best_val_top1:\n                    best_val_top1 = va_m[\"top1\"]\n                    if va_loss < best_val_loss:\n                        best_val_loss = va_loss\n                        best_epoch = epoch\n\n                    save_checkpoint(\n                        save_path, model, optimizer, scheduler, scaler,\n                        epoch=epoch, best_top1=best_val_top1,\n                        extra={\n                            \"autocast_dtype\": autocast_dtype,\n                            \"use_amp\": use_amp,\n                            \"best_val_loss\": best_val_loss,\n                            \"best_epoch\": best_epoch,\n                        },\n                    )\n                    print(f\"Best saved to {save_path} (val top1 {best_val_top1:.2f}%)\")\n\n                # Early stop (solo rank0 decide)\n                if early_stop:\n                    curr_metric = va_m[\"top1\"] if metric == \"top1\" else va_loss\n\n                    last_vals.append(float(curr_metric))\n                    if len(last_vals) > patience:\n                        last_vals = last_vals[-patience:]\n\n                    if _is_improvement(curr_metric, best_metric):\n                        best_metric = float(curr_metric)\n                        bad_epochs = 0\n                    else:\n                        bad_epochs += 1\n\n                    if bad_epochs >= patience and _degradation_monotonic(last_vals):\n                        print(f\"Early-stop: no improvement on val_{metric} for {patience} epochs.\")\n                        stop_now = True\n\n        # ✅ DDP: sincroniza el “stop” a todos los ranks\n        stop_now = ddp_broadcast_bool(stop_now, device=device)\n        if stop_now:\n            break\n\n        if is_main_process():\n            dt = time.time() - t_epoch\n            print(f\"Epoch time: {dt/60:.2f} min\")\n\n    # return: history solo en rank0; en otros ranks devuelve None\n    return history, (model.module if hasattr(model, \"module\") else model)","metadata":{"id":"o9ZlcH5I6JL2","trusted":true,"execution":{"iopub.status.busy":"2025-12-31T03:21:54.027515Z","iopub.execute_input":"2025-12-31T03:21:54.028210Z","iopub.status.idle":"2025-12-31T03:21:54.047894Z","shell.execute_reply.started":"2025-12-31T03:21:54.028184Z","shell.execute_reply":"2025-12-31T03:21:54.047130Z"}},"outputs":[],"execution_count":63},{"cell_type":"code","source":"\n\n%%writefile volo.py\n\nimport torch\nfrom torch.utils.data import DataLoader, random_split\nfrom torchvision import datasets, transforms\nfrom torchvision.transforms import RandAugment\nimport torch.distributed as dist\nfrom torch.utils.data import Subset\nfrom torch.utils.data import Subset, DataLoader\n\ndef get_cifar100_datasets(\n    data_dir: str = \"./data\",\n    val_split: float = 0.0,\n    ra_num_ops: int = 2,\n    ra_magnitude: int = 7,\n    random_erasing_p: float = 0.25,\n    erasing_scale=(0.02, 0.20),\n    erasing_ratio=(0.3, 3.3),\n    img_size: int = 32,):\n\n    \"\"\"\n    CIFAR-100 datasets con augmentations \"mix-friendly\":\n    diseñadas para complementar Mixup/CutMix (en el loop) sin pasarse.\n\n    img_size:\n      - 32 (default): CIFAR nativo.\n      - >32: upsample (p.ej. 64) para experimentos (más tokens/compute).\n    \"\"\"\n    if img_size < 32:\n        raise ValueError(f\"img_size must be >= 32 for CIFAR-100. Got {img_size}.\")\n\n    cifar100_mean = (0.5071, 0.4867, 0.4408)\n    cifar100_std  = (0.2675, 0.2565, 0.2761)\n\n    # Si subimos resolución, primero hacemos resize y adaptamos crop/padding.\n    # Padding recomendado proporcional: 32->4, 64->8, etc.\n\n    crop_padding = max(4, img_size // 8)\n\n    train_ops = []\n    if img_size != 32:\n        train_ops.append(transforms.Resize(img_size, interpolation=transforms.InterpolationMode.BICUBIC))\n\n    train_ops += [\n        transforms.RandomCrop(img_size, padding=crop_padding),\n        transforms.RandomHorizontalFlip(),\n        RandAugment(num_ops=ra_num_ops, magnitude=ra_magnitude),\n        transforms.ToTensor(),\n        transforms.Normalize(cifar100_mean, cifar100_std),\n        transforms.RandomErasing(\n            p=random_erasing_p,\n            scale=erasing_scale,\n            ratio=erasing_ratio,\n            value=\"random\",),]\n\n    train_transform = transforms.Compose(train_ops)\n\n    test_ops = []\n    if img_size != 32:\n        test_ops.append(transforms.Resize(img_size, interpolation=transforms.InterpolationMode.BICUBIC))\n\n    test_ops += [\n        transforms.ToTensor(),\n        transforms.Normalize(cifar100_mean, cifar100_std),]\n\n    test_transform = transforms.Compose(test_ops)\n\n    full_train_dataset = datasets.CIFAR100(\n        root=data_dir, train=True, download=True, transform=train_transform)\n\n    test_dataset = datasets.CIFAR100(\n        root=data_dir, train=False, download=True, transform=test_transform)\n\n    if val_split > 0.0:\n        n_total = len(full_train_dataset)\n        n_val = int(n_total * val_split)\n        n_train = n_total - n_val\n        train_dataset, val_dataset = random_split(\n            full_train_dataset,\n            [n_train, n_val],\n            generator=torch.Generator().manual_seed(7),)\n\n    else:\n        train_dataset = full_train_dataset\n        val_dataset = None\n\n    return train_dataset, val_dataset, test_dataset\n\n\ndef get_cifar100_dataloaders(\n    batch_size: int = 128,\n    data_dir: str = \"./data\",\n    num_workers: int = 2,\n    val_split: float = 0.0,\n    pin_memory: bool = True,\n    ra_num_ops: int = 2,\n    ra_magnitude: int = 7,\n    random_erasing_p: float = 0.25,\n    img_size: int = 32,):\n    \"\"\"\n    Dataloaders CIFAR-100 listos para entrenar con Mixup/CutMix en el loop.\n    Augmentations no tan agresivas.\n\n    img_size:\n      - 32 (default): CIFAR nativo.\n      - 64: experimento de upsample (ojo: más compute).\n    \"\"\"\n    train_ds, val_ds, test_ds = get_cifar100_datasets(\n        data_dir=data_dir,\n        val_split=val_split,\n        ra_num_ops=ra_num_ops,\n        ra_magnitude=ra_magnitude,\n        random_erasing_p=random_erasing_p,\n        img_size=img_size,)\n\n    train_loader = DataLoader(\n        train_ds,\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=num_workers,\n        pin_memory=pin_memory,\n        persistent_workers=(num_workers > 0),)\n\n    val_loader = None\n    if val_ds is not None:\n        val_loader = DataLoader(\n            val_ds,\n            batch_size=batch_size,\n            shuffle=False,\n            num_workers=num_workers,\n            pin_memory=pin_memory,\n            persistent_workers=(num_workers > 0),)\n\n    test_loader = DataLoader(\n        test_ds,\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=num_workers,\n        pin_memory=pin_memory,\n        persistent_workers=(num_workers > 0),)\n\n    return train_loader, val_loader, test_loader\n\ndef _ddp_is_on():\n    return dist.is_available() and dist.is_initialized()\n\ndef _ddp_rank():\n    return dist.get_rank() if _ddp_is_on() else 0\n\ndef _ddp_barrier():\n    if _ddp_is_on():\n        dist.barrier()\n\ndef get_cifar100_datasets(\n    data_dir: str = \"./data\",\n    val_split: float = 0.0,\n    ra_num_ops: int = 1,\n    ra_magnitude: int = 5,\n    random_erasing_p: float = 0.1,\n    erasing_scale=(0.02, 0.20),\n    erasing_ratio=(0.3, 3.3),\n    img_size: int = 32,\n    seed: int = 7,\n    ddp_safe_download: bool = True):\n    \"\"\"\n    CIFAR-100 datasets con aug 'mix-friendly' y soporte DDP:\n      - Descarga segura: solo rank0 descarga, luego barrier.\n      - Split determinista: train/val indices iguales en todos los ranks.\n      - Val usa test_transform (SIN aug estocásticos).\n    \"\"\"\n    if img_size < 32:\n        raise ValueError(f\"img_size must be >= 32 for CIFAR-100. Got {img_size}.\")\n\n    cifar100_mean = (0.5071, 0.4867, 0.4408)\n    cifar100_std  = (0.2675, 0.2565, 0.2761)\n\n    crop_padding = max(4, img_size // 8)\n\n    train_ops = []\n    if img_size != 32:\n        train_ops.append(transforms.Resize(img_size, interpolation=transforms.InterpolationMode.BICUBIC))\n    train_ops += [\n        transforms.RandomCrop(img_size, padding=crop_padding),\n        transforms.RandomHorizontalFlip(),\n        RandAugment(num_ops=ra_num_ops, magnitude=ra_magnitude),\n        transforms.ToTensor(),\n        transforms.Normalize(cifar100_mean, cifar100_std),\n        transforms.RandomErasing(\n            p=random_erasing_p,\n            scale=erasing_scale,\n            ratio=erasing_ratio,\n            value=\"random\",\n        ),\n    ]\n    train_transform = transforms.Compose(train_ops)\n\n    test_ops = []\n    if img_size != 32:\n        test_ops.append(transforms.Resize(img_size, interpolation=transforms.InterpolationMode.BICUBIC))\n    test_ops += [\n        transforms.ToTensor(),\n        transforms.Normalize(cifar100_mean, cifar100_std),]\n\n    test_transform = transforms.Compose(test_ops)\n\n    #  DDP-safe download\n    if ddp_safe_download and _ddp_is_on():\n        if _ddp_rank() == 0:\n            datasets.CIFAR100(root=data_dir, train=True, download=True)\n            datasets.CIFAR100(root=data_dir, train=False, download=True)\n        _ddp_barrier()\n        download_flag = False\n    else:\n        download_flag = True\n\n    # Base datasets (dos versiones: train aug y eval clean)\n    full_train_aug = datasets.CIFAR100(root=data_dir, train=True, download=download_flag, transform=train_transform)\n    full_train_eval = datasets.CIFAR100(root=data_dir, train=True, download=False, transform=test_transform)\n    test_dataset = datasets.CIFAR100(root=data_dir, train=False, download=download_flag, transform=test_transform)\n\n    if val_split > 0.0:\n        n_total = len(full_train_aug)\n        n_val = int(n_total * val_split)\n        n_train = n_total - n_val\n\n        g = torch.Generator().manual_seed(seed)\n        perm = torch.randperm(n_total, generator=g).tolist()\n        train_idx = perm[:n_train]\n        val_idx = perm[n_train:]\n\n        train_dataset = Subset(full_train_aug, train_idx)\n        val_dataset = Subset(full_train_eval, val_idx)\n    else:\n        train_dataset = full_train_aug\n        val_dataset = None\n\n    return train_dataset, val_dataset, test_dataset\n\n\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass PatchEmbeddingConv(nn.Module):\n    \"\"\"\n    Patch embedding estilo Swin.\n\n    - Conv2d con kernel=stride=patch_size para convertir imagen -> grilla de patches.\n    - Devuelve el mapa 2D en formato canal-al-final: [B, Hp, Wp, D],\n      (más cómodo para window partition).\n    - Opcionalmente devuelve tokens [B, N, D].\n    - Opcional padding automático si H/W no son divisibles por patch_size.\n    \"\"\"\n\n    def __init__(\n        self,\n        patch_size: int | tuple[int, int] = 4,\n        in_chans: int = 3,\n        embed_dim: int = 192,\n        norm_layer: type[nn.Module] | None = nn.LayerNorm,\n        pad_if_needed: bool = True,\n        return_tokens: bool = True):\n\n        super().__init__()\n\n        if isinstance(patch_size, int):\n            patch_size = (patch_size, patch_size)\n\n        self.patch_size = patch_size  # (Ph, Pw)\n        self.in_chans = in_chans\n        self.embed_dim = embed_dim\n        self.pad_if_needed = pad_if_needed\n        self.return_tokens = return_tokens\n\n        # [B, C, H, W] -> [B, D, Hp, Wp]\n        self.proj = nn.Conv2d(\n            in_channels=in_chans,\n            out_channels=embed_dim,\n            kernel_size=patch_size,\n            stride=patch_size,\n            bias=True,)\n\n        # En Swin normalmente LayerNorm sobre la última dimensión\n        self.norm = norm_layer(embed_dim) if norm_layer is not None else None\n\n    def forward(self, x: torch.Tensor):\n        \"\"\"\n        Args:\n            x: [B, C, H, W]\n\n        Returns:\n            x_map:    [B, Hp, Wp, D]\n            (Hp, Wp): tamaño espacial en patches\n            x_tokens (opcional): [B, N, D]\n            pad_hw (opcional): (pad_h, pad_w) aplicados a la imagen\n        \"\"\"\n        B, C, H, W = x.shape\n        Ph, Pw = self.patch_size\n\n        pad_h = (Ph - (H % Ph)) % Ph\n        pad_w = (Pw - (W % Pw)) % Pw\n\n        if (pad_h != 0 or pad_w != 0):\n            if not self.pad_if_needed:\n                raise AssertionError(\n                    f\"Image size ({H}x{W}) no es divisible por patch_size {self.patch_size} \"\n                    f\"y pad_if_needed=False.\")\n\n            x = F.pad(x, (0, pad_w, 0, pad_h))\n\n        # [B, D, Hp, Wp]\n        x = self.proj(x)\n        Hp, Wp = x.shape[2], x.shape[3]\n\n        # canal al final -> [B, Hp, Wp, D]\n        x_map = x.permute(0, 2, 3, 1).contiguous()\n\n        if self.norm is not None:\n            x_map = self.norm(x_map)\n\n        if self.return_tokens:\n            x_tokens = x_map.view(B, Hp * Wp, self.embed_dim)\n            return x_map, (Hp, Wp), x_tokens, (pad_h, pad_w)\n\n        return x_map, (Hp, Wp), (pad_h, pad_w)\n\ndef test_patch_embedding_conv():\n    torch.manual_seed(0)\n\n    #  tamaño divisible (64 con patch=4)\n    B, C, H, W = 2, 3, 64, 64\n    x = torch.randn(B, C, H, W)\n\n    pe = PatchEmbeddingConv(\n        patch_size=4,\n        in_chans=3,\n        embed_dim=192,\n        norm_layer=torch.nn.LayerNorm,\n        pad_if_needed=True,\n        return_tokens=True,)\n\n    x_map, (Hp, Wp), x_tok, (pad_h, pad_w) = pe(x)\n\n    assert x_map.shape == (B, Hp, Wp, 192)\n    assert x_tok.shape == (B, Hp * Wp, 192)\n    assert (pad_h, pad_w) == (0, 0)\n    assert (Hp, Wp) == (H // 4, W // 4)\n\n    print(\"[OK] PatchEmbeddingConv divisible:\",\n          \"x_map\", tuple(x_map.shape),\n          \"| x_tok\", tuple(x_tok.shape),\n          \"| pad\", (pad_h, pad_w))\n\n    # tamaño NO divisible (65x63 con patch=4) -> debería paddear\n    H2, W2 = 65, 63\n    x2 = torch.randn(B, C, H2, W2)\n\n    x_map2, (Hp2, Wp2), x_tok2, (pad_h2, pad_w2) = pe(x2)\n\n    assert (H2 + pad_h2) % 4 == 0\n    assert (W2 + pad_w2) % 4 == 0\n    assert x_map2.shape == (B, Hp2, Wp2, 192)\n    assert x_tok2.shape == (B, Hp2 * Wp2, 192)\n\n    print(\"[OK] PatchEmbeddingConv non-divisible:\",\n          \"input\", (H2, W2),\n          \"| padded by\", (pad_h2, pad_w2),\n          \"| patches\", (Hp2, Wp2),\n          \"| x_map\", tuple(x_map2.shape))\n\n\n\nclass OutlookAttention(nn.Module):\n    \"\"\"\n    Outlook Attention (VOLO): agregación local dinámica sobre ventanas.\n\n    Entrada:  x_map [B, H, W, C]  (channel-last)\n    Salida:   y_map [B, H, W, C]\n\n    Parámetros:\n      - dim: canales C\n      - kernel_size: k (vecindario k×k)\n      - stride: s (si s>1 hace downsample tipo \"outlook pooling\"; para CIFAR típicamente s=1)\n      - num_heads: h (partimos canales en cabezas, como MHSA)\n    \"\"\"\n\n    def __init__(\n        self,\n        dim: int,\n        num_heads: int = 6,\n        kernel_size: int = 3,\n        stride: int = 1,\n        attn_drop: float = 0.0,\n        proj_drop: float = 0.0,):\n\n        super().__init__()\n        assert dim % num_heads == 0, \"dim must be divisible by num_heads\"\n\n        self.dim = dim\n        self.num_heads = num_heads\n        self.head_dim = dim // num_heads\n\n        self.kernel_size = kernel_size\n        self.stride = stride\n\n        # Genera pesos de atención por posición: [B, H, W, heads * k*k]\n        self.attn = nn.Linear(dim, num_heads * kernel_size * kernel_size, bias=True)\n\n        # Proyección para values (antes de unfold)\n        self.v = nn.Linear(dim, dim, bias=True)\n\n        self.attn_drop = nn.Dropout(attn_drop)\n        self.proj = nn.Linear(dim, dim, bias=True)\n        self.proj_drop = nn.Dropout(proj_drop)\n\n    def forward(self, x_map: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        x_map: [B, H, W, C]\n        \"\"\"\n        B, H, W, C = x_map.shape\n        k = self.kernel_size\n        s = self.stride\n        heads = self.num_heads\n        hd = self.head_dim\n\n        # attention weights\n        a = self.attn(x_map)\n        # si stride>1, la atención se evalúa en posiciones downsampled\n        if s > 1:\n            # downsample espacialmente (simple avg pool sobre channel-last)\n            a = a.permute(0, 3, 1, 2)                       # [B, heads*k*k, H, W]\n            a = F.avg_pool2d(a, kernel_size=s, stride=s)    # [B, heads*k*k, Hs, Ws]\n            a = a.permute(0, 2, 3, 1).contiguous()          # [B, Hs, Ws, heads*k*k]\n\n        Hs, Ws = a.shape[1], a.shape[2]\n        a = a.view(B, Hs * Ws, heads, k * k)\n        a = F.softmax(a, dim=-1)\n        a = self.attn_drop(a)\n\n        # values map\n        v = self.v(x_map)\n        v = v.permute(0, 3, 1, 2).contiguous()\n\n        # unfold extrae vecindarios k×k para cada posición (con padding para \"same\")\n        pad = k // 2\n        v_unf = F.unfold(v, kernel_size=k, padding=pad, stride=s)\n        v_unf = v_unf.view(B, C, k * k, Hs * Ws).permute(0, 3, 1, 2).contiguous()\n        v_unf = v_unf.view(B, Hs * Ws, heads, hd, k * k)\n\n        # apply attention: weighted sum over neighborhood\n        # a:     [B, Hs*Ws, heads, k*k]\n        # v_unf: [B, Hs*Ws, heads, hd, k*k]\n        y = (v_unf * a.unsqueeze(3)).sum(dim=-1)\n        y = y.reshape(B, Hs * Ws, C)              # concat heads\n\n        # fold back to spatial map\n        y_map = y.view(B, Hs, Ws, C)\n\n        y_map = self.proj(y_map)\n        y_map = self.proj_drop(y_map)\n        return y_map\n\ndef test_outlook_attention_stride1():\n    torch.manual_seed(0)\n\n    B, H, W, C = 2, 16, 16, 192\n    x_map = torch.randn(B, H, W, C, requires_grad=True)\n\n    oa = OutlookAttention(\n        dim=C,\n        num_heads=6,\n        kernel_size=3,\n        stride=1,\n        attn_drop=0.0,\n        proj_drop=0.0)\n\n    y = oa(x_map)\n    assert y.shape == x_map.shape, f\"Expected {x_map.shape}, got {y.shape}\"\n\n    loss = y.mean()\n    loss.backward()\n\n    assert x_map.grad is not None, \"No gradient flowed to input!\"\n    assert torch.isfinite(x_map.grad).all(), \"Non-finite grads!\"\n\n    print(\"[OK] OutlookAttention stride=1:\",\n          \"in\", tuple(x_map.shape),\n          \"| out\", tuple(y.shape),\n          \"| grad mean\", float(x_map.grad.abs().mean()))\n\n\n\ndef test_outlook_attention_stride2():\n    torch.manual_seed(0)\n\n    B, H, W, C = 2, 16, 16, 192\n    x_map = torch.randn(B, H, W, C, requires_grad=True)\n\n    oa = OutlookAttention(\n        dim=C,\n        num_heads=6,\n        kernel_size=3,\n        stride=2,\n        attn_drop=0.0,\n        proj_drop=0.0)\n\n    y = oa(x_map)\n\n    assert y.shape[0] == B and y.shape[-1] == C\n    assert y.shape[1] == H // 2 and y.shape[2] == W // 2, f\"Got {y.shape[1:3]}\"\n\n    loss = y.mean()\n    loss.backward()\n    assert x_map.grad is not None\n    assert torch.isfinite(x_map.grad).all()\n\n    print(\"[OK] OutlookAttention stride=2:\",\n          \"in\", (B, H, W, C),\n          \"| out\", tuple(y.shape))\n\n\n\nclass DropPath(nn.Module):\n    def __init__(self, drop_prob: float = 0.0):\n        super().__init__()\n        self.drop_prob = float(drop_prob)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        if self.drop_prob == 0.0 or not self.training:\n            return x\n\n        keep_prob = 1.0 - self.drop_prob\n        shape = (x.shape[0],) + (1,) * (x.ndim - 1)\n        random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n        random_tensor.floor_()\n        return x.div(keep_prob) * random_tensor\n\n\nclass MLP(nn.Module):\n    def __init__(self, dim: int, hidden_dim: int, drop: float = 0.0):\n        super().__init__()\n        self.fc1 = nn.Linear(dim, hidden_dim)\n        self.act = nn.GELU()\n        self.fc2 = nn.Linear(hidden_dim, dim)\n        self.drop = nn.Dropout(drop)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.drop(x)\n        x = self.fc2(x)\n        x = self.drop(x)\n        return x\n\nclass OutlookerBlock(nn.Module):\n    \"\"\"\n    Bloque VOLO Outlooker:\n      x -> LN -> OutlookAttention -> DropPath + residual\n        -> LN -> MLP -> DropPath + residual\n\n    Input/Output: [B, H, W, C]\n    \"\"\"\n    def __init__(\n        self,\n        dim: int,\n        num_heads: int,\n        kernel_size: int = 3,\n        stride: int = 1,\n        mlp_ratio: float = 4.0,\n        attn_drop: float = 0.0,\n        proj_drop: float = 0.0,\n        drop_path: float = 0.0,\n        mlp_drop: float = 0.0):\n\n        super().__init__()\n        self.norm1 = nn.LayerNorm(dim)\n\n        self.attn = OutlookAttention(\n            dim=dim,\n            num_heads=num_heads,\n            kernel_size=kernel_size,\n            stride=stride,\n            attn_drop=attn_drop,\n            proj_drop=proj_drop,)\n\n        self.drop_path1 = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n\n        self.norm2 = nn.LayerNorm(dim)\n        hidden_dim = int(dim * mlp_ratio)\n\n        self.mlp = MLP(dim=dim, hidden_dim=hidden_dim, drop=mlp_drop)\n        self.drop_path2 = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n\n    def forward(self, x_map: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        x_map: tensor de forma (B, C, H, W) o (B, N, C), según el bloque.\n        \"\"\"\n\n        # Primer sub-bloque: Norm -> Atención -> DropPath -> Residual\n\n        # Normalización del input\n        x_norm_1 = self.norm1(x_map)\n\n        # Atención\n        attn_out = self.attn(x_norm_1)\n        attn_out = self.drop_path1(attn_out)\n\n        # Suma residual\n        x_map = x_map + attn_out\n\n        # Segundo sub-bloque: Norm -> MLP -> DropPath -> Residual ---\n\n        x_norm_2 = self.norm2(x_map)\n\n        # MLP\n        mlp_out = self.mlp(x_norm_2)\n        mlp_out = self.drop_path2(mlp_out)\n\n        # Segunda suma residual\n        x_out = x_map + mlp_out\n\n        return x_out\n\ndef test_outlooker_block():\n    torch.manual_seed(0)\n\n    B, H, W, C = 2, 16, 16, 192\n    x_map = torch.randn(B, H, W, C, requires_grad=True)\n\n    blk = OutlookerBlock(\n        dim=C,\n        num_heads=6,\n        kernel_size=3,\n        stride=1,\n        mlp_ratio=4.0,\n        attn_drop=0.0,\n        proj_drop=0.0,\n        drop_path=0.0,\n        mlp_drop=0.0,)\n\n    y = blk(x_map)\n    assert y.shape == x_map.shape\n\n    y.mean().backward()\n    assert x_map.grad is not None\n    assert torch.isfinite(x_map.grad).all()\n\n    print(\"[OK] OutlookerBlock:\",\n          \"in/out\", tuple(y.shape),\n          \"| grad mean\", float(x_map.grad.abs().mean()))\n\n\ndef test_embed_then_outlook(img_size=64, patch_size=4, dim=192, heads=6):\n    torch.manual_seed(0)\n\n    B = 2\n    x = torch.randn(B, 3, img_size, img_size, requires_grad=True)\n\n    pe = PatchEmbeddingConv(\n        patch_size=patch_size,\n        in_chans=3,\n        embed_dim=dim,\n        norm_layer=torch.nn.LayerNorm,\n        pad_if_needed=True,\n        return_tokens=True,)\n\n    blk = OutlookerBlock(\n        dim=dim,\n        num_heads=heads,\n        kernel_size=3,\n        stride=1,\n        mlp_ratio=4.0,\n        drop_path=0.0,)\n\n    x_map, (Hp, Wp), x_tok, pad_hw = pe(x)\n    y_map = blk(x_map)\n\n    assert y_map.shape == x_map.shape == (B, Hp, Wp, dim)\n\n    # grad\n    y_map.mean().backward()\n    assert x.grad is not None and torch.isfinite(x.grad).all()\n\n    print(\"[OK] Embed->Outlook:\",\n          \"img\", (img_size, img_size),\n          \"| patches\", (Hp, Wp),\n          \"| map\", tuple(y_map.shape),\n          \"| pad\", pad_hw)\n\n\n\nclass VOLOStage(nn.Module):\n    \"\"\"\n    Un stage VOLO basado en OutlookerBlocks.\n\n    Mantiene el formato channel-last:\n      Input:  [B, H, W, C]\n      Output: [B, H, W, C]  (si stride=1)\n    Si quisieras un stage que haga downsample, usa stride>1 en los bloques\n    (pero en CIFAR te recomiendo stride=1 en el stage inicial).\n    \"\"\"\n\n    def __init__(\n        self,\n        dim: int,\n        depth: int,\n        num_heads: int,\n        kernel_size: int = 3,\n        stride: int = 1,\n        mlp_ratio: float = 4.0,\n        attn_drop: float = 0.0,\n        proj_drop: float = 0.0,\n        drop_path: float | list[float] = 0.0,\n        mlp_drop: float = 0.0,):\n\n        super().__init__()\n\n        if isinstance(drop_path, float):\n            dpr = [drop_path] * depth\n        else:\n            assert len(drop_path) == depth, \"drop_path list must have length=depth\"\n            dpr = drop_path\n\n        self.blocks = nn.ModuleList([\n            OutlookerBlock(\n                dim=dim,\n                num_heads=num_heads,\n                kernel_size=kernel_size,\n                stride=stride,\n                mlp_ratio=mlp_ratio,\n                attn_drop=attn_drop,\n                proj_drop=proj_drop,\n                drop_path=dpr[i],\n                mlp_drop=mlp_drop,) for i in range(depth)])\n\n    def forward(self, x_map: torch.Tensor) -> torch.Tensor:\n        for blk in self.blocks:\n            x_map = blk(x_map)\n        return x_map\n\ndef test_volo_stage():\n    torch.manual_seed(0)\n\n    B, H, W, C = 2, 16, 16, 192\n    x = torch.randn(B, H, W, C, requires_grad=True)\n\n    stage = VOLOStage(\n        dim=C,\n        depth=3,\n        num_heads=6,\n        kernel_size=3,\n        stride=1,\n        drop_path=[0.0, 0.05, 0.1])\n\n    y = stage(x)\n    assert y.shape == x.shape\n    y.mean().backward()\n    assert x.grad is not None and torch.isfinite(x.grad).all()\n\n    print(\"[OK] VOLOStage:\", tuple(y.shape), \"| grad mean\", float(x.grad.abs().mean()))\n\n\n\n\"\"\"## Attention\"\"\"\n\ndef scaled_dot_product_attention(q, k, v, mask=None, attn_dropout_p: float = 0.0, training: bool = True):\n    \"\"\"\n    q: (B, H, Lq, d)\n    k: (B, H, Lk, d)\n    v: (B, H, Lk, d)\n    mask: broadcastable a (B, H, Lq, Lk)\n          - bool: True = BLOQUEAR (poner -inf)\n          - float: 1.0 = permitir, 0.0 = bloquear\n    \"\"\"\n    scores = torch.matmul(q, k.transpose(-2, -1))\n    dk = q.size(-1)\n    scores = scores / (dk ** 0.5)\n\n    if mask is not None:\n        if mask.dtype == torch.bool:\n            scores = scores.masked_fill(mask, float(\"-inf\"))\n        else:\n            scores = scores.masked_fill(mask <= 0, float(\"-inf\"))\n\n    attn = F.softmax(scores, dim=-1)\n    if attn_dropout_p > 0.0:\n        attn = F.dropout(attn, p=attn_dropout_p, training=training)\n\n    output = torch.matmul(attn, v)\n    return output, attn\n\nclass MultiHeadAttention(nn.Module):\n    def __init__(self, d_model: int, num_heads: int, dropout: float = 0.1):\n        super().__init__()\n        assert d_model % num_heads == 0, \"d_model debe ser múltiplo de num_heads\"\n\n        self.num_heads = num_heads\n        self.d_head = d_model // num_heads\n        self.d_model = d_model\n\n        self.w_q = nn.Linear(d_model, d_model)\n        self.w_k = nn.Linear(d_model, d_model)\n        self.w_v = nn.Linear(d_model, d_model)\n        self.w_o = nn.Linear(d_model, d_model)\n\n        # \"dropout\" lo usaremos como dropout de atención (sobre attn)\n        self.attn_dropout_p = dropout\n        # y también dejamos dropout de salida si quieres (común en ViT)\n        self.out_dropout = nn.Dropout(dropout)\n\n    def _split_heads(self, x):\n        B, L, _ = x.shape\n        return x.view(B, L, self.num_heads, self.d_head).transpose(1, 2)\n\n    def _combine_heads(self, x):\n        B, H, L, D = x.shape\n        return x.transpose(1, 2).contiguous().view(B, L, H * D)\n\n    def forward(self, x_q, x_kv, mask=None):\n        q = self._split_heads(self.w_q(x_q))\n        k = self._split_heads(self.w_k(x_kv))\n        v = self._split_heads(self.w_v(x_kv))\n\n        if mask is not None:\n            if mask.dim() == 2:\n                mask = mask[:, None, None, :]\n            elif mask.dim() == 3:\n                mask = mask[:, None, :, :]\n            elif mask.dim() == 4:\n                pass\n            else:\n                raise ValueError(f\"Máscara con dims no soportadas: {mask.shape}\")\n\n            if mask.dtype != torch.bool:\n                mask = (mask <= 0)\n\n        attn_out, _ = scaled_dot_product_attention(\n            q, k, v,\n            mask=mask,\n            attn_dropout_p=self.attn_dropout_p,\n            training=self.training)\n\n        attn_out = self._combine_heads(attn_out)\n\n        attn_out = self.w_o(attn_out)\n        attn_out = self.out_dropout(attn_out)\n        return attn_out\n\nclass FeedForward(nn.Module):\n    def __init__(self, dim: int, hidden_dim: int, dropout: float = 0.1):\n        super().__init__()\n        self.fc1 = nn.Linear(dim, hidden_dim)\n        self.act = nn.GELU()\n        self.fc2 = nn.Linear(hidden_dim, dim)\n        self.dropout = nn.Dropout(dropout)\n\n    def forward(self, x):\n        x = self.fc1(x)\n        x = self.act(x)\n        x = self.dropout(x)\n        x = self.fc2(x)\n        x = self.dropout(x)\n        return x\n\n\nclass TransformerBlock(nn.Module):\n    \"\"\"\n    Bloque encoder para ViT (pre-norm):\n    x -> LN -> MHA -> DropPath -> +residual\n       -> LN -> MLP -> DropPath -> +residual\n    \"\"\"\n    def __init__(\n        self,\n        dim: int,\n        num_heads: int,\n        mlp_ratio: float = 4.0,\n        attn_dropout: float = 0.0,\n        dropout: float = 0.1,\n        drop_path: float = 0.0):\n\n        super().__init__()\n        self.norm1 = nn.LayerNorm(dim)\n        self.attn = MultiHeadAttention(d_model=dim, num_heads=num_heads, dropout=attn_dropout)\n        self.drop_path1 = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n\n        self.norm2 = nn.LayerNorm(dim)\n        hidden_dim = int(dim * mlp_ratio)\n        self.mlp = FeedForward(dim, hidden_dim, dropout=dropout)\n        self.drop_path2 = DropPath(drop_path) if drop_path > 0.0 else nn.Identity()\n\n    def forward(self, x):\n        x = x + self.drop_path1(self.attn(self.norm1(x), self.norm1(x), mask=None))\n        x = x + self.drop_path2(self.mlp(self.norm2(x)))\n        return x\n\nclass TransformerStack(nn.Module):\n    \"\"\"Stack simple de TransformerBlock sobre tokens [B, N, C].\"\"\"\n    def __init__(self, dim: int, depth: int, num_heads: int, mlp_ratio=4.0,\n                 attn_dropout=0.0, dropout=0.1, drop_path: float | list[float] = 0.0):\n        super().__init__()\n        if isinstance(drop_path, float):\n            dpr = [drop_path] * depth\n        else:\n            assert len(drop_path) == depth\n            dpr = drop_path\n\n        self.blocks = nn.ModuleList([\n            TransformerBlock(\n                dim=dim,\n                num_heads=num_heads,\n                mlp_ratio=mlp_ratio,\n                attn_dropout=attn_dropout,\n                dropout=dropout,\n                drop_path=dpr[i] if \"drop_path\" in TransformerBlock.__init__.__code__.co_varnames else 0.0) for i in range(depth)])\n\n    def forward(self, x_tok: torch.Tensor) -> torch.Tensor:\n        for blk in self.blocks:\n            x_tok = blk(x_tok)\n        return x_tok\n\ndef test_transformer_block():\n    torch.manual_seed(0)\n    B, N, C = 2, 256, 192\n    x = torch.randn(B, N, C, requires_grad=True)\n\n    blk = TransformerBlock(dim=C, num_heads=6, mlp_ratio=4.0, attn_dropout=0.0, dropout=0.1, drop_path=0.0)\n    y = blk(x)\n    assert y.shape == x.shape\n    y.mean().backward()\n    assert x.grad is not None and torch.isfinite(x.grad).all()\n    print(\"[OK] TransformerBlock:\", tuple(y.shape), \"grad\", float(x.grad.abs().mean()))\n\n\n\"\"\"# Hiratical\"\"\"\n\nclass MapDownsample(nn.Module):\n    \"\"\"\n    Downsample para mapas channel-last: [B, H, W, C_in] -> [B, H/2, W/2, C_out]\n    usando conv2d stride=2 en formato channel-first internamente.\n    \"\"\"\n    def __init__(self, dim_in: int, dim_out: int, kernel_size: int = 3, norm_layer=nn.LayerNorm):\n        super().__init__()\n        pad = kernel_size // 2\n        self.conv = nn.Conv2d(dim_in, dim_out, kernel_size=kernel_size, stride=2, padding=pad, bias=True)\n        self.norm = norm_layer(dim_out) if norm_layer is not None else None\n\n    def forward(self, x_map: torch.Tensor):\n        # x_map: [B, H, W, C_in]\n        B, H, W, C = x_map.shape\n        x = x_map.permute(0, 3, 1, 2).contiguous()     # [B, C, H, W]\n        x = self.conv(x)                               # [B, C_out, H2, W2]\n        x_map = x.permute(0, 2, 3, 1).contiguous()     # [B, H2, W2, C_out]\n        if self.norm is not None:\n            x_map = self.norm(x_map)\n        return x_map\n\nclass PoolingLayer(nn.Module):\n    \"\"\"\n    Pooling jerárquico para ViT:\n\n    - Toma tokens [B, N, D_in] + grid_size (H, W)\n    - Los reinterpreta como feature map [B, D_in, H, W]\n    - Aplica:\n        depthwise conv (3x3, stride=2, padding=1)\n        pointwise conv (1x1) para cambiar D_in -> D_out\n    - Devuelve:\n        tokens [B, N_out, D_out] y nuevo grid_size (H_out, W_out)\n    \"\"\"\n\n    def __init__(self,\n        dim_in: int,\n        dim_out: int,\n        kernel_size: int = 3,\n        stride: int = 2,\n        norm_layer: type[nn.Module] | None = nn.LayerNorm):\n\n        super().__init__()\n        padding = kernel_size // 2\n\n        # Depthwise conv: cada canal se filtra por separado\n        self.depthwise_conv = nn.Conv2d(\n            in_channels=dim_in,\n            out_channels=dim_in,\n            kernel_size=kernel_size,\n            stride=stride,\n            padding=padding,\n            groups=dim_in)\n\n        # Pointwise conv: mezcla canales y cambia dim\n        self.pointwise_conv = nn.Conv2d(\n            in_channels=dim_in,\n            out_channels=dim_out,\n            kernel_size=1,\n            stride=1,\n            padding=0)\n\n        self.norm = norm_layer(dim_out) if norm_layer is not None else None\n\n        self.dim_in = dim_in\n        self.dim_out = dim_out\n        self.stride = stride\n\n    def forward(self, x: torch.Tensor, grid_size: tuple[int, int]):\n        \"\"\"\n        Args:\n            x: tokens [B, N, D_in]\n            grid_size: (H, W) tal que H*W = N\n\n        Returns:\n            x_out: tokens [B, N_out, D_out]\n            new_grid: (H_out, W_out)\n        \"\"\"\n        B, N, D_in = x.shape\n        H, W = grid_size\n\n        assert D_in == self.dim_in, f\"dim_in {D_in} != {self.dim_in}\"\n        assert H * W == N, f\"H*W={H*W} no coincide con N={N}\"\n\n        # [B, N, D_in] -> [B, D_in, H, W]\n        x = x.view(B, H, W, D_in).permute(0, 3, 1, 2)\n\n        # Depthwise + pointwise\n        x = self.depthwise_conv(x)\n        x = self.pointwise_conv(x)\n\n        B, D_out, H_out, W_out = x.shape\n        N_out = H_out * W_out\n\n        # Volver a tokens: [B, D_out, H_out, W_out] -> [B, N_out, D_out]\n        x = x.flatten(2).transpose(1, 2)\n\n        if self.norm is not None:\n            x = self.norm(x)\n\n        new_grid = (H_out, W_out)\n        return x, new_grid\n\n\"\"\"# VOLO BackBone\"\"\"\n\ndef map_to_tokens(x_map: torch.Tensor) -> torch.Tensor:\n    B, H, W, C = x_map.shape\n    return x_map.view(B, H * W, C)\n\ndef tokens_to_map(x_tok: torch.Tensor, H: int, W: int) -> torch.Tensor:\n    B, N, C = x_tok.shape\n    assert N == H * W\n    return x_tok.view(B, H, W, C)\n\nclass VOLOPyramid(nn.Module):\n    \"\"\"\n    Backbone jerárquico para VOLO (sin classifier head aún).\n    - Local: VOLOStage (Outlooker)\n    - Global: TransformerStack (opcional)\n    - Downsample: map-space (recomendado) o token-space (PoolingLayer tuyo)\n    \"\"\"\n    def __init__(\n        self,\n        dims: tuple[int, ...],                 # ej (192, 256, 384)\n        outlooker_depths: tuple[int, ...],     # ej (4, 2, 0)  (0 si no hay outlooker en ese nivel)\n        outlooker_heads: tuple[int, ...],      # ej (6, 8, 12)\n        transformer_depths: tuple[int, ...],   # ej (0, 4, 6)\n        transformer_heads: tuple[int, ...],    # ej (6, 8, 12)\n        kernel_size: int = 3,\n        mlp_ratio: float = 4.0,\n        downsample_kind: str = \"map\",          # \"map\" o \"token\"\n        drop_path_rate: float = 0.0):\n\n        super().__init__()\n        L = len(dims)\n\n        assert len(outlooker_depths) == L\n        assert len(outlooker_heads) == L\n        assert len(transformer_depths) == L\n        assert len(transformer_heads) == L\n\n        # schedule lineal de droppath a través de todos los bloques (local+global)\n        total_blocks = sum(outlooker_depths) + sum(transformer_depths)\n        dpr = torch.linspace(0, drop_path_rate, total_blocks).tolist() if total_blocks > 0 else []\n        dp_i = 0\n\n        self.levels = nn.ModuleList()\n        self.downsamples = nn.ModuleList()\n        self.downsample_kind = downsample_kind\n\n        for i in range(L):\n            dim = dims[i]\n\n            # Local stage (Outlooker)\n            local = None\n            if outlooker_depths[i] > 0:\n                local_dpr = dpr[dp_i: dp_i + outlooker_depths[i]]\n                dp_i += outlooker_depths[i]\n                local = VOLOStage(\n                    dim=dim,\n                    depth=outlooker_depths[i],\n                    num_heads=outlooker_heads[i],\n                    kernel_size=kernel_size,\n                    stride=1,\n                    mlp_ratio=mlp_ratio,\n                    drop_path=local_dpr)\n\n            # Global stage (Transformer)\n            global_ = None\n            if transformer_depths[i] > 0:\n                glob_dpr = dpr[dp_i: dp_i + transformer_depths[i]]\n                dp_i += transformer_depths[i]\n\n                global_ = TransformerStack(\n                    dim=dim,\n                    depth=transformer_depths[i],\n                    num_heads=transformer_heads[i],\n                    mlp_ratio=mlp_ratio,\n                    attn_dropout=0.0,\n                    dropout=0.1,\n                    drop_path=glob_dpr,)\n\n            self.levels.append(nn.ModuleDict({\"local\": local, \"global\": global_}))\n\n            # Downsample para pasar dim_i -> dim_{i+1} (si no es el último nivel)\n            if i < L - 1:\n                if downsample_kind == \"map\":\n                    self.downsamples.append(MapDownsample(dim_in=dim, dim_out=dims[i + 1], kernel_size=3))\n                elif downsample_kind == \"token\":\n                    # reusar PoolingLayer\n                    self.downsamples.append(PoolingLayer(dim_in=dim, dim_out=dims[i + 1], kernel_size=3, stride=2))\n                else:\n                    raise ValueError(f\"downsample_kind must be 'map' or 'token'. Got {downsample_kind}\")\n\n        assert dp_i == total_blocks\n\n    def forward(self, x_map: torch.Tensor):\n        \"\"\"\n        x_map: [B, H, W, C0]\n        returns:\n          x_final_tokens: [B, N_last, C_last]\n          last_grid: (H_last, W_last)\n        \"\"\"\n        B, H, W, C = x_map.shape\n\n        for i, lvl in enumerate(self.levels):\n            # local stage en map\n            if lvl[\"local\"] is not None:\n                x_map = lvl[\"local\"](x_map)\n\n            # global stage en tokens (si existe)\n            if lvl[\"global\"] is not None:\n                x_tok = map_to_tokens(x_map)\n                x_tok = lvl[\"global\"](x_tok)\n                x_map = tokens_to_map(x_tok, H, W)\n\n            # downsample (si aplica)\n            if i < len(self.downsamples):\n                ds = self.downsamples[i]\n                if self.downsample_kind == \"map\":\n                    x_map = ds(x_map)\n                    H, W = x_map.shape[1], x_map.shape[2]\n                else:\n                    # token downsample: necesita grid\n                    x_tok = map_to_tokens(x_map)\n                    x_tok, (H, W) = ds(x_tok, (H, W))\n                    x_map = tokens_to_map(x_tok, H, W)\n\n        x_final = map_to_tokens(x_map)\n        return x_final, (H, W)\n\n\ndef test_volo_pyramid_map():\n    torch.manual_seed(0)\n    B = 2\n    H = W = 16\n    x_map = torch.randn(B, H, W, 192)\n\n    pyr = VOLOPyramid(\n        dims=(192, 256, 384),\n        outlooker_depths=(2, 2, 0),\n        outlooker_heads=(6, 8, 12),\n        transformer_depths=(0, 2, 2),\n        transformer_heads=(6, 8, 12),\n        downsample_kind=\"map\",\n        drop_path_rate=0.1,)\n\n    x_tok, (Hf, Wf) = pyr(x_map)\n    print(\"[OK] Pyramid-map:\", x_tok.shape, \"grid\", (Hf, Wf))\n    assert x_tok.shape[0] == B\n    assert x_tok.shape[2] == 384\n    assert Hf * Wf == x_tok.shape[1]\n\n\n\ndef test_volo_pyramid_token():\n    torch.manual_seed(0)\n    B = 2\n    H = W = 16\n    x_map = torch.randn(B, H, W, 192)\n\n    pyr = VOLOPyramid(\n        dims=(192, 256, 384),\n        outlooker_depths=(2, 2, 0),\n        outlooker_heads=(6, 8, 12),\n        transformer_depths=(0, 2, 2),\n        transformer_heads=(6, 8, 12),\n        downsample_kind=\"token\",\n        drop_path_rate=0.1)\n\n    x_tok, (Hf, Wf) = pyr(x_map)\n    print(\"[OK] Pyramid-token:\", x_tok.shape, \"grid\", (Hf, Wf))\n    assert x_tok.shape[2] == 384\n    assert Hf * Wf == x_tok.shape[1]\n\n\n\nclass ClassAttention(nn.Module):\n    \"\"\"\n    Class Attention: sólo el CLS atiende al conjunto [CLS | tokens].\n    Inputs:\n      cls:    [B, 1, C]\n      tokens: [B, N, C]\n    Output:\n      cls_out: [B, 1, C]\n    \"\"\"\n    def __init__(self, dim: int, num_heads: int, attn_dropout: float = 0.0, proj_dropout: float = 0.0):\n        super().__init__()\n        self.attn = MultiHeadAttention(d_model=dim, num_heads=num_heads, dropout=attn_dropout)\n        self.proj_drop = nn.Dropout(proj_dropout)\n\n    def forward(self, cls: torch.Tensor, tokens: torch.Tensor) -> torch.Tensor:\n        kv = torch.cat([cls, tokens], dim=1)        # [B, 1+N, C]\n        cls_out = self.attn(cls, kv, mask=None) # [B, 1, C] (solo CLS sale actualizado)\n        return self.proj_drop(cls_out)\n\n\nclass ClassAttentionBlock(nn.Module):\n    \"\"\"\n    Pre-norm (CaiT-style):\n      cls -> LN -> ClassAttn(cls, [cls|tokens]) -> +res\n          -> LN -> MLP -> +res\n    Nota: tokens NO se actualizan.\n    \"\"\"\n    def __init__(\n        self,\n        dim: int,\n        num_heads: int,\n        mlp_ratio: float = 4.0,\n        attn_dropout: float = 0.0,\n        dropout: float = 0.0):\n\n        super().__init__()\n        self.norm_cls = nn.LayerNorm(dim)\n        self.norm_tok = nn.LayerNorm(dim)\n        self.ca = ClassAttention(dim, num_heads, attn_dropout=attn_dropout, proj_dropout=dropout)\n\n        self.norm2 = nn.LayerNorm(dim)\n        self.mlp = FeedForward(dim, int(dim * mlp_ratio), dropout=dropout)\n\n    def forward(self, cls: torch.Tensor, tokens: torch.Tensor) -> torch.Tensor:\n        # Class attention update (solo CLS)\n        cls_norm = self.norm_cls(cls)\n        tok_norm = self.norm_tok(tokens)\n        cls = cls + self.ca(cls_norm, tok_norm)\n\n        # MLP update (solo CLS)\n        cls = cls + self.mlp(self.norm2(cls))\n        return cls\n\nclass CLIPool(nn.Module):\n    \"\"\"\n    \"CLI\" style pooling: mezcla aprendible entre CLS y mean(tokens).\n      z = alpha * cls + (1-alpha) * mean\n    \"\"\"\n    def __init__(self, init_alpha: float = 0.5):\n        super().__init__()\n        # parametriza alpha en logits para mantenerlo en (0,1)\n        init_alpha = float(init_alpha)\n        init_alpha = min(max(init_alpha, 1e-4), 1 - 1e-4)\n        logit = math.log(init_alpha / (1 - init_alpha))\n        self.alpha_logit = nn.Parameter(torch.tensor([logit], dtype=torch.float32))\n\n    def forward(self, cls_vec: torch.Tensor, tok_mean: torch.Tensor) -> torch.Tensor:\n        \"\"\"\n        cls_vec:  [B, C]\n        tok_mean: [B, C]\n        \"\"\"\n        alpha = torch.sigmoid(self.alpha_logit)  # scalar in (0,1)\n        return alpha * cls_vec + (1.0 - alpha) * tok_mean\n\n\"\"\"# VOLO\"\"\"\n\nimport math\n\ndef trunc_normal_(tensor, mean=0., std=1.):\n    with torch.no_grad():\n        return tensor.normal_(mean=mean, std=std)\n\nclass PosEmbed2D(nn.Module):\n    \"\"\"\n    Positional embedding aprendible para grilla (H, W) en tokens.\n\n    Guarda [1, H*W, C]. Si en forward llega otro (H,W), interpola.\n    \"\"\"\n    def __init__(self, H: int, W: int, dim: int):\n        super().__init__()\n        self.H0 = H\n        self.W0 = W\n        self.dim = dim\n        self.pos = nn.Parameter(torch.zeros(1, H * W, dim))\n        trunc_normal_(self.pos, std=0.02)\n\n    def forward(self, x_tok: torch.Tensor, grid: tuple[int, int]):\n        \"\"\"\n        x_tok: [B, N, C]\n        grid: (H, W)\n        \"\"\"\n        B, N, C = x_tok.shape\n        H, W = grid\n        if (H == self.H0) and (W == self.W0):\n            return x_tok + self.pos\n\n        # Interpola pos emb como mapa [1, C, H, W] -> nuevo tamaño\n        pos = self.pos.reshape(1, self.H0, self.W0, self.dim).permute(0, 3, 1, 2)  # [1,C,H0,W0]\n        pos = nn.functional.interpolate(pos, size=(H, W), mode=\"bicubic\", align_corners=False)\n        pos = pos.permute(0, 2, 3, 1).reshape(1, H * W, self.dim)\n        return x_tok + pos\n\nclass VOLOClassifier(nn.Module):\n    \"\"\"\n    VOLO para CIFAR-100 (y similares), con dos modos:\n      - flat: OutlookerStage -> TransformerStack (sin downsample)\n              pooling: mean | cls | cli (cls via class-attn final)\n      - hierarchical: pirámide con downsample (map o token)\n              pooling: SOLO mean (por ahora)\n\n    Flujo base:\n      x [B,3,H,W]\n        -> PatchEmbeddingConv -> x_tok [B, N, C0]\n        -> pos emb (opcional)\n        -> backbone (flat o pyramid)\n        -> pooling\n        -> head\n    \"\"\"\n\n    def __init__(\n        self,\n        num_classes: int = 100,\n        img_size: int = 32,\n        in_chans: int = 3,\n        patch_size: int = 4,\n\n        # mode\n        hierarchical: bool = False,\n        downsample_kind: str = \"map\",   # si hierarchical=True: \"map\" o \"token\"\n\n        # dims / depths (flat)\n        embed_dim: int = 192,\n        outlooker_depth: int = 4,\n        outlooker_heads: int = 6,\n        transformer_depth: int = 6,\n        transformer_heads: int = 6,\n\n        # hierarchical configs (si hierarchical=True)\n        dims: tuple[int, ...] = (192, 256, 384),\n        outlooker_depths: tuple[int, ...] = (2, 2, 0),\n        outlooker_heads_list: tuple[int, ...] = (6, 8, 12),\n        transformer_depths: tuple[int, ...] = (0, 2, 2),\n        transformer_heads_list: tuple[int, ...] = (6, 8, 12),\n\n        # block hyperparams\n        kernel_size: int = 3,\n        mlp_ratio: float = 4.0,\n        dropout: float = 0.1,\n        attn_dropout: float = 0.0,\n        drop_path_rate: float = 0.0,\n\n        # head / pooling\n        pooling: str = \"mean\",          # flat: \"mean\"|\"cls\"|\"cli\" ; hierarchical: \"mean\"\n        use_pos_embed: bool = True,\n\n        # cls refinamiento (flat)\n        cls_attn_depth: int = 2,        # # capas ClassAttentionBlock\n        cli_init_alpha: float = 0.5,    # init alpha para pooling=\"cli\"\n        use_cls_pos: bool = True):\n\n        super().__init__()\n\n        self.hierarchical = hierarchical\n        self.use_pos_embed = use_pos_embed\n\n        if self.hierarchical:\n            assert pooling == \"mean\", \"Por ahora hierarchical solo soporta pooling='mean'.\"\n        else:\n            assert pooling in [\"mean\", \"cls\", \"cli\"], \"pooling en flat debe ser 'mean', 'cls' o 'cli'.\"\n        self.pooling = pooling\n\n        # ---- Patch Embedding ----\n        C0 = (dims[0] if hierarchical else embed_dim)\n\n        self.patch_embed = PatchEmbeddingConv(\n            patch_size=patch_size,\n            in_chans=in_chans,\n            embed_dim=C0,\n            norm_layer=nn.LayerNorm,\n            pad_if_needed=True,\n            return_tokens=True,)\n\n        Hp0 = math.ceil(img_size / patch_size)\n        Wp0 = math.ceil(img_size / patch_size)\n\n        self.pos_embed = PosEmbed2D(Hp0, Wp0, C0) if use_pos_embed else None\n        self.pos_drop = nn.Dropout(dropout)\n\n        # ---- Backbone ----\n        if not hierarchical:\n            total = outlooker_depth + transformer_depth\n            dpr = torch.linspace(0, drop_path_rate, total).tolist() if total > 0 else []\n            dpr_local = dpr[:outlooker_depth]\n            dpr_glob = dpr[outlooker_depth:]\n\n            self.local_stage = VOLOStage(\n                dim=embed_dim,\n                depth=outlooker_depth,\n                num_heads=outlooker_heads,\n                kernel_size=kernel_size,\n                stride=1,\n                mlp_ratio=mlp_ratio,\n                attn_drop=attn_dropout,\n                proj_drop=dropout,\n                drop_path=dpr_local if len(dpr_local) else 0.0,\n                mlp_drop=dropout)\n\n            self.global_blocks = nn.ModuleList([\n                TransformerBlock(\n                    dim=embed_dim,\n                    num_heads=transformer_heads,\n                    mlp_ratio=mlp_ratio,\n                    attn_dropout=attn_dropout,\n                    dropout=dropout,\n                    drop_path=(dpr_glob[i] if len(dpr_glob) else 0.0),\n                ) for i in range(transformer_depth)])\n\n            # --- CLS  (solo si pooling usa cls/cli) ---\n            self.use_cls = (pooling in [\"cls\", \"cli\"])\n            if self.use_cls:\n                self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n                trunc_normal_(self.cls_token, std=0.02)\n\n                self.cls_pos = None\n                if use_cls_pos:\n                    self.cls_pos = nn.Parameter(torch.zeros(1, 1, embed_dim))\n                    trunc_normal_(self.cls_pos, std=0.02)\n\n                self.cls_attn_blocks = nn.ModuleList([\n                    ClassAttentionBlock(\n                        dim=embed_dim,\n                        num_heads=transformer_heads,\n                        mlp_ratio=mlp_ratio,\n                        attn_dropout=attn_dropout,\n                        dropout=dropout,) for _ in range(int(cls_attn_depth))])\n\n                self.cli_pool = CLIPool(init_alpha=cli_init_alpha) if pooling == \"cli\" else None\n            else:\n                self.cls_token = None\n                self.cls_pos = None\n                self.cls_attn_blocks = None\n                self.cli_pool = None\n\n\n            self.norm = nn.LayerNorm(embed_dim)\n            self.norm_feat = nn.LayerNorm(embed_dim)\n\n            self.head = nn.Linear(embed_dim, num_classes)\n\n        else:\n            self.pyramid = VOLOPyramid(\n                dims=dims,\n                outlooker_depths=outlooker_depths,\n                outlooker_heads=outlooker_heads_list,\n                transformer_depths=transformer_depths,\n                transformer_heads=transformer_heads_list,\n                kernel_size=kernel_size,\n                mlp_ratio=mlp_ratio,\n                downsample_kind=downsample_kind,\n                drop_path_rate=drop_path_rate,)\n\n\n            self.norm = nn.LayerNorm(dims[-1])\n            self.norm_feat = nn.LayerNorm(dims[-1])\n            self.head = nn.Linear(dims[-1], num_classes)\n\n    def forward(self, x: torch.Tensor) -> torch.Tensor:\n        # Patch embedding\n        x_map, (Hp, Wp), x_tok, _pad = self.patch_embed(x)   # x_tok [B,N,C0]\n        B, N, C0 = x_tok.shape\n\n        # Pos emb sobre tokens del grid\n        if self.use_pos_embed and (self.pos_embed is not None):\n            x_tok = self.pos_embed(x_tok, (Hp, Wp))\n        x_tok = self.pos_drop(x_tok)\n\n        if not self.hierarchical:\n            # ---- Flat backbone ----\n            # Outlooker trabaja en map (sin CLS)\n            x_map = x_tok.view(B, Hp, Wp, C0)\n            x_map = self.local_stage(x_map)\n            x_tok = x_map.view(B, Hp * Wp, C0)  # [B,N,C]\n\n            # Transformer global (tokens sin CLS)\n            for blk in self.global_blocks:\n                x_tok = blk(x_tok)\n\n            #  Pooling\n            if self.pooling == \"mean\":\n                # Normaliza tokens y promedia\n                x_tok_n = self.norm(x_tok)           # [B,N,C]\n                feat = x_tok_n.mean(dim=1)           # [B,C]\n                feat = self.norm_feat(feat)          # [B,C]\n                return self.head(feat)\n\n            # CLS refinado con class-attn final (CaiT-style)\n            cls = self.cls_token.expand(B, -1, -1)   # [B,1,C]\n            if self.cls_pos is not None:\n                cls = cls + self.cls_pos\n\n            for cab in self.cls_attn_blocks:\n                cls = cab(cls, x_tok)               # [B,1,C]\n\n            cls_vec = cls.squeeze(1)                # [B,C]\n            cls_vec = self.norm_feat(cls_vec)\n\n            if self.pooling == \"cls\":\n                feat = cls_vec\n                return self.head(feat)\n\n            # pooling == \"cli\": mezcla CLS con mean(tokens) normalizado\n            tok_mean = self.norm(x_tok).mean(dim=1)  # [B,C]\n            feat = self.cli_pool(cls_vec, tok_mean)\n            feat = self.norm_feat(feat)\n            return self.head(feat)\n\n        else:\n            # ---- Hierarchical backbone (solo mean) ----\n            x_map = x_tok.view(B, Hp, Wp, C0)\n            x_last, (Hf, Wf) = self.pyramid(x_map)      # x_last: [B, Nf, C_last]\n\n            x_last = self.norm(x_last)\n            feat = x_last.mean(dim=1)\n            feat = self.norm_feat(feat)\n            return self.head(feat)\n\ndef test_volo_classifier_flat():\n    torch.manual_seed(0)\n    model = VOLOClassifier(\n        num_classes=100,\n        img_size=64,\n        patch_size=4,\n        hierarchical=False,\n        embed_dim=192,\n        outlooker_depth=2,\n        transformer_depth=2,\n        outlooker_heads=6,\n        transformer_heads=6,\n        pooling=\"mean\")\n\n    x = torch.randn(2, 3, 64, 64)\n    y = model(x)\n    print(\"[OK] flat logits:\", y.shape)\n    assert y.shape == (2, 100)\n\ndef test_volo_classifier_hier():\n    torch.manual_seed(0)\n    model = VOLOClassifier(\n        num_classes=100,\n        img_size=64,\n        patch_size=4,\n        hierarchical=True,\n        downsample_kind=\"map\",\n        dims=(192, 256, 384),\n        outlooker_depths=(2, 2, 0),\n        outlooker_heads_list=(6, 8, 12),\n        transformer_depths=(0, 2, 2),\n        transformer_heads_list=(6, 8, 12),\n        pooling=\"mean\",)\n\n    x = torch.randn(2, 3, 64, 64)\n    y = model(x)\n    print(\"[OK] hier logits:\", y.shape)\n    assert y.shape == (2, 100)\n\n\n\ndef _fmt_out(output):\n    if isinstance(output, (tuple, list)):\n        shapes = []\n        for o in output:\n            if hasattr(o, \"shape\"):\n                shapes.append(tuple(o.shape))\n            else:\n                shapes.append(type(o).__name__)\n        return shapes\n    if hasattr(output, \"shape\"):\n        return tuple(output.shape)\n    return type(output).__name__\n\n\ndef attach_shape_hooks_volo(model: nn.Module, verbose: bool = True):\n    hooks = []\n\n    def add_hook(mod: nn.Module, name: str):\n        if mod is None:\n            return\n        def hook(_m, _inp, out):\n            print(f\"{name:35s} -> {_fmt_out(out)}\")\n        hooks.append(mod.register_forward_hook(hook))\n\n    # Top-level components\n    add_hook(getattr(model, \"patch_embed\", None), \"patch_embed\")\n    add_hook(getattr(model, \"local_stage\", None), \"local_stage (outlooker)\")\n    add_hook(getattr(model, \"pyramid\", None), \"pyramid (top)\")\n    add_hook(getattr(model, \"norm\", None), \"norm\")\n    add_hook(getattr(model, \"head\", None), \"head\")\n\n    # Global blocks (flat)\n    if hasattr(model, \"global_blocks\"):\n        for i, blk in enumerate(model.global_blocks):\n            add_hook(blk, f\"global_block[{i}]\")\n\n    # Pyramid internals (hierarchical)\n    pyr = getattr(model, \"pyramid\", None)\n    if pyr is not None:\n        if hasattr(pyr, \"levels\"):\n            for i, lvl in enumerate(pyr.levels):\n                # lvl es nn.ModuleDict: NO tiene .get\n                loc = lvl[\"local\"] if \"local\" in lvl else None\n                glob = lvl[\"global\"] if \"global\" in lvl else None\n                add_hook(loc,  f\"pyr.level[{i}].local\")\n                add_hook(glob, f\"pyr.level[{i}].global\")\n\n        if hasattr(pyr, \"downsamples\"):\n            for i, ds in enumerate(pyr.downsamples):\n                add_hook(ds, f\"pyr.down[{i}]\")\n\n    return hooks\n\ndef remove_hooks(hooks):\n    for h in hooks:\n        h.remove()\n\n@torch.no_grad()\ndef debug_forward_shapes(model: nn.Module, img_size: int, device: str = \"cpu\", batch_size: int = 2):\n    model = model.to(device).eval()\n    hooks = attach_shape_hooks_volo(model)\n\n    x = torch.randn(batch_size, 3, img_size, img_size, device=device)\n    print(f\"\\n=== Forward debug | img_size={img_size} | model={model.__class__.__name__} ===\")\n    y = model(x)\n    print(f\"{'OUTPUT logits':35s} -> {tuple(y.shape)}\")\n\n    remove_hooks(hooks)\n\n\n\n\"\"\"---\"\"\"\n\nimport os, math, random, inspect\nfrom contextlib import contextmanager, nullcontext\nfrom typing import Dict\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\ndef seed_everything(seed: int = 0, deterministic: bool = False):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed_all(seed)\n    if deterministic:\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n    else:\n        torch.backends.cudnn.benchmark = True\n\n_DTYPE_MAP = {\n    \"bf16\": torch.bfloat16, \"bfloat16\": torch.bfloat16,\n    \"fp16\": torch.float16,  \"float16\": torch.float16,\n    \"fp32\": torch.float32,  \"float32\": torch.float32,}\n\ndef _cuda_dtype_supported(dtype: torch.dtype) -> bool:\n    if not torch.cuda.is_available():\n        return False\n    return dtype in (torch.float16, torch.bfloat16)\n\ndef make_grad_scaler(device: str = \"cuda\", enabled: bool = True):\n    if not enabled:\n        return None\n\n    if hasattr(torch, \"amp\") and hasattr(torch.amp, \"GradScaler\"):\n        try:\n            sig = inspect.signature(torch.amp.GradScaler)\n            if len(sig.parameters) >= 1:\n                return torch.amp.GradScaler(device if device in (\"cuda\", \"cpu\") else \"cuda\")\n            return torch.amp.GradScaler()\n        except Exception:\n            pass\n\n    if hasattr(torch.cuda, \"amp\") and hasattr(torch.cuda.amp, \"GradScaler\"):\n        return torch.cuda.amp.GradScaler()\n    return None\n\n\n@contextmanager\ndef autocast_ctx(\n    device: str = \"cuda\",\n    enabled: bool = True,\n    dtype: str = \"fp16\",\n    cache_enabled: bool = True,):\n    \"\"\"\n    Context manager de autocast:\n      - cuda: fp16 por defecto (ideal en T4)\n      - cpu: bfloat16 si está disponible\n    \"\"\"\n    if not enabled:\n        with nullcontext():\n            yield\n        return\n\n    if device == \"cuda\":\n        want = _DTYPE_MAP.get(dtype.lower(), torch.float16)\n        use = want if _cuda_dtype_supported(want) else torch.float16\n        with torch.amp.autocast(device_type=\"cuda\", dtype=use, cache_enabled=cache_enabled):\n            yield\n        return\n\n    if device == \"cpu\":\n        try:\n            with torch.amp.autocast(device_type=\"cpu\", dtype=torch.bfloat16, cache_enabled=cache_enabled):\n                yield\n        except Exception:\n            with nullcontext():\n                yield\n        return\n\n    with nullcontext():\n        yield\n\ndef build_param_groups_no_wd(model: nn.Module, weight_decay: float):\n    decay, no_decay = [], []\n    for name, p in model.named_parameters():\n        if not p.requires_grad:\n            continue\n\n        name_l = name.lower()\n        # no decay for biases + norms + positional/class tokens\n        if (\n            name.endswith(\".bias\")\n            or (\"norm\" in name_l)\n            or (\"bn\" in name_l)\n            or (\"ln\" in name_l)\n            or (\"pos\" in name_l)         # pos_embed / pos\n            or (\"cls_token\" in name_l)\n        ):\n            no_decay.append(p)\n        else:\n            decay.append(p)\n\n    return [\n        {\"params\": decay, \"weight_decay\": weight_decay},\n        {\"params\": no_decay, \"weight_decay\": 0.0}]\n\n\nclass WarmupCosineLR:\n    \"\"\"Warmup linear for warmup_steps, then cosine to min_lr. Step-based.\"\"\"\n    def __init__(self, optimizer, total_steps: int, warmup_steps: int, min_lr: float = 0.0):\n        self.optimizer = optimizer\n        self.total_steps = int(total_steps)\n        self.warmup_steps = int(warmup_steps)\n        self.min_lr = float(min_lr)\n        self.base_lrs = [g[\"lr\"] for g in optimizer.param_groups]\n        self.step_num = 0\n\n    def step(self):\n        self.step_num += 1\n        t = self.step_num\n\n        for i, group in enumerate(self.optimizer.param_groups):\n            base = self.base_lrs[i]\n            if t <= self.warmup_steps and self.warmup_steps > 0:\n                lr = base * (t / self.warmup_steps)\n            else:\n                tt = min(t, self.total_steps)\n                denom = max(1, self.total_steps - self.warmup_steps)\n                progress = (tt - self.warmup_steps) / denom\n                cosine = 0.5 * (1.0 + math.cos(math.pi * progress))\n                lr = self.min_lr + (base - self.min_lr) * cosine\n            group[\"lr\"] = lr\n\n    def state_dict(self):\n        return {\"step_num\": self.step_num}\n\n    def load_state_dict(self, d):\n        self.step_num = int(d.get(\"step_num\", 0))\n\ndef save_checkpoint(\n    path: str,\n    model,\n    optimizer,\n    scheduler,\n    scaler,\n    epoch: int,\n    best_top1: float,\n    extra: dict | None = None,):\n\n    ckpt = {\n        \"model\": model.state_dict(),\n        \"optimizer\": optimizer.state_dict() if optimizer is not None else None,\n        \"scheduler\": scheduler.state_dict() if scheduler is not None else None,\n        \"scaler\": scaler.state_dict() if scaler is not None else None,\n        \"epoch\": epoch,\n        \"best_top1\": best_top1,\n        \"extra\": extra or {},}\n    torch.save(ckpt, path)\n\n\ndef load_checkpoint(\n    path: str,\n    model,\n    optimizer=None,\n    scheduler=None,\n    scaler=None,\n    map_location=\"cpu\",\n    strict: bool = True,):\n    ckpt = torch.load(path, map_location=map_location)\n    model.load_state_dict(ckpt[\"model\"], strict=strict)\n\n    if optimizer is not None and ckpt.get(\"optimizer\") is not None:\n        optimizer.load_state_dict(ckpt[\"optimizer\"])\n    if scheduler is not None and ckpt.get(\"scheduler\") is not None:\n        scheduler.load_state_dict(ckpt[\"scheduler\"])\n    if scaler is not None and ckpt.get(\"scaler\") is not None:\n        scaler.load_state_dict(ckpt[\"scaler\"])\n    return ckpt\n\n# -------------------------\n# Mixup / CutMix + Loss\n# -------------------------\ndef _one_hot(targets: torch.Tensor, num_classes: int) -> torch.Tensor:\n    return F.one_hot(targets, num_classes=num_classes).float()\n\n\ndef soft_target_cross_entropy(logits: torch.Tensor, targets_soft: torch.Tensor) -> torch.Tensor:\n    logp = F.log_softmax(logits, dim=1)\n    return -(targets_soft * logp).sum(dim=1).mean()\n\n\ndef apply_mixup_cutmix(\n    images: torch.Tensor,\n    targets: torch.Tensor,\n    num_classes: int,\n    mixup_alpha: float = 0.0,\n    cutmix_alpha: float = 0.0,\n    prob: float = 1.0,):\n    \"\"\"\n    Returns:\n      images_aug: [B,3,H,W]\n      targets_soft: [B,K]\n    \"\"\"\n    if prob <= 0.0 or (mixup_alpha <= 0.0 and cutmix_alpha <= 0.0):\n        return images, _one_hot(targets, num_classes)\n\n    if random.random() > prob:\n        return images, _one_hot(targets, num_classes)\n\n    use_cutmix = (cutmix_alpha > 0.0) and (mixup_alpha <= 0.0 or random.random() < 0.5)\n    B, _, H, W = images.shape\n    perm = torch.randperm(B, device=images.device)\n\n    y1 = _one_hot(targets, num_classes)\n    y2 = _one_hot(targets[perm], num_classes)\n\n    if use_cutmix:\n        lam = torch.distributions.Beta(cutmix_alpha, cutmix_alpha).sample().item()\n        cut_w = int(W * math.sqrt(1.0 - lam))\n        cut_h = int(H * math.sqrt(1.0 - lam))\n        cx = random.randint(0, W - 1)\n        cy = random.randint(0, H - 1)\n\n        x1 = max(cx - cut_w // 2, 0)\n        x2 = min(cx + cut_w // 2, W)\n        y1b = max(cy - cut_h // 2, 0)\n        y2b = min(cy + cut_h // 2, H)\n\n        images_aug = images.clone()\n        images_aug[:, :, y1b:y2b, x1:x2] = images[perm, :, y1b:y2b, x1:x2]\n\n        # adjust lambda based on actual area swapped\n        area = (x2 - x1) * (y2b - y1b)\n        lam = 1.0 - area / float(W * H)\n    else:\n        lam = torch.distributions.Beta(mixup_alpha, mixup_alpha).sample().item()\n        images_aug = images * lam + images[perm] * (1.0 - lam)\n\n    targets_soft = y1 * lam + y2 * (1.0 - lam)\n    return images_aug, targets_soft\n\n# -------------------------\n# Metrics\n# -------------------------\n@torch.no_grad()\ndef accuracy_topk(logits: torch.Tensor, targets: torch.Tensor, ks=(1, 3, 5)) -> Dict[int, float]:\n    \"\"\"\n    targets can be:\n      - int64 class indices [B]\n      - soft targets [B, num_classes] (we'll argmax for accuracy reporting)\n    \"\"\"\n    if targets.ndim == 2:\n        targets = targets.argmax(dim=1)\n\n    max_k = max(ks)\n    B = targets.size(0)\n    _, pred = torch.topk(logits, k=max_k, dim=1)\n    correct = pred.eq(targets.view(-1, 1).expand_as(pred))\n    out = {}\n    for k in ks:\n        out[k] = 100.0 * correct[:, :k].any(dim=1).float().sum().item() / B\n    return out\n\nimport torch\nimport torch.distributed as dist\nfrom torch.utils.data.distributed import DistributedSampler\n\ndef ddp_is_on() -> bool:\n    return dist.is_available() and dist.is_initialized()\n\ndef ddp_rank() -> int:\n    return dist.get_rank() if ddp_is_on() else 0\n\ndef is_main_process() -> bool:\n    return (not ddp_is_on()) or ddp_rank() == 0\n\ndef ddp_sum_(tensor: torch.Tensor) -> torch.Tensor:\n    \"\"\"All-reduce SUM in-place and return tensor.\"\"\"\n    if ddp_is_on():\n        dist.all_reduce(tensor, op=dist.ReduceOp.SUM)\n    return tensor\n\ndef ddp_broadcast_bool(flag: bool, device: torch.device | str) -> bool:\n    \"\"\"Broadcast a stop flag from rank0 to all ranks.\"\"\"\n    t = torch.tensor([1 if flag else 0], device=device)\n    if ddp_is_on():\n        dist.broadcast(t, src=0)\n    return bool(t.item())\n\nfrom typing import Optional\nimport time\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\ndef train_one_epoch(\n    model: nn.Module,\n    dataloader,\n    optimizer: torch.optim.Optimizer,\n    scheduler,\n    device: str = \"cuda\",\n    scaler=None,\n    autocast_dtype: str = \"fp16\",\n    use_amp: bool = True,\n    grad_clip_norm: Optional[float] = 1.0,\n    label_smoothing: float = 0.1,\n    mixup_alpha: float = 0.0,\n    cutmix_alpha: float = 0.0,\n    mix_prob: float = 1.0,\n    num_classes: int = 100,\n    channels_last: bool = False,\n    print_every: int = 100,\n):\n    model.train()\n\n    use_scaler = (scaler is not None) and use_amp and autocast_dtype.lower() in (\"fp16\", \"float16\")\n\n    running_loss = 0.0\n    total = 0\n    c1 = c3 = c5 = 0.0  # “correct counts” acumulados (no %)\n\n    t0 = time.time()\n    for step, (images, targets) in enumerate(dataloader, start=1):\n        images = images.to(device, non_blocking=True)\n        targets = targets.to(device, non_blocking=True)\n\n        if channels_last:\n            images = images.contiguous(memory_format=torch.channels_last)\n\n        B = targets.size(0)\n\n        images_aug, targets_soft = apply_mixup_cutmix(\n            images, targets,\n            num_classes=num_classes,\n            mixup_alpha=mixup_alpha,\n            cutmix_alpha=cutmix_alpha,\n            prob=mix_prob\n        )\n\n        use_mix = (mixup_alpha > 0.0) or (cutmix_alpha > 0.0)\n        targets_for_acc = targets_soft.argmax(dim=1)\n\n        optimizer.zero_grad(set_to_none=True)\n        \n        with autocast_ctx(device=device, enabled=use_amp, dtype=autocast_dtype, cache_enabled=True):\n            logits = model(images_aug)\n\n            assert logits.ndim == 2 and logits.size(1) == num_classes, f\"logits shape {logits.shape} != [B,{num_classes}]\"\n            tmin = int(targets.min().item()); tmax = int(targets.max().item())\n            assert 0 <= tmin and tmax < num_classes, f\"targets out of range: min={tmin}, max={tmax}, K={num_classes}\"\n            \n        if use_mix:\n            loss = soft_target_cross_entropy(logits.float(), targets_soft)\n        else:\n            loss = F.cross_entropy(logits.float(), targets, label_smoothing=label_smoothing)\n\n        if use_scaler:\n            scaler.scale(loss).backward()\n            if grad_clip_norm is not None:\n                scaler.unscale_(optimizer)\n                torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip_norm)\n            scaler.step(optimizer)\n            scaler.update()\n        else:\n            loss.backward()\n            if grad_clip_norm is not None:\n                torch.nn.utils.clip_grad_norm_(model.parameters(), grad_clip_norm)\n            optimizer.step()\n\n        if scheduler is not None:\n            scheduler.step()\n\n        # métricas “locales”\n        running_loss += loss.item() * B\n        total += B\n        accs = accuracy_topk(logits.detach(), targets_for_acc, ks=(1, 3, 5))\n        c1 += accs[1] * B / 100.0\n        c3 += accs[3] * B / 100.0\n        c5 += accs[5] * B / 100.0\n\n        # log solo en rank0 (si DDP)\n        if print_every and (step % print_every == 0) and is_main_process():\n            dt = time.time() - t0\n            imgs_sec = total / max(dt, 1e-9)\n            print(\n                f\"[train step {step}/{len(dataloader)}] \"\n                f\"loss {running_loss/total:.4f} | \"\n                f\"top1 {100*c1/total:.2f}% | top3 {100*c3/total:.2f}% | top5 {100*c5/total:.2f}% | \"\n                f\"{imgs_sec:.1f} img/s | lr {optimizer.param_groups[0]['lr']:.2e}\"\n            )\n\n    # ---- REDUCCIÓN GLOBAL (DDP) ----\n    stats = torch.tensor([running_loss, total, c1, c3, c5], device=device, dtype=torch.float64)\n    ddp_sum_(stats)\n    running_loss_g, total_g, c1_g, c3_g, c5_g = stats.tolist()\n\n    avg_loss = running_loss_g / max(total_g, 1e-12)\n    metrics = {\n        \"top1\": 100.0 * c1_g / max(total_g, 1e-12),\n        \"top3\": 100.0 * c3_g / max(total_g, 1e-12),\n        \"top5\": 100.0 * c5_g / max(total_g, 1e-12),}\n\n    return avg_loss, metrics\n\n@torch.no_grad()\ndef evaluate_one_epoch(\n    model: nn.Module,\n    dataloader,\n    device: str = \"cuda\",\n    autocast_dtype: str = \"fp16\",\n    use_amp: bool = True,\n    label_smoothing: float = 0.0,\n    channels_last: bool = False,\n):\n    model.eval()\n\n    running_loss = 0.0\n    total = 0\n    c1 = c3 = c5 = 0.0\n\n    for images, targets in dataloader:\n        images = images.to(device, non_blocking=True)\n        targets = targets.to(device, non_blocking=True)\n\n        if channels_last:\n            images = images.contiguous(memory_format=torch.channels_last)\n\n        B = targets.size(0)\n\n        with autocast_ctx(device=device, enabled=use_amp, dtype=autocast_dtype, cache_enabled=True):\n            logits = model(images)\n\n        loss = F.cross_entropy(logits.float(), targets, label_smoothing=label_smoothing)\n\n        running_loss += loss.item() * B\n        total += B\n\n        accs = accuracy_topk(logits, targets, ks=(1, 3, 5))\n        c1 += accs[1] * B / 100.0\n        c3 += accs[3] * B / 100.0\n        c5 += accs[5] * B / 100.0\n\n    # ---- REDUCCIÓN GLOBAL (DDP) ----\n    stats = torch.tensor([running_loss, total, c1, c3, c5], device=device, dtype=torch.float64)\n    ddp_sum_(stats)\n    running_loss_g, total_g, c1_g, c3_g, c5_g = stats.tolist()\n\n    avg_loss = running_loss_g / max(total_g, 1e-12)\n    metrics = {\n        \"top1\": 100.0 * c1_g / max(total_g, 1e-12),\n        \"top3\": 100.0 * c3_g / max(total_g, 1e-12),\n        \"top5\": 100.0 * c5_g / max(total_g, 1e-12),}\n\n    return avg_loss, metrics\n\nimport time\nimport torch\nimport torch.nn as nn\n\ndef train_model(\n    model: nn.Module,\n    train_loader,\n    epochs: int,\n    val_loader=None,\n    device: str = \"cuda\",\n    lr: float = 5e-4,\n    weight_decay: float = 0.05,\n    autocast_dtype: str = \"fp16\",\n    use_amp: bool = True,\n    grad_clip_norm: float | None = 1.0,\n    warmup_ratio: float = 0.05,\n    min_lr: float = 0.0,\n    label_smoothing: float = 0.1,\n    print_every: int = 100,\n    save_path: str = \"best_model.pt\",\n    last_path: str = \"last_model.pt\",\n    resume_path: str | None = None,\n\n    mixup_alpha: float = 0.0,\n    cutmix_alpha: float = 0.0,\n    mix_prob: float = 1.0,\n    num_classes: int = 100,\n    channels_last: bool = False,\n\n    early_stop: bool = True,\n    early_stop_metric: str = \"top1\",\n    early_stop_patience: int = 10,\n    early_stop_min_delta: float = 0.0,\n    early_stop_require_monotonic: bool = False):\n\n    model.to(device)\n\n    # Optimizer\n    param_groups = build_param_groups_no_wd(model, weight_decay=weight_decay)\n    optimizer = torch.optim.AdamW(param_groups, lr=lr, betas=(0.9, 0.999), eps=1e-8)\n\n    # Scheduler warmup + cosine (step-based)\n    total_steps = epochs * len(train_loader)\n    warmup_steps = int(total_steps * warmup_ratio)\n    scheduler = WarmupCosineLR(\n        optimizer,\n        total_steps=total_steps,\n        warmup_steps=warmup_steps,\n        min_lr=min_lr,\n    )\n\n    scaler = None\n    if use_amp and autocast_dtype.lower() in (\"fp16\", \"float16\"):\n        scaler = make_grad_scaler(device=device, enabled=True)\n\n    # Resume\n    start_epoch = 0\n    best_val_top1 = -float(\"inf\")\n    best_val_loss = float(\"inf\")\n    best_epoch = 0\n\n    if resume_path is not None:\n        ckpt = load_checkpoint(\n            resume_path, model,\n            optimizer=optimizer, scheduler=scheduler, scaler=scaler,\n            map_location=device,\n            strict=True,\n        )\n\n        start_epoch = int(ckpt.get(\"epoch\", 0))\n        best_val_top1 = float(ckpt.get(\"best_top1\", best_val_top1))\n        extra = ckpt.get(\"extra\", {}) or {}\n        best_val_loss = float(extra.get(\"best_val_loss\", best_val_loss))\n        best_epoch = int(extra.get(\"best_epoch\", best_epoch))\n\n        if is_main_process():\n            print(f\"Resumed from {resume_path} at epoch {start_epoch} | best_top1 {best_val_top1:.2f}% | best_loss {best_val_loss:.4f}\")\n\n    history = {\n        \"train_loss\": [], \"train_top1\": [], \"train_top3\": [], \"train_top5\": [],\n        \"val_loss\": [], \"val_top1\": [], \"val_top3\": [], \"val_top5\": [],\n        \"lr\": [],\n    } if is_main_process() else None  # <- solo rank0 guarda history\n\n    # Early stop state (solo rank0 lleva el estado)\n    metric = early_stop_metric.lower()\n    assert metric in (\"top1\", \"loss\")\n    patience = int(early_stop_patience)\n    mode = \"max\" if metric == \"top1\" else \"min\"\n    best_metric = best_val_top1 if metric == \"top1\" else best_val_loss\n    bad_epochs = 0\n    last_vals = []\n\n    def _is_improvement(curr: float, best: float) -> bool:\n        d = float(early_stop_min_delta)\n        return (curr > (best + d)) if mode == \"max\" else (curr < (best - d))\n\n    def _degradation_monotonic(vals: list[float]) -> bool:\n        if not early_stop_require_monotonic or len(vals) < 2:\n            return True\n        if mode == \"max\":\n            return all(vals[i] >= vals[i + 1] for i in range(len(vals) - 1))\n        else:\n            return all(vals[i] <= vals[i + 1] for i in range(len(vals) - 1))\n\n    for epoch in range(start_epoch + 1, epochs + 1):\n        if is_main_process():\n            print(f\"\\n=== Epoch {epoch}/{epochs} ===\")\n        t_epoch = time.time()\n\n        # ✅ DDP: reshuffle correcto por epoch\n        if hasattr(train_loader, \"sampler\") and isinstance(train_loader.sampler, DistributedSampler):\n            train_loader.sampler.set_epoch(epoch)\n        if val_loader is not None and hasattr(val_loader, \"sampler\") and isinstance(val_loader.sampler, DistributedSampler):\n            val_loader.sampler.set_epoch(epoch)\n\n        # --- Train ---\n        tr_loss, tr_m = train_one_epoch(\n            model=model,\n            dataloader=train_loader,\n            optimizer=optimizer,\n            scheduler=scheduler,\n            device=device,\n            scaler=scaler,\n            autocast_dtype=autocast_dtype,\n            use_amp=use_amp,\n            grad_clip_norm=grad_clip_norm,\n            label_smoothing=label_smoothing,\n            mixup_alpha=mixup_alpha,\n            cutmix_alpha=cutmix_alpha,\n            mix_prob=mix_prob,\n            num_classes=num_classes,\n            channels_last=channels_last,\n            print_every=print_every,\n        )\n\n        if is_main_process():\n            history[\"train_loss\"].append(tr_loss)\n            history[\"train_top1\"].append(tr_m[\"top1\"])\n            history[\"train_top3\"].append(tr_m[\"top3\"])\n            history[\"train_top5\"].append(tr_m[\"top5\"])\n            history[\"lr\"].append(optimizer.param_groups[0][\"lr\"])\n\n            print(f\"[Train] loss {tr_loss:.4f} | top1 {tr_m['top1']:.2f}% | top3 {tr_m['top3']:.2f}% | top5 {tr_m['top5']:.2f}% | lr {optimizer.param_groups[0]['lr']:.2e}\")\n\n            # ✅ guardar \"last\" SOLO en rank0\n            save_checkpoint(\n                last_path, model, optimizer, scheduler, scaler,\n                epoch=epoch, best_top1=best_val_top1,\n                extra={\n                    \"autocast_dtype\": autocast_dtype,\n                    \"use_amp\": use_amp,\n                    \"best_val_loss\": best_val_loss,\n                    \"best_epoch\": best_epoch,\n                    \"early_stop_metric\": metric,\n                    \"early_stop_patience\": patience,\n                    \"early_stop_min_delta\": float(early_stop_min_delta),\n                },\n            )\n\n        stop_now = False\n\n        # --- Val ---\n        if val_loader is not None:\n            va_loss, va_m = evaluate_one_epoch(\n                model=model,\n                dataloader=val_loader,\n                device=device,\n                autocast_dtype=autocast_dtype,\n                use_amp=use_amp,\n                label_smoothing=0.0,\n                channels_last=channels_last,\n            )\n\n            if is_main_process():\n                history[\"val_loss\"].append(va_loss)\n                history[\"val_top1\"].append(va_m[\"top1\"])\n                history[\"val_top3\"].append(va_m[\"top3\"])\n                history[\"val_top5\"].append(va_m[\"top5\"])\n\n                print(f\"[Val]   loss {va_loss:.4f} | top1 {va_m['top1']:.2f}% | top3 {va_m['top3']:.2f}% | top5 {va_m['top5']:.2f}%\")\n\n                # Best saved por top1\n                if va_m[\"top1\"] > best_val_top1:\n                    best_val_top1 = va_m[\"top1\"]\n                    if va_loss < best_val_loss:\n                        best_val_loss = va_loss\n                        best_epoch = epoch\n\n                    save_checkpoint(\n                        save_path, model, optimizer, scheduler, scaler,\n                        epoch=epoch, best_top1=best_val_top1,\n                        extra={\n                            \"autocast_dtype\": autocast_dtype,\n                            \"use_amp\": use_amp,\n                            \"best_val_loss\": best_val_loss,\n                            \"best_epoch\": best_epoch,\n                        },\n                    )\n                    print(f\"Best saved to {save_path} (val top1 {best_val_top1:.2f}%)\")\n\n                # Early stop (solo rank0 decide)\n                if early_stop:\n                    curr_metric = va_m[\"top1\"] if metric == \"top1\" else va_loss\n\n                    last_vals.append(float(curr_metric))\n                    if len(last_vals) > patience:\n                        last_vals = last_vals[-patience:]\n\n                    if _is_improvement(curr_metric, best_metric):\n                        best_metric = float(curr_metric)\n                        bad_epochs = 0\n                    else:\n                        bad_epochs += 1\n\n                    if bad_epochs >= patience and _degradation_monotonic(last_vals):\n                        print(f\"Early-stop: no improvement on val_{metric} for {patience} epochs.\")\n                        stop_now = True\n\n        # ✅ DDP: sincroniza el “stop” a todos los ranks\n        stop_now = ddp_broadcast_bool(stop_now, device=device)\n        if stop_now:\n            break\n\n        if is_main_process():\n            dt = time.time() - t_epoch\n            print(f\"Epoch time: {dt/60:.2f} min\")\n\n    # return: history solo en rank0; en otros ranks devuelve None\n    return history, (model.module if hasattr(model, \"module\") else model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T03:43:43.043553Z","iopub.execute_input":"2025-12-31T03:43:43.044029Z","iopub.status.idle":"2025-12-31T03:43:43.076896Z","shell.execute_reply.started":"2025-12-31T03:43:43.044005Z","shell.execute_reply":"2025-12-31T03:43:43.076322Z"},"_kg_hide-input":true,"jupyter":{"source_hidden":true}},"outputs":[{"name":"stdout","text":"Overwriting volo.py\n","output_type":"stream"}],"execution_count":88},{"cell_type":"markdown","source":"---\n\n# Training","metadata":{"id":"VagXinWMCV-t"}},{"cell_type":"code","source":"%%writefile train_ddp.py\nimport os\nimport torch\nimport torch.distributed as dist\nfrom torch.nn.parallel import DistributedDataParallel as DDP\nfrom torch.utils.data import DataLoader\nfrom torch.utils.data.distributed import DistributedSampler\n\nfrom volo import *\n\n\ndef setup_ddp():\n    dist.init_process_group(backend=\"nccl\")\n    local_rank = int(os.environ[\"LOCAL_RANK\"])\n    torch.cuda.set_device(local_rank)\n    return local_rank\n\ndef is_main():\n    return (not dist.is_available()) or (not dist.is_initialized()) or dist.get_rank() == 0\n\ndef main():\n    torch.set_num_threads(1)\n    torch.set_num_interop_threads(1)\n    \n    local_rank = setup_ddp()\n    device = torch.device(f\"cuda:{local_rank}\")\n\n    train_ds, val_ds, test_ds = get_cifar100_datasets(\n        data_dir=\"./data/cifar100\",\n        val_split=0.1,\n        img_size=32,\n        ddp_safe_download=True,)\n\n    train_sampler = DistributedSampler(train_ds, shuffle=True, drop_last=True)\n\n    train_loader = DataLoader(\n            train_ds,\n            batch_size=256,\n            sampler=train_sampler,\n            num_workers=2,\n            pin_memory=True,\n            persistent_workers=True,\n            prefetch_factor=2,)\n\n    val_loader = None\n    \n    if val_ds is not None:\n        val_sampler = DistributedSampler(val_ds, shuffle=False, drop_last=False)\n        val_loader = DataLoader(\n            val_ds,\n            batch_size=256,\n            sampler=val_sampler,\n            num_workers=2,\n            pin_memory=True,\n            persistent_workers=True,)\n\n    model = VOLOClassifier(\n        num_classes=100,\n        img_size=32,\n        patch_size=4,\n        hierarchical=False,\n        embed_dim=320,\n        outlooker_depth=5,\n        outlooker_heads=10,\n        transformer_depth=10,\n        transformer_heads=10,\n        kernel_size=3,\n        mlp_ratio=4.0,\n        dropout=0.12,\n        attn_dropout=0.05,\n        drop_path_rate=0.20,\n        pooling=\"cls\",\n        cls_attn_depth=2,\n        use_pos_embed=True,\n        use_cls_pos=True,).to(device)\n\n    model = DDP(model, device_ids=[local_rank], output_device=local_rank ,find_unused_parameters=True)\n\n    history, best = train_model(\n        model=model,\n        train_loader=train_loader,\n        val_loader=val_loader,\n        epochs=130,\n        device=str(device),\n        lr=5e-4,\n        weight_decay=0.05,\n        use_amp=True,\n        autocast_dtype=\"fp16\",\n        print_every=25,\n        num_classes=100,\n        save_path=\"best_model.pt\",\n        last_path=\"last_model.pt\",)\n\n    dist.destroy_process_group()\n\n\nif __name__ == \"__main__\":\n    main()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T03:46:22.989707Z","iopub.execute_input":"2025-12-31T03:46:22.990564Z","iopub.status.idle":"2025-12-31T03:46:22.996927Z","shell.execute_reply.started":"2025-12-31T03:46:22.990531Z","shell.execute_reply":"2025-12-31T03:46:22.996301Z"}},"outputs":[{"name":"stdout","text":"Overwriting train_ddp.py\n","output_type":"stream"}],"execution_count":91},{"cell_type":"code","source":"!torchrun --nproc_per_node=2 train_ddp.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-31T03:46:24.508444Z","iopub.execute_input":"2025-12-31T03:46:24.509433Z","iopub.status.idle":"2025-12-31T05:40:25.297779Z","shell.execute_reply.started":"2025-12-31T03:46:24.509404Z","shell.execute_reply":"2025-12-31T05:40:25.296828Z"}},"outputs":[{"name":"stdout","text":"W1231 03:46:26.248000 2847 torch/distributed/run.py:792] \nW1231 03:46:26.248000 2847 torch/distributed/run.py:792] *****************************************\nW1231 03:46:26.248000 2847 torch/distributed/run.py:792] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. \nW1231 03:46:26.248000 2847 torch/distributed/run.py:792] *****************************************\n[rank1]:[W1231 03:46:29.459079996 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 1]  using GPU 1 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\n[rank0]:[W1231 03:46:30.310899516 ProcessGroupNCCL.cpp:4561] [PG ID 0 PG GUID 0 Rank 0]  using GPU 0 to perform barrier as devices used by this process are currently unknown. This can potentially cause a hang if this rank to GPU mapping is incorrect. Specify device_ids in barrier() to force use of a particular device, or call init_process_group() with a device_id.\n\n=== Epoch 1/130 ===\n[train step 25/88] loss 4.6723 | top1 1.84% | top3 5.27% | top5 8.59% | 232.3 img/s | lr 2.19e-05\n[train step 50/88] loss 4.5742 | top1 2.70% | top3 7.07% | top5 10.90% | 243.0 img/s | lr 4.37e-05\n[train step 75/88] loss 4.5107 | top1 3.36% | top3 8.59% | top5 12.90% | 252.0 img/s | lr 6.56e-05\n[Train] loss 4.4824 | top1 3.64% | top3 9.26% | top5 13.85% | lr 7.69e-05\n[Val]   loss 4.1166 | top1 6.74% | top3 16.02% | top5 23.24%\nBest saved to best_model.pt (val top1 6.74%)\nEpoch time: 1.55 min\n\n=== Epoch 2/130 ===\n[train step 25/88] loss 4.2280 | top1 6.72% | top3 16.38% | top5 23.55% | 257.8 img/s | lr 9.88e-05\n[train step 50/88] loss 4.1768 | top1 7.66% | top3 17.80% | top5 25.23% | 258.6 img/s | lr 1.21e-04\n[train step 75/88] loss 4.1308 | top1 8.28% | top3 19.18% | top5 26.95% | 261.0 img/s | lr 1.42e-04\n[Train] loss 4.1105 | top1 8.80% | top3 19.97% | top5 27.73% | lr 1.54e-04\n[Val]   loss 3.7379 | top1 12.14% | top3 26.40% | top5 36.20%\nBest saved to best_model.pt (val top1 12.14%)\nEpoch time: 1.51 min\n\n=== Epoch 3/130 ===\n[train step 25/88] loss 3.9248 | top1 11.84% | top3 25.92% | top5 35.86% | 261.1 img/s | lr 1.76e-04\n[train step 50/88] loss 3.8937 | top1 12.47% | top3 27.05% | top5 36.80% | 261.9 img/s | lr 1.98e-04\n[train step 75/88] loss 3.8698 | top1 13.34% | top3 27.95% | top5 37.55% | 262.6 img/s | lr 2.19e-04\n[Train] loss 3.8498 | top1 13.69% | top3 28.43% | top5 38.04% | lr 2.31e-04\n[Val]   loss 3.4662 | top1 18.00% | top3 33.68% | top5 44.16%\nBest saved to best_model.pt (val top1 18.00%)\nEpoch time: 1.50 min\n\n=== Epoch 4/130 ===\n[train step 25/88] loss 3.6862 | top1 16.62% | top3 32.94% | top5 43.67% | 261.1 img/s | lr 2.53e-04\n[train step 50/88] loss 3.6772 | top1 17.12% | top3 33.48% | top5 43.97% | 261.8 img/s | lr 2.74e-04\n[train step 75/88] loss 3.6631 | top1 17.66% | top3 34.31% | top5 44.53% | 262.3 img/s | lr 2.96e-04\n[Train] loss 3.6441 | top1 18.12% | top3 35.05% | top5 45.11% | lr 3.08e-04\n[Val]   loss 3.2105 | top1 22.78% | top3 40.64% | top5 50.96%\nBest saved to best_model.pt (val top1 22.78%)\nEpoch time: 1.50 min\n\n=== Epoch 5/130 ===\n[train step 25/88] loss 3.4965 | top1 21.73% | top3 39.50% | top5 49.50% | 258.9 img/s | lr 3.30e-04\n[train step 50/88] loss 3.4682 | top1 22.56% | top3 40.35% | top5 50.67% | 259.1 img/s | lr 3.51e-04\n[train step 75/88] loss 3.4494 | top1 22.81% | top3 40.95% | top5 51.49% | 258.6 img/s | lr 3.73e-04\n[Train] loss 3.4431 | top1 22.59% | top3 41.04% | top5 51.53% | lr 3.85e-04\n[Val]   loss 2.8959 | top1 28.46% | top3 48.46% | top5 59.18%\nBest saved to best_model.pt (val top1 28.46%)\nEpoch time: 1.53 min\n\n=== Epoch 6/130 ===\n[train step 25/88] loss 3.3084 | top1 25.33% | top3 45.23% | top5 56.47% | 258.1 img/s | lr 4.06e-04\n[train step 50/88] loss 3.2981 | top1 25.91% | top3 45.92% | top5 56.59% | 259.2 img/s | lr 4.28e-04\n[train step 75/88] loss 3.2757 | top1 26.22% | top3 46.65% | top5 57.19% | 259.1 img/s | lr 4.50e-04\n[Train] loss 3.2690 | top1 26.36% | top3 46.60% | top5 57.28% | lr 4.62e-04\n[Val]   loss 2.7216 | top1 31.76% | top3 53.04% | top5 63.60%\nBest saved to best_model.pt (val top1 31.76%)\nEpoch time: 1.52 min\n\n=== Epoch 7/130 ===\n[train step 25/88] loss 3.1312 | top1 29.16% | top3 50.73% | top5 61.48% | 258.7 img/s | lr 4.83e-04\n[train step 50/88] loss 3.1299 | top1 29.27% | top3 50.54% | top5 61.34% | 260.7 img/s | lr 5.00e-04\n[train step 75/88] loss 3.1089 | top1 29.90% | top3 51.20% | top5 61.99% | 260.1 img/s | lr 5.00e-04\n[Train] loss 3.1112 | top1 30.02% | top3 51.14% | top5 61.93% | lr 5.00e-04\n[Val]   loss 2.6124 | top1 32.02% | top3 54.92% | top5 65.64%\nBest saved to best_model.pt (val top1 32.02%)\nEpoch time: 1.52 min\n\n=== Epoch 8/130 ===\n[train step 25/88] loss 2.9805 | top1 32.98% | top3 54.88% | top5 65.44% | 259.8 img/s | lr 5.00e-04\n[train step 50/88] loss 2.9735 | top1 33.47% | top3 55.10% | top5 65.73% | 260.0 img/s | lr 5.00e-04\n[train step 75/88] loss 2.9515 | top1 34.05% | top3 55.67% | top5 66.28% | 259.3 img/s | lr 5.00e-04\n[Train] loss 2.9558 | top1 33.87% | top3 55.85% | top5 66.09% | lr 5.00e-04\n[Val]   loss 2.4713 | top1 36.14% | top3 58.42% | top5 68.94%\nBest saved to best_model.pt (val top1 36.14%)\nEpoch time: 1.52 min\n\n=== Epoch 9/130 ===\n[train step 25/88] loss 2.8551 | top1 36.52% | top3 58.38% | top5 68.59% | 255.1 img/s | lr 5.00e-04\n[train step 50/88] loss 2.8319 | top1 36.86% | top3 59.05% | top5 69.49% | 256.4 img/s | lr 5.00e-04\n[train step 75/88] loss 2.8319 | top1 36.81% | top3 59.18% | top5 69.53% | 257.0 img/s | lr 5.00e-04\n[Train] loss 2.8225 | top1 37.20% | top3 59.46% | top5 69.68% | lr 4.99e-04\n[Val]   loss 2.2822 | top1 40.82% | top3 63.46% | top5 72.90%\nBest saved to best_model.pt (val top1 40.82%)\nEpoch time: 1.53 min\n\n=== Epoch 10/130 ===\n[train step 25/88] loss 2.7362 | top1 39.28% | top3 62.64% | top5 71.89% | 254.8 img/s | lr 4.99e-04\n[train step 50/88] loss 2.7301 | top1 39.77% | top3 62.52% | top5 71.98% | 256.3 img/s | lr 4.99e-04\n[train step 75/88] loss 2.7195 | top1 40.15% | top3 62.81% | top5 72.39% | 256.9 img/s | lr 4.99e-04\n[Train] loss 2.7113 | top1 40.20% | top3 62.91% | top5 72.47% | lr 4.99e-04\n[Val]   loss 2.1488 | top1 42.98% | top3 65.78% | top5 75.88%\nBest saved to best_model.pt (val top1 42.98%)\nEpoch time: 1.53 min\n\n=== Epoch 11/130 ===\n[train step 25/88] loss 2.6213 | top1 42.91% | top3 65.39% | top5 74.81% | 255.7 img/s | lr 4.99e-04\n[train step 50/88] loss 2.6165 | top1 42.88% | top3 65.67% | top5 75.09% | 257.2 img/s | lr 4.99e-04\n[train step 75/88] loss 2.6168 | top1 42.69% | top3 65.47% | top5 75.06% | 257.8 img/s | lr 4.98e-04\n[Train] loss 2.6171 | top1 42.72% | top3 65.49% | top5 75.06% | lr 4.98e-04\n[Val]   loss 2.0487 | top1 45.44% | top3 67.62% | top5 77.76%\nBest saved to best_model.pt (val top1 45.44%)\nEpoch time: 1.53 min\n\n=== Epoch 12/130 ===\n[train step 25/88] loss 2.4867 | top1 45.72% | top3 69.09% | top5 78.02% | 258.9 img/s | lr 4.98e-04\n[train step 50/88] loss 2.5112 | top1 45.21% | top3 68.59% | top5 77.41% | 260.1 img/s | lr 4.98e-04\n[train step 75/88] loss 2.5085 | top1 45.35% | top3 68.54% | top5 77.47% | 259.9 img/s | lr 4.98e-04\n[Train] loss 2.5295 | top1 45.09% | top3 67.86% | top5 76.92% | lr 4.98e-04\n[Val]   loss 1.9830 | top1 47.50% | top3 69.34% | top5 78.36%\nBest saved to best_model.pt (val top1 47.50%)\nEpoch time: 1.52 min\n\n=== Epoch 13/130 ===\n[train step 25/88] loss 2.4275 | top1 48.23% | top3 70.42% | top5 79.31% | 255.9 img/s | lr 4.97e-04\n[train step 50/88] loss 2.4497 | top1 47.74% | top3 69.80% | top5 78.84% | 257.5 img/s | lr 4.97e-04\n[train step 75/88] loss 2.4498 | top1 47.67% | top3 70.01% | top5 79.00% | 258.0 img/s | lr 4.97e-04\n[Train] loss 2.4526 | top1 47.63% | top3 70.05% | top5 78.83% | lr 4.97e-04\n[Val]   loss 1.8828 | top1 49.42% | top3 71.56% | top5 80.46%\nBest saved to best_model.pt (val top1 49.42%)\nEpoch time: 1.53 min\n\n=== Epoch 14/130 ===\n[train step 25/88] loss 2.3588 | top1 49.84% | top3 72.41% | top5 81.14% | 257.1 img/s | lr 4.96e-04\n[train step 50/88] loss 2.3742 | top1 49.62% | top3 71.82% | top5 80.45% | 258.3 img/s | lr 4.96e-04\n[train step 75/88] loss 2.3806 | top1 49.67% | top3 71.84% | top5 80.35% | 258.4 img/s | lr 4.96e-04\n[Train] loss 2.3782 | top1 49.65% | top3 72.04% | top5 80.45% | lr 4.95e-04\n[Val]   loss 1.8068 | top1 51.98% | top3 74.04% | top5 81.66%\nBest saved to best_model.pt (val top1 51.98%)\nEpoch time: 1.53 min\n\n=== Epoch 15/130 ===\n[train step 25/88] loss 2.2973 | top1 51.66% | top3 74.12% | top5 83.00% | 256.9 img/s | lr 4.95e-04\n[train step 50/88] loss 2.3121 | top1 51.43% | top3 73.78% | top5 82.47% | 258.1 img/s | lr 4.95e-04\n[train step 75/88] loss 2.2956 | top1 51.97% | top3 74.09% | top5 82.60% | 258.5 img/s | lr 4.94e-04\n[Train] loss 2.3124 | top1 51.44% | top3 73.69% | top5 82.00% | lr 4.94e-04\n[Val]   loss 1.8152 | top1 50.66% | top3 73.68% | top5 81.60%\nEpoch time: 1.52 min\n\n=== Epoch 16/130 ===\n[train step 25/88] loss 2.2387 | top1 52.83% | top3 75.66% | top5 83.95% | 257.0 img/s | lr 4.94e-04\n[train step 50/88] loss 2.2591 | top1 52.71% | top3 75.16% | top5 83.48% | 258.3 img/s | lr 4.93e-04\n[train step 75/88] loss 2.2632 | top1 52.58% | top3 75.09% | top5 83.40% | 258.6 img/s | lr 4.93e-04\n[Train] loss 2.2559 | top1 52.97% | top3 75.27% | top5 83.43% | lr 4.93e-04\n[Val]   loss 1.7109 | top1 53.38% | top3 75.68% | top5 84.14%\nBest saved to best_model.pt (val top1 53.38%)\nEpoch time: 1.53 min\n\n=== Epoch 17/130 ===\n[train step 25/88] loss 2.1867 | top1 55.12% | top3 76.91% | top5 84.70% | 256.3 img/s | lr 4.92e-04\n[train step 50/88] loss 2.1958 | top1 54.38% | top3 76.96% | top5 84.86% | 257.6 img/s | lr 4.92e-04\n[train step 75/88] loss 2.1925 | top1 54.83% | top3 76.81% | top5 84.59% | 258.8 img/s | lr 4.91e-04\n[Train] loss 2.1973 | top1 54.69% | top3 76.71% | top5 84.42% | lr 4.91e-04\n[Val]   loss 1.6739 | top1 55.32% | top3 75.70% | top5 83.70%\nBest saved to best_model.pt (val top1 55.32%)\nEpoch time: 1.52 min\n\n=== Epoch 18/130 ===\n[train step 25/88] loss 2.1184 | top1 57.47% | top3 78.66% | top5 85.92% | 257.8 img/s | lr 4.91e-04\n[train step 50/88] loss 2.1402 | top1 56.54% | top3 77.95% | top5 85.27% | 258.4 img/s | lr 4.90e-04\n[train step 75/88] loss 2.1488 | top1 56.36% | top3 77.71% | top5 85.14% | 259.0 img/s | lr 4.90e-04\n[Train] loss 2.1465 | top1 56.40% | top3 77.71% | top5 85.20% | lr 4.89e-04\n[Val]   loss 1.6486 | top1 55.90% | top3 76.62% | top5 83.84%\nBest saved to best_model.pt (val top1 55.90%)\nEpoch time: 1.52 min\n\n=== Epoch 19/130 ===\n[train step 25/88] loss 2.0880 | top1 58.28% | top3 79.12% | top5 86.16% | 257.8 img/s | lr 4.89e-04\n[train step 50/88] loss 2.0868 | top1 58.24% | top3 79.58% | top5 86.39% | 258.2 img/s | lr 4.88e-04\n[train step 75/88] loss 2.0989 | top1 57.61% | top3 79.35% | top5 86.48% | 259.1 img/s | lr 4.88e-04\n[Train] loss 2.0955 | top1 57.84% | top3 79.32% | top5 86.51% | lr 4.87e-04\n[Val]   loss 1.5988 | top1 56.42% | top3 77.74% | top5 85.10%\nBest saved to best_model.pt (val top1 56.42%)\nEpoch time: 1.52 min\n\n=== Epoch 20/130 ===\n[train step 25/88] loss 2.0296 | top1 60.52% | top3 80.69% | top5 87.52% | 259.9 img/s | lr 4.87e-04\n[train step 50/88] loss 2.0340 | top1 59.97% | top3 80.91% | top5 87.73% | 259.3 img/s | lr 4.86e-04\n[train step 75/88] loss 2.0467 | top1 59.39% | top3 80.66% | top5 87.53% | 259.2 img/s | lr 4.86e-04\n[Train] loss 2.0475 | top1 59.40% | top3 80.73% | top5 87.46% | lr 4.85e-04\n[Val]   loss 1.5955 | top1 57.46% | top3 78.58% | top5 84.94%\nBest saved to best_model.pt (val top1 57.46%)\nEpoch time: 1.52 min\n\n=== Epoch 21/130 ===\n[train step 25/88] loss 1.9795 | top1 61.16% | top3 82.31% | top5 88.83% | 259.5 img/s | lr 4.85e-04\n[train step 50/88] loss 1.9929 | top1 61.23% | top3 81.74% | top5 88.20% | 260.0 img/s | lr 4.84e-04\n[train step 75/88] loss 1.9969 | top1 60.76% | top3 81.82% | top5 88.24% | 259.6 img/s | lr 4.84e-04\n[Train] loss 2.0068 | top1 60.63% | top3 81.40% | top5 87.96% | lr 4.83e-04\n[Val]   loss 1.5524 | top1 58.20% | top3 78.54% | top5 85.70%\nBest saved to best_model.pt (val top1 58.20%)\nEpoch time: 1.52 min\n\n=== Epoch 22/130 ===\n[train step 25/88] loss 1.9314 | top1 63.25% | top3 83.31% | top5 89.41% | 256.9 img/s | lr 4.83e-04\n[train step 50/88] loss 1.9382 | top1 62.66% | top3 83.38% | top5 89.55% | 257.9 img/s | lr 4.82e-04\n[train step 75/88] loss 1.9507 | top1 62.15% | top3 83.17% | top5 89.40% | 258.2 img/s | lr 4.81e-04\n[Train] loss 1.9550 | top1 62.09% | top3 82.90% | top5 89.22% | lr 4.81e-04\n[Val]   loss 1.5021 | top1 59.68% | top3 79.62% | top5 86.12%\nBest saved to best_model.pt (val top1 59.68%)\nEpoch time: 1.53 min\n\n=== Epoch 23/130 ===\n[train step 25/88] loss 1.9018 | top1 64.62% | top3 84.03% | top5 90.50% | 257.3 img/s | lr 4.80e-04\n[train step 50/88] loss 1.9070 | top1 64.41% | top3 83.90% | top5 90.15% | 258.1 img/s | lr 4.79e-04\n[train step 75/88] loss 1.9137 | top1 63.96% | top3 83.95% | top5 90.10% | 258.3 img/s | lr 4.79e-04\n[Train] loss 1.9207 | top1 63.44% | top3 83.62% | top5 89.92% | lr 4.78e-04\n[Val]   loss 1.5417 | top1 59.14% | top3 78.88% | top5 85.86%\nEpoch time: 1.52 min\n\n=== Epoch 24/130 ===\n[train step 25/88] loss 1.8449 | top1 65.70% | top3 85.06% | top5 90.75% | 257.3 img/s | lr 4.78e-04\n[train step 50/88] loss 1.8616 | top1 65.05% | top3 84.81% | top5 90.90% | 258.8 img/s | lr 4.77e-04\n[train step 75/88] loss 1.8747 | top1 64.80% | top3 84.66% | top5 90.63% | 259.0 img/s | lr 4.76e-04\n[Train] loss 1.8805 | top1 64.50% | top3 84.45% | top5 90.51% | lr 4.76e-04\n[Val]   loss 1.4936 | top1 60.34% | top3 79.98% | top5 86.38%\nBest saved to best_model.pt (val top1 60.34%)\nEpoch time: 1.52 min\n\n=== Epoch 25/130 ===\n[train step 25/88] loss 1.7972 | top1 67.48% | top3 86.45% | top5 91.84% | 257.0 img/s | lr 4.75e-04\n[train step 50/88] loss 1.8269 | top1 66.61% | top3 85.84% | top5 91.21% | 258.4 img/s | lr 4.74e-04\n[train step 75/88] loss 1.8341 | top1 66.29% | top3 85.66% | top5 91.20% | 258.8 img/s | lr 4.73e-04\n[Train] loss 1.8423 | top1 66.00% | top3 85.43% | top5 91.08% | lr 4.73e-04\n[Val]   loss 1.4598 | top1 59.78% | top3 81.20% | top5 87.14%\nEpoch time: 1.52 min\n\n=== Epoch 26/130 ===\n[train step 25/88] loss 1.7887 | top1 67.56% | top3 86.88% | top5 92.06% | 256.1 img/s | lr 4.72e-04\n[train step 50/88] loss 1.7986 | top1 67.44% | top3 86.58% | top5 92.03% | 257.3 img/s | lr 4.71e-04\n[train step 75/88] loss 1.7994 | top1 67.55% | top3 86.41% | top5 91.94% | 258.0 img/s | lr 4.70e-04\n[Train] loss 1.7992 | top1 67.23% | top3 86.50% | top5 91.98% | lr 4.70e-04\n[Val]   loss 1.4839 | top1 60.26% | top3 80.10% | top5 86.60%\nEpoch time: 1.52 min\n\n=== Epoch 27/130 ===\n[train step 25/88] loss 1.7256 | top1 69.89% | top3 87.84% | top5 92.86% | 256.2 img/s | lr 4.69e-04\n[train step 50/88] loss 1.7501 | top1 68.98% | top3 87.61% | top5 92.83% | 257.2 img/s | lr 4.68e-04\n[train step 75/88] loss 1.7636 | top1 68.49% | top3 87.24% | top5 92.50% | 257.7 img/s | lr 4.67e-04\n[Train] loss 1.7712 | top1 68.16% | top3 87.24% | top5 92.37% | lr 4.67e-04\n[Val]   loss 1.4618 | top1 60.80% | top3 80.46% | top5 86.92%\nBest saved to best_model.pt (val top1 60.80%)\nEpoch time: 1.53 min\n\n=== Epoch 28/130 ===\n[train step 25/88] loss 1.6957 | top1 71.09% | top3 88.81% | top5 93.19% | 256.5 img/s | lr 4.66e-04\n[train step 50/88] loss 1.7178 | top1 70.12% | top3 88.23% | top5 92.95% | 257.4 img/s | lr 4.65e-04\n[train step 75/88] loss 1.7254 | top1 69.78% | top3 88.18% | top5 93.02% | 258.0 img/s | lr 4.64e-04\n[Train] loss 1.7318 | top1 69.54% | top3 88.14% | top5 92.98% | lr 4.64e-04\n[Val]   loss 1.4254 | top1 62.32% | top3 81.28% | top5 87.30%\nBest saved to best_model.pt (val top1 62.32%)\nEpoch time: 1.53 min\n\n=== Epoch 29/130 ===\n[train step 25/88] loss 1.6738 | top1 72.09% | top3 89.16% | top5 93.83% | 258.9 img/s | lr 4.63e-04\n[train step 50/88] loss 1.6798 | top1 71.58% | top3 89.05% | top5 93.83% | 259.3 img/s | lr 4.62e-04\n[train step 75/88] loss 1.6984 | top1 70.92% | top3 88.67% | top5 93.49% | 259.0 img/s | lr 4.61e-04\n[Train] loss 1.7064 | top1 70.53% | top3 88.46% | top5 93.33% | lr 4.60e-04\n[Val]   loss 1.4442 | top1 62.02% | top3 80.82% | top5 86.64%\nEpoch time: 1.52 min\n\n=== Epoch 30/130 ===\n[train step 25/88] loss 1.6251 | top1 73.22% | top3 90.14% | top5 94.67% | 256.1 img/s | lr 4.59e-04\n[train step 50/88] loss 1.6496 | top1 72.55% | top3 89.75% | top5 94.10% | 257.4 img/s | lr 4.58e-04\n[train step 75/88] loss 1.6711 | top1 71.92% | top3 89.16% | top5 93.82% | 258.2 img/s | lr 4.57e-04\n[Train] loss 1.6752 | top1 71.52% | top3 89.16% | top5 93.85% | lr 4.57e-04\n[Val]   loss 1.4337 | top1 62.22% | top3 81.30% | top5 87.10%\nEpoch time: 1.52 min\n\n=== Epoch 31/130 ===\n[train step 25/88] loss 1.6134 | top1 73.78% | top3 90.64% | top5 94.84% | 259.6 img/s | lr 4.56e-04\n[train step 50/88] loss 1.6342 | top1 72.99% | top3 90.28% | top5 94.58% | 260.3 img/s | lr 4.55e-04\n[train step 75/88] loss 1.6494 | top1 72.21% | top3 90.02% | top5 94.26% | 259.9 img/s | lr 4.54e-04\n[Train] loss 1.6458 | top1 72.48% | top3 90.04% | top5 94.28% | lr 4.53e-04\n[Val]   loss 1.4207 | top1 62.50% | top3 81.30% | top5 87.32%\nBest saved to best_model.pt (val top1 62.50%)\nEpoch time: 1.52 min\n\n=== Epoch 32/130 ===\n[train step 25/88] loss 1.5829 | top1 74.95% | top3 91.59% | top5 95.22% | 257.4 img/s | lr 4.52e-04\n[train step 50/88] loss 1.5886 | top1 74.79% | top3 91.55% | top5 95.26% | 258.1 img/s | lr 4.51e-04\n[train step 75/88] loss 1.6090 | top1 73.98% | top3 90.84% | top5 94.89% | 258.3 img/s | lr 4.50e-04\n[Train] loss 1.6156 | top1 73.61% | top3 90.62% | top5 94.77% | lr 4.49e-04\n[Val]   loss 1.4368 | top1 62.48% | top3 81.72% | top5 87.30%\nEpoch time: 1.52 min\n\n=== Epoch 33/130 ===\n[train step 25/88] loss 1.5592 | top1 75.45% | top3 91.78% | top5 95.59% | 256.7 img/s | lr 4.48e-04\n[train step 50/88] loss 1.5749 | top1 74.77% | top3 91.61% | top5 95.44% | 257.8 img/s | lr 4.47e-04\n[train step 75/88] loss 1.5905 | top1 74.26% | top3 91.05% | top5 95.09% | 258.1 img/s | lr 4.46e-04\n[Train] loss 1.5911 | top1 74.35% | top3 91.10% | top5 95.10% | lr 4.45e-04\n[Val]   loss 1.4072 | top1 62.58% | top3 82.04% | top5 87.60%\nBest saved to best_model.pt (val top1 62.58%)\nEpoch time: 1.53 min\n\n=== Epoch 34/130 ===\n[train step 25/88] loss 1.5335 | top1 77.06% | top3 92.41% | top5 95.62% | 257.5 img/s | lr 4.44e-04\n[train step 50/88] loss 1.5336 | top1 76.91% | top3 92.38% | top5 95.76% | 258.9 img/s | lr 4.43e-04\n[train step 75/88] loss 1.5536 | top1 76.18% | top3 91.93% | top5 95.46% | 259.0 img/s | lr 4.42e-04\n[Train] loss 1.5624 | top1 75.60% | top3 91.71% | top5 95.43% | lr 4.41e-04\n[Val]   loss 1.3817 | top1 63.68% | top3 81.52% | top5 87.86%\nBest saved to best_model.pt (val top1 63.68%)\nEpoch time: 1.53 min\n\n=== Epoch 35/130 ===\n[train step 25/88] loss 1.5218 | top1 76.56% | top3 92.25% | top5 96.06% | 257.5 img/s | lr 4.40e-04\n[train step 50/88] loss 1.5195 | top1 76.68% | top3 92.32% | top5 96.07% | 259.3 img/s | lr 4.39e-04\n[train step 75/88] loss 1.5302 | top1 76.36% | top3 92.07% | top5 95.90% | 260.0 img/s | lr 4.38e-04\n[Train] loss 1.5379 | top1 76.11% | top3 92.12% | top5 95.81% | lr 4.37e-04\n[Val]   loss 1.3966 | top1 63.50% | top3 81.70% | top5 87.64%\nEpoch time: 1.51 min\n\n=== Epoch 36/130 ===\n[train step 25/88] loss 1.4776 | top1 78.59% | top3 93.12% | top5 96.55% | 255.6 img/s | lr 4.36e-04\n[train step 50/88] loss 1.4910 | top1 78.11% | top3 92.88% | top5 96.35% | 257.5 img/s | lr 4.35e-04\n[train step 75/88] loss 1.5092 | top1 77.38% | top3 92.56% | top5 96.06% | 259.2 img/s | lr 4.33e-04\n[Train] loss 1.5127 | top1 77.24% | top3 92.55% | top5 96.00% | lr 4.33e-04\n[Val]   loss 1.3825 | top1 63.72% | top3 81.70% | top5 87.68%\nBest saved to best_model.pt (val top1 63.72%)\nEpoch time: 1.52 min\n\n=== Epoch 37/130 ===\n[train step 25/88] loss 1.4412 | top1 79.88% | top3 94.11% | top5 96.95% | 259.0 img/s | lr 4.32e-04\n[train step 50/88] loss 1.4533 | top1 79.34% | top3 93.99% | top5 96.93% | 258.8 img/s | lr 4.30e-04\n[train step 75/88] loss 1.4737 | top1 78.48% | top3 93.42% | top5 96.54% | 258.7 img/s | lr 4.29e-04\n[Train] loss 1.4814 | top1 78.22% | top3 93.23% | top5 96.42% | lr 4.28e-04\n[Val]   loss 1.4039 | top1 63.56% | top3 81.98% | top5 87.14%\nEpoch time: 1.52 min\n\n=== Epoch 38/130 ===\n[train step 25/88] loss 1.4460 | top1 79.70% | top3 94.06% | top5 96.45% | 256.1 img/s | lr 4.27e-04\n[train step 50/88] loss 1.4682 | top1 78.77% | top3 93.52% | top5 96.33% | 257.4 img/s | lr 4.26e-04\n[train step 75/88] loss 1.4704 | top1 78.67% | top3 93.62% | top5 96.36% | 258.2 img/s | lr 4.25e-04\n[Train] loss 1.4754 | top1 78.59% | top3 93.41% | top5 96.36% | lr 4.24e-04\n[Val]   loss 1.3772 | top1 63.86% | top3 82.14% | top5 87.70%\nBest saved to best_model.pt (val top1 63.86%)\nEpoch time: 1.53 min\n\n=== Epoch 39/130 ===\n[train step 25/88] loss 1.4089 | top1 81.14% | top3 94.69% | top5 97.31% | 258.4 img/s | lr 4.23e-04\n[train step 50/88] loss 1.4277 | top1 80.45% | top3 94.12% | top5 96.94% | 258.6 img/s | lr 4.21e-04\n[train step 75/88] loss 1.4338 | top1 80.20% | top3 93.98% | top5 96.83% | 258.4 img/s | lr 4.20e-04\n[Train] loss 1.4431 | top1 79.81% | top3 93.82% | top5 96.79% | lr 4.19e-04\n[Val]   loss 1.4032 | top1 63.96% | top3 82.24% | top5 87.70%\nBest saved to best_model.pt (val top1 63.96%)\nEpoch time: 1.53 min\n\n=== Epoch 40/130 ===\n[train step 25/88] loss 1.3972 | top1 81.70% | top3 94.70% | top5 97.11% | 258.2 img/s | lr 4.18e-04\n[train step 50/88] loss 1.4106 | top1 80.92% | top3 94.62% | top5 97.11% | 259.0 img/s | lr 4.17e-04\n[train step 75/88] loss 1.4158 | top1 80.76% | top3 94.56% | top5 97.20% | 259.2 img/s | lr 4.15e-04\n[Train] loss 1.4179 | top1 80.67% | top3 94.48% | top5 97.13% | lr 4.15e-04\n[Val]   loss 1.4218 | top1 63.72% | top3 81.72% | top5 87.42%\nEpoch time: 1.51 min\n\n=== Epoch 41/130 ===\n[train step 25/88] loss 1.3772 | top1 82.25% | top3 95.30% | top5 97.50% | 256.2 img/s | lr 4.13e-04\n[train step 50/88] loss 1.3857 | top1 81.87% | top3 94.93% | top5 97.23% | 257.3 img/s | lr 4.12e-04\n[train step 75/88] loss 1.3899 | top1 81.80% | top3 94.80% | top5 97.21% | 258.0 img/s | lr 4.10e-04\n[Train] loss 1.4006 | top1 81.39% | top3 94.72% | top5 97.24% | lr 4.10e-04\n[Val]   loss 1.3777 | top1 64.10% | top3 82.28% | top5 87.54%\nBest saved to best_model.pt (val top1 64.10%)\nEpoch time: 1.53 min\n\n=== Epoch 42/130 ===\n[train step 25/88] loss 1.3762 | top1 82.44% | top3 95.28% | top5 97.83% | 258.2 img/s | lr 4.08e-04\n[train step 50/88] loss 1.3693 | top1 82.60% | top3 95.27% | top5 97.68% | 259.0 img/s | lr 4.07e-04\n[train step 75/88] loss 1.3770 | top1 82.22% | top3 95.20% | top5 97.60% | 258.9 img/s | lr 4.06e-04\n[Train] loss 1.3824 | top1 82.05% | top3 95.07% | top5 97.40% | lr 4.05e-04\n[Val]   loss 1.3907 | top1 64.72% | top3 82.22% | top5 87.86%\nBest saved to best_model.pt (val top1 64.72%)\nEpoch time: 1.52 min\n\n=== Epoch 43/130 ===\n[train step 25/88] loss 1.3348 | top1 83.73% | top3 95.98% | top5 97.92% | 256.8 img/s | lr 4.03e-04\n[train step 50/88] loss 1.3528 | top1 82.86% | top3 95.60% | top5 97.70% | 257.7 img/s | lr 4.02e-04\n[train step 75/88] loss 1.3625 | top1 82.48% | top3 95.41% | top5 97.56% | 258.0 img/s | lr 4.01e-04\n[Train] loss 1.3620 | top1 82.61% | top3 95.45% | top5 97.55% | lr 4.00e-04\n[Val]   loss 1.3644 | top1 65.00% | top3 82.40% | top5 87.84%\nBest saved to best_model.pt (val top1 65.00%)\nEpoch time: 1.53 min\n\n=== Epoch 44/130 ===\n[train step 25/88] loss 1.3145 | top1 85.20% | top3 95.91% | top5 97.84% | 257.9 img/s | lr 3.98e-04\n[train step 50/88] loss 1.3233 | top1 84.52% | top3 95.98% | top5 97.88% | 259.0 img/s | lr 3.97e-04\n[train step 75/88] loss 1.3305 | top1 84.07% | top3 95.82% | top5 97.87% | 259.0 img/s | lr 3.95e-04\n[Train] loss 1.3423 | top1 83.65% | top3 95.59% | top5 97.62% | lr 3.95e-04\n[Val]   loss 1.3899 | top1 64.02% | top3 82.42% | top5 87.72%\nEpoch time: 1.51 min\n\n=== Epoch 45/130 ===\n[train step 25/88] loss 1.3164 | top1 85.02% | top3 95.95% | top5 97.78% | 256.4 img/s | lr 3.93e-04\n[train step 50/88] loss 1.3181 | top1 84.83% | top3 95.91% | top5 97.77% | 257.7 img/s | lr 3.92e-04\n[train step 75/88] loss 1.3223 | top1 84.44% | top3 95.76% | top5 97.76% | 258.0 img/s | lr 3.90e-04\n[Train] loss 1.3269 | top1 84.10% | top3 95.76% | top5 97.77% | lr 3.89e-04\n[Val]   loss 1.3685 | top1 65.08% | top3 82.46% | top5 87.84%\nBest saved to best_model.pt (val top1 65.08%)\nEpoch time: 1.53 min\n\n=== Epoch 46/130 ===\n[train step 25/88] loss 1.3019 | top1 85.20% | top3 96.27% | top5 98.06% | 257.0 img/s | lr 3.88e-04\n[train step 50/88] loss 1.2999 | top1 85.23% | top3 96.30% | top5 98.12% | 257.8 img/s | lr 3.86e-04\n[train step 75/88] loss 1.3103 | top1 84.77% | top3 96.14% | top5 98.02% | 258.2 img/s | lr 3.85e-04\n[Train] loss 1.3135 | top1 84.51% | top3 96.08% | top5 97.92% | lr 3.84e-04\n[Val]   loss 1.3820 | top1 65.20% | top3 82.48% | top5 87.88%\nBest saved to best_model.pt (val top1 65.20%)\nEpoch time: 1.53 min\n\n=== Epoch 47/130 ===\n[train step 25/88] loss 1.2815 | top1 86.14% | top3 96.61% | top5 98.16% | 256.7 img/s | lr 3.83e-04\n[train step 50/88] loss 1.2883 | top1 85.57% | top3 96.42% | top5 98.16% | 257.7 img/s | lr 3.81e-04\n[train step 75/88] loss 1.2986 | top1 85.06% | top3 96.17% | top5 98.02% | 258.6 img/s | lr 3.79e-04\n[Train] loss 1.2972 | top1 85.16% | top3 96.14% | top5 98.02% | lr 3.79e-04\n[Val]   loss 1.3654 | top1 65.34% | top3 82.48% | top5 88.08%\nBest saved to best_model.pt (val top1 65.34%)\nEpoch time: 1.52 min\n\n=== Epoch 48/130 ===\n[train step 25/88] loss 1.2682 | top1 85.97% | top3 96.45% | top5 98.14% | 259.1 img/s | lr 3.77e-04\n[train step 50/88] loss 1.2707 | top1 86.09% | top3 96.52% | top5 98.11% | 258.8 img/s | lr 3.76e-04\n[train step 75/88] loss 1.2802 | top1 85.58% | top3 96.33% | top5 98.02% | 258.8 img/s | lr 3.74e-04\n[Train] loss 1.2819 | top1 85.71% | top3 96.32% | top5 98.04% | lr 3.73e-04\n[Val]   loss 1.3867 | top1 65.64% | top3 82.38% | top5 87.68%\nBest saved to best_model.pt (val top1 65.64%)\nEpoch time: 1.52 min\n\n=== Epoch 49/130 ===\n[train step 25/88] loss 1.2663 | top1 86.47% | top3 96.47% | top5 98.08% | 257.0 img/s | lr 3.72e-04\n[train step 50/88] loss 1.2713 | top1 86.25% | top3 96.50% | top5 98.10% | 257.8 img/s | lr 3.70e-04\n[train step 75/88] loss 1.2711 | top1 86.25% | top3 96.60% | top5 98.17% | 258.4 img/s | lr 3.68e-04\n[Train] loss 1.2768 | top1 85.98% | top3 96.46% | top5 98.10% | lr 3.68e-04\n[Val]   loss 1.3483 | top1 65.28% | top3 83.46% | top5 88.50%\nEpoch time: 1.52 min\n\n=== Epoch 50/130 ===\n[train step 25/88] loss 1.2306 | top1 87.61% | top3 97.34% | top5 98.58% | 258.3 img/s | lr 3.66e-04\n[train step 50/88] loss 1.2438 | top1 87.16% | top3 97.05% | top5 98.47% | 258.7 img/s | lr 3.64e-04\n[train step 75/88] loss 1.2501 | top1 86.99% | top3 96.94% | top5 98.42% | 258.7 img/s | lr 3.63e-04\n[Train] loss 1.2538 | top1 86.83% | top3 96.88% | top5 98.35% | lr 3.62e-04\n[Val]   loss 1.3477 | top1 65.84% | top3 83.40% | top5 88.50%\nBest saved to best_model.pt (val top1 65.84%)\nEpoch time: 1.52 min\n\n=== Epoch 51/130 ===\n[train step 25/88] loss 1.2257 | top1 87.75% | top3 96.83% | top5 98.39% | 256.7 img/s | lr 3.60e-04\n[train step 50/88] loss 1.2364 | top1 87.30% | top3 96.86% | top5 98.32% | 257.7 img/s | lr 3.59e-04\n[train step 75/88] loss 1.2463 | top1 87.02% | top3 96.74% | top5 98.23% | 258.0 img/s | lr 3.57e-04\n[Train] loss 1.2471 | top1 86.96% | top3 96.83% | top5 98.29% | lr 3.56e-04\n[Val]   loss 1.3541 | top1 66.04% | top3 82.98% | top5 88.14%\nBest saved to best_model.pt (val top1 66.04%)\nEpoch time: 1.53 min\n\n=== Epoch 52/130 ===\n[train step 25/88] loss 1.2120 | top1 88.55% | top3 97.33% | top5 98.42% | 257.8 img/s | lr 3.55e-04\n[train step 50/88] loss 1.2156 | top1 88.39% | top3 97.23% | top5 98.48% | 258.1 img/s | lr 3.53e-04\n[train step 75/88] loss 1.2189 | top1 88.26% | top3 97.19% | top5 98.52% | 258.6 img/s | lr 3.51e-04\n[Train] loss 1.2330 | top1 87.62% | top3 96.86% | top5 98.34% | lr 3.50e-04\n[Val]   loss 1.3587 | top1 66.22% | top3 82.96% | top5 87.98%\nBest saved to best_model.pt (val top1 66.22%)\nEpoch time: 1.52 min\n\n=== Epoch 53/130 ===\n[train step 25/88] loss 1.1961 | top1 88.77% | top3 97.41% | top5 98.58% | 258.1 img/s | lr 3.49e-04\n[train step 50/88] loss 1.2083 | top1 88.38% | top3 97.29% | top5 98.62% | 258.4 img/s | lr 3.47e-04\n[train step 75/88] loss 1.2184 | top1 88.17% | top3 97.17% | top5 98.52% | 258.2 img/s | lr 3.45e-04\n[Train] loss 1.2184 | top1 88.17% | top3 97.19% | top5 98.47% | lr 3.45e-04\n[Val]   loss 1.3675 | top1 66.18% | top3 82.86% | top5 88.40%\nEpoch time: 1.52 min\n\n=== Epoch 54/130 ===\n[train step 25/88] loss 1.1981 | top1 89.00% | top3 97.31% | top5 98.52% | 258.8 img/s | lr 3.43e-04\n[train step 50/88] loss 1.1982 | top1 88.90% | top3 97.30% | top5 98.57% | 259.8 img/s | lr 3.41e-04\n[train step 75/88] loss 1.2059 | top1 88.58% | top3 97.27% | top5 98.54% | 259.6 img/s | lr 3.40e-04\n[Train] loss 1.2081 | top1 88.46% | top3 97.18% | top5 98.55% | lr 3.39e-04\n[Val]   loss 1.3637 | top1 65.94% | top3 83.02% | top5 87.96%\nEpoch time: 1.51 min\n\n=== Epoch 55/130 ===\n[train step 25/88] loss 1.1808 | top1 89.47% | top3 97.47% | top5 98.58% | 257.6 img/s | lr 3.37e-04\n[train step 50/88] loss 1.1950 | top1 89.04% | top3 97.27% | top5 98.40% | 258.6 img/s | lr 3.35e-04\n[train step 75/88] loss 1.2048 | top1 88.60% | top3 97.23% | top5 98.39% | 258.6 img/s | lr 3.34e-04\n[Train] loss 1.1968 | top1 88.83% | top3 97.30% | top5 98.52% | lr 3.33e-04\n[Val]   loss 1.3386 | top1 66.30% | top3 83.24% | top5 88.46%\nBest saved to best_model.pt (val top1 66.30%)\nEpoch time: 1.52 min\n\n=== Epoch 56/130 ===\n[train step 25/88] loss 1.1662 | top1 90.09% | top3 97.59% | top5 98.69% | 256.8 img/s | lr 3.31e-04\n[train step 50/88] loss 1.1783 | top1 89.49% | top3 97.57% | top5 98.68% | 257.9 img/s | lr 3.29e-04\n[train step 75/88] loss 1.1838 | top1 89.26% | top3 97.49% | top5 98.64% | 258.4 img/s | lr 3.28e-04\n[Train] loss 1.1880 | top1 89.05% | top3 97.38% | top5 98.62% | lr 3.27e-04\n[Val]   loss 1.3745 | top1 65.86% | top3 82.86% | top5 87.66%\nEpoch time: 1.52 min\n\n=== Epoch 57/130 ===\n[train step 25/88] loss 1.1644 | top1 90.05% | top3 97.83% | top5 98.77% | 258.2 img/s | lr 3.25e-04\n[train step 50/88] loss 1.1754 | top1 89.70% | top3 97.51% | top5 98.66% | 259.1 img/s | lr 3.23e-04\n[train step 75/88] loss 1.1750 | top1 89.59% | top3 97.55% | top5 98.71% | 259.0 img/s | lr 3.21e-04\n[Train] loss 1.1779 | top1 89.39% | top3 97.46% | top5 98.66% | lr 3.21e-04\n[Val]   loss 1.3157 | top1 67.08% | top3 83.66% | top5 88.36%\nBest saved to best_model.pt (val top1 67.08%)\nEpoch time: 1.52 min\n\n=== Epoch 58/130 ===\n[train step 25/88] loss 1.1356 | top1 91.12% | top3 98.14% | top5 99.06% | 256.5 img/s | lr 3.19e-04\n[train step 50/88] loss 1.1495 | top1 90.40% | top3 97.96% | top5 98.91% | 257.4 img/s | lr 3.17e-04\n[train step 75/88] loss 1.1535 | top1 90.25% | top3 97.89% | top5 98.92% | 258.0 img/s | lr 3.15e-04\n[Train] loss 1.1622 | top1 89.95% | top3 97.70% | top5 98.81% | lr 3.14e-04\n[Val]   loss 1.3304 | top1 67.38% | top3 83.54% | top5 88.44%\nBest saved to best_model.pt (val top1 67.38%)\nEpoch time: 1.53 min\n\n=== Epoch 59/130 ===\n[train step 25/88] loss 1.1494 | top1 90.67% | top3 97.70% | top5 98.59% | 257.5 img/s | lr 3.13e-04\n[train step 50/88] loss 1.1493 | top1 90.56% | top3 97.77% | top5 98.70% | 258.0 img/s | lr 3.11e-04\n[train step 75/88] loss 1.1558 | top1 90.28% | top3 97.69% | top5 98.68% | 258.4 img/s | lr 3.09e-04\n[Train] loss 1.1562 | top1 90.24% | top3 97.72% | top5 98.73% | lr 3.08e-04\n[Val]   loss 1.3392 | top1 66.96% | top3 83.62% | top5 88.30%\nEpoch time: 1.52 min\n\n=== Epoch 60/130 ===\n[train step 25/88] loss 1.1280 | top1 91.00% | top3 98.14% | top5 99.14% | 259.7 img/s | lr 3.07e-04\n[train step 50/88] loss 1.1395 | top1 90.88% | top3 97.99% | top5 98.95% | 260.4 img/s | lr 3.05e-04\n[train step 75/88] loss 1.1385 | top1 91.03% | top3 97.97% | top5 98.91% | 260.1 img/s | lr 3.03e-04\n[Train] loss 1.1406 | top1 90.79% | top3 97.87% | top5 98.89% | lr 3.02e-04\n[Val]   loss 1.3365 | top1 66.90% | top3 83.70% | top5 88.52%\nEpoch time: 1.51 min\n\n=== Epoch 61/130 ===\n[train step 25/88] loss 1.1277 | top1 91.30% | top3 97.73% | top5 98.81% | 255.7 img/s | lr 3.00e-04\n[train step 50/88] loss 1.1300 | top1 91.26% | top3 97.84% | top5 98.77% | 257.2 img/s | lr 2.99e-04\n[train step 75/88] loss 1.1314 | top1 90.96% | top3 97.94% | top5 98.89% | 258.2 img/s | lr 2.97e-04\n[Train] loss 1.1348 | top1 90.90% | top3 97.88% | top5 98.83% | lr 2.96e-04\n[Val]   loss 1.3513 | top1 66.88% | top3 83.30% | top5 88.06%\nEpoch time: 1.52 min\n\n=== Epoch 62/130 ===\n[train step 25/88] loss 1.1087 | top1 91.95% | top3 98.02% | top5 99.03% | 257.6 img/s | lr 2.94e-04\n[train step 50/88] loss 1.1096 | top1 91.80% | top3 98.12% | top5 99.12% | 258.7 img/s | lr 2.92e-04\n[train step 75/88] loss 1.1185 | top1 91.53% | top3 98.06% | top5 99.02% | 258.9 img/s | lr 2.91e-04\n[Train] loss 1.1222 | top1 91.43% | top3 97.97% | top5 98.95% | lr 2.90e-04\n[Val]   loss 1.3387 | top1 67.02% | top3 83.30% | top5 88.34%\nEpoch time: 1.51 min\n\n=== Epoch 63/130 ===\n[train step 25/88] loss 1.1137 | top1 91.70% | top3 98.03% | top5 98.97% | 256.1 img/s | lr 2.88e-04\n[train step 50/88] loss 1.1162 | top1 91.79% | top3 98.00% | top5 98.91% | 257.3 img/s | lr 2.86e-04\n[train step 75/88] loss 1.1157 | top1 91.74% | top3 98.03% | top5 98.95% | 257.9 img/s | lr 2.84e-04\n[Train] loss 1.1194 | top1 91.48% | top3 98.02% | top5 98.95% | lr 2.83e-04\n[Val]   loss 1.3082 | top1 67.90% | top3 83.76% | top5 88.78%\nBest saved to best_model.pt (val top1 67.90%)\nEpoch time: 1.53 min\n\n=== Epoch 64/130 ===\n[train step 25/88] loss 1.0862 | top1 92.86% | top3 98.45% | top5 99.14% | 256.3 img/s | lr 2.81e-04\n[train step 50/88] loss 1.0993 | top1 92.34% | top3 98.29% | top5 99.03% | 257.4 img/s | lr 2.80e-04\n[train step 75/88] loss 1.1021 | top1 92.28% | top3 98.15% | top5 98.96% | 257.9 img/s | lr 2.78e-04\n[Train] loss 1.1048 | top1 92.10% | top3 98.13% | top5 98.99% | lr 2.77e-04\n[Val]   loss 1.3598 | top1 67.14% | top3 83.40% | top5 88.18%\nEpoch time: 1.52 min\n\n=== Epoch 65/130 ===\n[train step 25/88] loss 1.1138 | top1 91.89% | top3 97.84% | top5 98.77% | 256.1 img/s | lr 2.75e-04\n[train step 50/88] loss 1.1053 | top1 92.16% | top3 98.19% | top5 98.99% | 257.4 img/s | lr 2.73e-04\n[train step 75/88] loss 1.1049 | top1 92.04% | top3 98.23% | top5 99.03% | 258.6 img/s | lr 2.72e-04\n[Train] loss 1.0993 | top1 92.23% | top3 98.26% | top5 99.06% | lr 2.71e-04\n[Val]   loss 1.3130 | top1 67.38% | top3 84.26% | top5 88.50%\nEpoch time: 1.51 min\n\n=== Epoch 66/130 ===\n[train step 25/88] loss 1.0821 | top1 92.69% | top3 98.45% | top5 99.22% | 258.3 img/s | lr 2.69e-04\n[train step 50/88] loss 1.0859 | top1 92.61% | top3 98.30% | top5 99.12% | 258.5 img/s | lr 2.67e-04\n[train step 75/88] loss 1.0940 | top1 92.30% | top3 98.21% | top5 99.05% | 258.5 img/s | lr 2.65e-04\n[Train] loss 1.0962 | top1 92.22% | top3 98.14% | top5 98.99% | lr 2.64e-04\n[Val]   loss 1.3114 | top1 67.40% | top3 84.10% | top5 89.10%\nEpoch time: 1.52 min\n\n=== Epoch 67/130 ===\n[train step 25/88] loss 1.0719 | top1 93.17% | top3 98.66% | top5 99.23% | 257.6 img/s | lr 2.62e-04\n[train step 50/88] loss 1.0817 | top1 92.47% | top3 98.49% | top5 99.20% | 258.5 img/s | lr 2.61e-04\n[train step 75/88] loss 1.0802 | top1 92.55% | top3 98.54% | top5 99.24% | 258.6 img/s | lr 2.59e-04\n[Train] loss 1.0817 | top1 92.63% | top3 98.49% | top5 99.23% | lr 2.58e-04\n[Val]   loss 1.3307 | top1 66.70% | top3 83.86% | top5 88.54%\nEpoch time: 1.51 min\n\n=== Epoch 68/130 ===\n[train step 25/88] loss 1.0770 | top1 93.52% | top3 98.19% | top5 98.84% | 258.5 img/s | lr 2.56e-04\n[train step 50/88] loss 1.0827 | top1 93.11% | top3 98.20% | top5 98.95% | 259.2 img/s | lr 2.54e-04\n[train step 75/88] loss 1.0801 | top1 93.03% | top3 98.35% | top5 99.07% | 258.8 img/s | lr 2.53e-04\n[Train] loss 1.0777 | top1 92.95% | top3 98.40% | top5 99.13% | lr 2.52e-04\n[Val]   loss 1.3304 | top1 67.72% | top3 83.94% | top5 88.66%\nEpoch time: 1.51 min\n\n=== Epoch 69/130 ===\n[train step 25/88] loss 1.0553 | top1 93.75% | top3 98.67% | top5 99.19% | 256.5 img/s | lr 2.50e-04\n[train step 50/88] loss 1.0594 | top1 93.52% | top3 98.61% | top5 99.17% | 257.9 img/s | lr 2.48e-04\n[train step 75/88] loss 1.0585 | top1 93.60% | top3 98.57% | top5 99.22% | 258.1 img/s | lr 2.46e-04\n[Train] loss 1.0623 | top1 93.37% | top3 98.51% | top5 99.18% | lr 2.45e-04\n[Val]   loss 1.3225 | top1 67.66% | top3 83.82% | top5 88.52%\nEpoch time: 1.52 min\n\n=== Epoch 70/130 ===\n[train step 25/88] loss 1.0464 | top1 94.23% | top3 98.47% | top5 99.17% | 257.5 img/s | lr 2.43e-04\n[train step 50/88] loss 1.0516 | top1 93.91% | top3 98.48% | top5 99.19% | 259.1 img/s | lr 2.42e-04\n[train step 75/88] loss 1.0573 | top1 93.53% | top3 98.50% | top5 99.20% | 259.3 img/s | lr 2.40e-04\n[Train] loss 1.0600 | top1 93.41% | top3 98.51% | top5 99.16% | lr 2.39e-04\n[Val]   loss 1.3205 | top1 68.30% | top3 83.86% | top5 88.70%\nBest saved to best_model.pt (val top1 68.30%)\nEpoch time: 1.52 min\n\n=== Epoch 71/130 ===\n[train step 25/88] loss 1.0544 | top1 93.69% | top3 98.64% | top5 99.11% | 255.9 img/s | lr 2.37e-04\n[train step 50/88] loss 1.0532 | top1 93.66% | top3 98.64% | top5 99.27% | 257.3 img/s | lr 2.35e-04\n[train step 75/88] loss 1.0541 | top1 93.51% | top3 98.58% | top5 99.26% | 257.7 img/s | lr 2.33e-04\n[Train] loss 1.0554 | top1 93.48% | top3 98.55% | top5 99.21% | lr 2.33e-04\n[Val]   loss 1.3262 | top1 67.62% | top3 83.92% | top5 88.74%\nEpoch time: 1.52 min\n\n=== Epoch 72/130 ===\n[train step 25/88] loss 1.0385 | top1 94.17% | top3 98.59% | top5 99.20% | 255.8 img/s | lr 2.31e-04\n[train step 50/88] loss 1.0393 | top1 94.17% | top3 98.67% | top5 99.28% | 257.3 img/s | lr 2.29e-04\n[train step 75/88] loss 1.0461 | top1 93.97% | top3 98.58% | top5 99.19% | 258.1 img/s | lr 2.27e-04\n[Train] loss 1.0486 | top1 93.83% | top3 98.57% | top5 99.19% | lr 2.26e-04\n[Val]   loss 1.3085 | top1 68.14% | top3 84.10% | top5 88.60%\nEpoch time: 1.52 min\n\n=== Epoch 73/130 ===\n[train step 25/88] loss 1.0351 | top1 94.17% | top3 98.77% | top5 99.27% | 255.9 img/s | lr 2.24e-04\n[train step 50/88] loss 1.0380 | top1 94.10% | top3 98.68% | top5 99.24% | 256.7 img/s | lr 2.23e-04\n[train step 75/88] loss 1.0426 | top1 93.80% | top3 98.66% | top5 99.23% | 257.5 img/s | lr 2.21e-04\n[Train] loss 1.0416 | top1 93.93% | top3 98.70% | top5 99.24% | lr 2.20e-04\n[Val]   loss 1.2951 | top1 68.20% | top3 84.34% | top5 88.68%\nEpoch time: 1.52 min\n\n=== Epoch 74/130 ===\n[train step 25/88] loss 1.0245 | top1 94.70% | top3 98.67% | top5 99.30% | 257.1 img/s | lr 2.18e-04\n[train step 50/88] loss 1.0306 | top1 94.39% | top3 98.70% | top5 99.34% | 257.7 img/s | lr 2.16e-04\n[train step 75/88] loss 1.0328 | top1 94.33% | top3 98.69% | top5 99.30% | 258.0 img/s | lr 2.14e-04\n[Train] loss 1.0371 | top1 94.05% | top3 98.63% | top5 99.26% | lr 2.14e-04\n[Val]   loss 1.3177 | top1 67.90% | top3 84.34% | top5 88.84%\nEpoch time: 1.52 min\n\n=== Epoch 75/130 ===\n[train step 25/88] loss 1.0176 | top1 94.64% | top3 98.95% | top5 99.38% | 256.3 img/s | lr 2.12e-04\n[train step 50/88] loss 1.0206 | top1 94.54% | top3 98.91% | top5 99.38% | 257.3 img/s | lr 2.10e-04\n[train step 75/88] loss 1.0261 | top1 94.32% | top3 98.82% | top5 99.33% | 257.7 img/s | lr 2.08e-04\n^C\nW1231 05:40:24.593000 2847 torch/distributed/elastic/agent/server/api.py:719] Received 2 death signal, shutting down workers\nW1231 05:40:24.593000 2847 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 2849 closing signal SIGINT\nW1231 05:40:24.594000 2847 torch/distributed/elastic/multiprocessing/api.py:897] Sending process 2850 closing signal SIGINT\n[rank0]: Traceback (most recent call last):\n[rank0]:   File \"/kaggle/working/train_ddp.py\", line 97, in <module>\n[rank0]:     main()\n[rank0]:   File \"/kaggle/working/train_ddp.py\", line 78, in main\n[rank0]:     history, best = train_model(\n[rank0]:                     ^^^^^^^^^^^^\n[rank0]:   File \"/kaggle/working/volo.py\", line 2233, in train_model\n[rank0]:     tr_loss, tr_m = train_one_epoch(\n[rank0]:                     ^^^^^^^^^^^^^^^^\n[rank0]:   File \"/kaggle/working/volo.py\", line 2018, in train_one_epoch\n[rank0]:     scaler.scale(loss).backward()\n[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\", line 626, in backward\n[rank0]:     torch.autograd.backward(\n[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\", line 347, in backward\n[rank0]:     _engine_run_backward(\n[rank0]:   File \"/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\", line 823, in _engine_run_backward\n[rank0]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n[rank0]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank0]: KeyboardInterrupt\n[rank1]: Traceback (most recent call last):\n[rank1]:   File \"/kaggle/working/train_ddp.py\", line 97, in <module>\n[rank1]:     main()\n[rank1]:   File \"/kaggle/working/train_ddp.py\", line 78, in main\n[rank1]:     history, best = train_model(\n[rank1]:                     ^^^^^^^^^^^^\n[rank1]:   File \"/kaggle/working/volo.py\", line 2233, in train_model\n[rank1]:     tr_loss, tr_m = train_one_epoch(\n[rank1]:                     ^^^^^^^^^^^^^^^^\n[rank1]:   File \"/kaggle/working/volo.py\", line 2018, in train_one_epoch\n[rank1]:     scaler.scale(loss).backward()\n[rank1]:   File \"/usr/local/lib/python3.11/dist-packages/torch/_tensor.py\", line 626, in backward\n[rank1]:     torch.autograd.backward(\n[rank1]:   File \"/usr/local/lib/python3.11/dist-packages/torch/autograd/__init__.py\", line 347, in backward\n[rank1]:     _engine_run_backward(\n[rank1]:   File \"/usr/local/lib/python3.11/dist-packages/torch/autograd/graph.py\", line 823, in _engine_run_backward\n[rank1]:     return Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n[rank1]:            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n[rank1]: KeyboardInterrupt\n","output_type":"stream"}],"execution_count":92},{"cell_type":"code","source":"model = VOLOClassifier(\n        num_classes=100,\n        img_size=32,\n        patch_size=4,\n        hierarchical=False,\n        embed_dim=320,\n        outlooker_depth=5,\n        outlooker_heads=10,\n        transformer_depth=10,\n        transformer_heads=10,\n        kernel_size=3,\n        mlp_ratio=4.0,\n        dropout=0.12,\n        attn_dropout=0.05,\n        drop_path_rate=0.20,\n        pooling=\"cls\",\n        cls_attn_depth=2,\n        use_pos_embed=True,\n        use_cls_pos=True,)\n\nstate = torch.load(\"best_model.pt\", map_location=\"cpu\")\n\nif isinstance(state, dict) and (\"model\" in state or \"state_dict\" in state):\n    sd = state.get(\"model\", state.get(\"state_dict\"))\nelse:\n    sd = state \n\n\nif any(k.startswith(\"module.\") for k in sd.keys()):\n    sd = {k.replace(\"module.\", \"\", 1): v for k, v in sd.items()}\n\n\nmissing, unexpected = model.load_state_dict(sd, strict=True)\nprint(\"missing:\", missing)\nprint(\"unexpected:\", unexpected)\n\ndevice = torch.device(\"cuda:0\")\nmodel = model.to(device) \n\nmodel.eval()\n\ntest_loss, test_m = evaluate_one_epoch(\n    model=model,\n    dataloader=test_loader,\n    device=\"cuda\",\n    use_amp=False,         \n    autocast_dtype=\"fp16\")\n\nprint(\"[Test VOLO paper-like] loss\", test_loss, \"|\", test_m)","metadata":{"id":"VDMZKIM0_P83","trusted":true,"execution":{"iopub.status.busy":"2025-12-31T05:48:44.697688Z","iopub.execute_input":"2025-12-31T05:48:44.698254Z","iopub.status.idle":"2025-12-31T05:48:59.715643Z","shell.execute_reply.started":"2025-12-31T05:48:44.698221Z","shell.execute_reply":"2025-12-31T05:48:59.714832Z"}},"outputs":[{"name":"stdout","text":"missing: []\nunexpected: []\n[Test VOLO paper-like] loss 1.3181868873596192 | {'top1': 67.9, 'top3': 83.93, 'top5': 88.22}\n","output_type":"stream"}],"execution_count":98}]}